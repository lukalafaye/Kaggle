{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9644358,"sourceType":"datasetVersion","datasetId":5887736},{"sourceId":145569,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":123427,"modelId":139891}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lukalafaye/gan-mnist-dslab2?scriptVersionId=206819999\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Recall / Precision stuff","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\nimport os\nfrom functools import partial\nfrom collections import namedtuple\nfrom glob import glob\nimport numpy as np\nfrom PIL import Image\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\nfrom tqdm import tqdm\nfrom tqdm import trange\n\nimport torch\nimport torchvision.models as models\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, datasets\nfrom torchvision.utils import save_image\n\nManifold = namedtuple(\"Manifold\", [\"features\", \"radii\"])\nPrecisionAndRecall = namedtuple(\"PrecisionAndRecall\", [\"precision\", \"recall\"])\n\n\ndef convert_mnist_to_images(output_dir, num_images=100):\n    os.makedirs(output_dir, exist_ok=True)\n    transform = transforms.Compose([transforms.ToTensor()])\n    mnist_data = datasets.MNIST(\n        root=\"data/MNIST\", train=True, transform=transform, download=True\n    )\n\n    for i in range(num_images):\n        img, label = mnist_data[i]\n        save_image(img, os.path.join(output_dir, f\"real_{i}.png\"))\n    print(f\"Converted {num_images} MNIST images to PNG format in '{output_dir}'.\")\n\n\nclass IPR:\n    def __init__(self, batch_size=50, k=3, num_samples=10000, model=None):\n        self.manifold_ref = None\n        self.batch_size = batch_size\n        self.k = k\n        self.num_samples = num_samples\n        if model is None:\n            print(\n                \"loading vgg16 for improved precision and recall...\", end=\"\", flush=True\n            )\n            self.vgg16 = models.vgg16(pretrained=True).cuda().eval()\n            print(\"done\")\n        else:\n            self.vgg16 = model\n\n    def __call__(self, subject):\n        return self.precision_and_recall(subject)\n\n    def precision_and_recall(self, subject):\n        \"\"\"\n        Compute precision and recall for given subject\n        reference should be precomputed by IPR.compute_manifold_ref()\n        args:\n            subject: path or images\n                path: a directory containing images or precalculated .npz file\n                images: torch.Tensor of N x C x H x W\n        returns:\n            PrecisionAndRecall\n        \"\"\"\n        assert self.manifold_ref is not None, \"call IPR.compute_manifold_ref() first\"\n\n        manifold_subject = self.compute_manifold(subject)\n        precision = compute_metric(\n            self.manifold_ref, manifold_subject.features, \"computing precision...\"\n        )\n        recall = compute_metric(\n            manifold_subject, self.manifold_ref.features, \"computing recall...\"\n        )\n        return PrecisionAndRecall(precision, recall)\n\n    def compute_manifold_ref(self, path):\n        self.manifold_ref = self.compute_manifold(path)\n\n    def realism(self, image):\n        \"\"\"\n        args:\n            image: torch.Tensor of 1 x C x H x W\n        \"\"\"\n        feat = self.extract_features(image)\n        return realism(self.manifold_ref, feat)\n\n    def compute_manifold(self, input):\n        \"\"\"\n        Compute manifold of given input\n        args:\n            input: path or images, same as above\n        returns:\n            Manifold(features, radii)\n        \"\"\"\n        # features\n        if isinstance(input, str):\n            if input.endswith(\".npz\"):  # input is precalculated file\n                print(\"loading\", input)\n                f = np.load(input)\n                feats = f[\"feature\"]\n                radii = f[\"radii\"]\n                f.close()\n                return Manifold(feats, radii)\n            else:  # input is dir\n                feats = self.extract_features_from_files(input)\n        elif isinstance(input, torch.Tensor):\n            feats = self.extract_features(input)\n        elif isinstance(input, np.ndarray):\n            input = torch.Tensor(input)\n            feats = self.extract_features(input)\n        elif isinstance(input, list):\n            if isinstance(input[0], torch.Tensor):\n                input = torch.cat(input, dim=0)\n                feats = self.extract_features(input)\n            elif isinstance(input[0], np.ndarray):\n                input = np.concatenate(input, axis=0)\n                input = torch.Tensor(input)\n                feats = self.extract_features(input)\n            elif isinstance(input[0], str):  # input is list of fnames\n                feats = self.extract_features_from_files(input)\n            else:\n                raise TypeError\n        else:\n            print(type(input))\n            raise TypeError\n\n        # radii\n        distances = compute_pairwise_distances(feats)\n        radii = distances2radii(distances, k=self.k)\n        return Manifold(feats, radii)\n\n    def extract_features(self, images):\n        \"\"\"\n        Extract features of vgg16-fc2 for all images\n        params:\n            images: torch.Tensors of size N x C x H x W\n        returns:\n            A numpy array of dimension (num images, dims)\n        \"\"\"\n        desc = \"extracting features of %d images\" % images.size(0)\n        num_batches = int(np.ceil(images.size(0) / self.batch_size))\n        _, _, height, width = images.shape\n        if height != 224 or width != 224:\n            print(\"IPR: resizing %s to (224, 224)\" % str((height, width)))\n            resize = partial(F.interpolate, size=(224, 224))\n        else:\n\n            def resize(x):\n                return x\n\n        features = []\n        for bi in trange(num_batches, desc=desc):\n            start = bi * self.batch_size\n            end = start + self.batch_size\n            batch = images[start:end]\n            batch = resize(batch)\n            before_fc = self.vgg16.features(batch.cuda())\n            before_fc = before_fc.view(-1, 7 * 7 * 512)\n            feature = self.vgg16.classifier[:4](before_fc)\n            features.append(feature.cpu().data.numpy())\n\n        return np.concatenate(features, axis=0)\n\n    def extract_features_from_files(self, path_or_fnames):\n        \"\"\"\n        Extract features of vgg16-fc2 for all images in path\n        params:\n            path_or_fnames: dir containing images or list of fnames(str)\n        returns:\n            A numpy array of dimension (num images, dims)\n        \"\"\"\n\n        dataloader = get_custom_loader(\n            path_or_fnames, batch_size=self.batch_size, num_samples=self.num_samples\n        )\n        num_found_images = len(dataloader.dataset)\n        desc = \"extracting features of %d images\" % num_found_images\n        if num_found_images < self.num_samples:\n            print(\n                \"WARNING: num_found_images(%d) < num_samples(%d)\"\n                % (num_found_images, self.num_samples)\n            )\n\n        features = []\n        for batch in tqdm(dataloader, desc=desc):\n            before_fc = self.vgg16.features(batch.cuda())\n            before_fc = before_fc.view(-1, 7 * 7 * 512)\n            feature = self.vgg16.classifier[:4](before_fc)\n            features.append(feature.cpu().data.numpy())\n\n        return np.concatenate(features, axis=0)\n\n    def save_ref(self, fname):\n        print(\"saving manifold to\", fname, \"...\")\n        np.savez_compressed(\n            fname, feature=self.manifold_ref.features, radii=self.manifold_ref.radii\n        )\n\n\ndef compute_pairwise_distances(X, Y=None):\n    \"\"\"\n    args:\n        X: np.array of shape N x dim\n        Y: np.array of shape N x dim\n    returns:\n        N x N symmetric np.array\n    \"\"\"\n    num_X = X.shape[0]\n    if Y is None:\n        num_Y = num_X\n    else:\n        num_Y = Y.shape[0]\n    X = X.astype(np.float64)  # to prevent underflow\n    X_norm_square = np.sum(X**2, axis=1, keepdims=True)\n    if Y is None:\n        Y_norm_square = X_norm_square\n    else:\n        Y_norm_square = np.sum(Y**2, axis=1, keepdims=True)\n    X_square = np.repeat(X_norm_square, num_Y, axis=1)\n    Y_square = np.repeat(Y_norm_square.T, num_X, axis=0)\n    if Y is None:\n        Y = X\n    XY = np.dot(X, Y.T)\n    diff_square = X_square - 2 * XY + Y_square\n\n    # check negative distance\n    min_diff_square = diff_square.min()\n    if min_diff_square < 0:\n        idx = diff_square < 0\n        diff_square[idx] = 0\n        print(\n            \"WARNING: %d negative diff_squares found and set to zero, min_diff_square=\"\n            % idx.sum(),\n            min_diff_square,\n        )\n\n    distances = np.sqrt(diff_square)\n    return distances\n\n\ndef distances2radii(distances, k=3):\n    num_features = distances.shape[0]\n    radii = np.zeros(num_features)\n    for i in range(num_features):\n        radii[i] = get_kth_value(distances[i], k=k)\n    return radii\n\n\ndef get_kth_value(np_array, k):\n    kprime = k + 1  # kth NN should be (k+1)th because closest one is itself\n    idx = np.argpartition(np_array, kprime)\n    k_smallests = np_array[idx[:kprime]]\n    kth_value = k_smallests.max()\n    return kth_value\n\n\ndef compute_metric(manifold_ref, feats_subject, desc=\"\"):\n    num_subjects = feats_subject.shape[0]\n    count = 0\n    dist = compute_pairwise_distances(manifold_ref.features, feats_subject)\n    for i in trange(num_subjects, desc=desc):\n        count += (dist[:, i] < manifold_ref.radii).any()\n    return count / num_subjects\n\n\ndef is_in_ball(center, radius, subject):\n    return distance(center, subject) < radius\n\n\ndef distance(feat1, feat2):\n    return np.linalg.norm(feat1 - feat2)\n\n\ndef realism(manifold_real, feat_subject):\n    feats_real = manifold_real.features\n    radii_real = manifold_real.radii\n    diff = feats_real - feat_subject\n    dists = np.linalg.norm(diff, axis=1)\n    eps = 1e-6\n    ratios = radii_real / (dists + eps)\n    max_realism = float(ratios.max())\n    return max_realism\n\n\nclass ImageFolder(Dataset):\n    def __init__(self, root, transform=None):\n        # self.fnames = list(map(lambda x: os.path.join(root, x), os.listdir(root)))\n        self.fnames = glob(os.path.join(root, \"**\", \"*.jpg\"), recursive=True) + glob(\n            os.path.join(root, \"**\", \"*.png\"), recursive=True\n        )\n\n        self.transform = transform\n\n    def __getitem__(self, index):\n        image_path = self.fnames[index]\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform is not None:\n            image = self.transform(image)\n        return image\n\n    def __len__(self):\n        return len(self.fnames)\n\n\nclass FileNames(Dataset):\n    def __init__(self, fnames, transform=None):\n        self.fnames = fnames\n        self.transform = transform\n\n    def __getitem__(self, index):\n        image_path = self.fnames[index]\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform is not None:\n            image = self.transform(image)\n        return image\n\n    def __len__(self):\n        return len(self.fnames)\n\n\ndef get_custom_loader(\n    image_dir_or_fnames, image_size=224, batch_size=50, num_workers=4, num_samples=-1\n):\n    transform = []\n    transform.append(transforms.Resize([image_size, image_size]))\n    transform.append(transforms.ToTensor())\n    transform.append(\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    )\n    transform = transforms.Compose(transform)\n\n    if isinstance(image_dir_or_fnames, list):\n        dataset = FileNames(image_dir_or_fnames, transform)\n    elif isinstance(image_dir_or_fnames, str):\n        dataset = ImageFolder(image_dir_or_fnames, transform=transform)\n    else:\n        raise TypeError\n\n    if num_samples > 0:\n        dataset.fnames = dataset.fnames[:num_samples]\n    data_loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    return data_loader\n\n\ndef toy():\n    offset = 2\n    feats_real = np.random.rand(10).reshape(-1, 1)\n    feats_fake = np.random.rand(10).reshape(-1, 1) + offset\n    feats_real[0] = offset\n    feats_fake[0] = 1\n    print(\"real:\", feats_real)\n    print(\"fake:\", feats_fake)\n\n    print(\"computing pairwise distances...\")\n    distances_real = compute_pairwise_distances(feats_real)\n    print(\"distances to radii...\")\n    radii_real = distances2radii(distances_real)\n    manifold_real = Manifold(feats_real, radii_real)\n\n    print(\"computing pairwise distances...\")\n    distances_fake = compute_pairwise_distances(feats_fake)\n    print(\"distances to radii...\")\n    radii_fake = distances2radii(distances_fake)\n    manifold_fake = Manifold(feats_fake, radii_fake)\n\n    precision = compute_metric(manifold_real, feats_fake)\n    recall = compute_metric(manifold_fake, feats_real)\n    print(\"precision:\", precision)\n    print(\"recall:\", recall)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:15.367351Z","iopub.execute_input":"2024-11-12T15:15:15.368025Z","iopub.status.idle":"2024-11-12T15:15:18.111609Z","shell.execute_reply.started":"2024-11-12T15:15:15.367981Z","shell.execute_reply":"2024-11-12T15:15:18.110614Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# SAN","metadata":{}},{"cell_type":"code","source":"!pip3 install torch torchview tqdm matplotlib pytorch-fid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:18.113639Z","iopub.execute_input":"2024-11-12T15:15:18.114131Z","iopub.status.idle":"2024-11-12T15:15:29.847717Z","shell.execute_reply.started":"2024-11-12T15:15:18.114084Z","shell.execute_reply":"2024-11-12T15:15:29.846521Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchview in /opt/conda/lib/python3.10/site-packages (0.2.6)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: pytorch-fid in /opt/conda/lib/python3.10/site-packages (0.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pytorch-fid) (1.14.1)\nRequirement already satisfied: torchvision>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-fid) (0.19.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Generator","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np \nfrom torch.nn.utils import spectral_norm\n\nclass Generator(nn.Module):\n    def __init__(self, g_output_dim=784, dim_latent=100):\n        super(Generator, self).__init__()\n        self.fc1 = nn.Linear(100, 256)\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        return torch.tanh(self.fc4(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:29.849389Z","iopub.execute_input":"2024-11-12T15:15:29.849767Z","iopub.status.idle":"2024-11-12T15:15:29.858521Z","shell.execute_reply.started":"2024-11-12T15:15:29.849729Z","shell.execute_reply":"2024-11-12T15:15:29.857537Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Discriminator","metadata":{}},{"cell_type":"code","source":"class BaseDiscriminator(nn.Module):\n    def __init__(self, d_input_dim=784):\n        super(BaseDiscriminator, self).__init__()\n        self.h_function = nn.Sequential(\n            #nn.Linear(d_input_dim, 512),\n            spectral_norm(nn.Linear(d_input_dim, 1024)),\n            nn.LeakyReLU(0.2),\n            #nn.Linear(1024, 512),\n            #nn.LeakyReLU(0.2),\n            #nn.Linear(512, 256),\n            spectral_norm(nn.Linear(1024, 512)),\n            nn.LeakyReLU(0.2),\n            #nn.Linear(256, 1)\n            spectral_norm(nn.Linear(512, 256)),\n            nn.LeakyReLU(0.2),\n        )\n\n        self.fc_w = nn.Parameter(torch.randn(1, 256))\n\n    def forward(self, x, flg_train: bool):        \n        h_feature = self.h_function(x)\n        weights = self.fc_w\n        out = (h_feature * weights).sum(dim=1)\n        return out\n\nclass SanDiscriminator(BaseDiscriminator):\n    def __init__(self, d_input_dim=784):\n        super(SanDiscriminator, self).__init__(d_input_dim)\n\n    def forward(self, x, flg_train: bool):\n        h_feature = self.h_function(x)        \n        weights = self.fc_w\n        direction = F.normalize(weights, dim=1)  # Normalize the last layer\n        scale = torch.norm(weights, dim=1).unsqueeze(1)\n        h_feature = h_feature * scale  # Keep the scale\n        if flg_train:  # For discriminator training\n            out_fun = (h_feature * direction.detach()).sum(dim=1)\n            out_dir = (h_feature.detach() * direction).sum(dim=1)\n            out = dict(fun=out_fun, dir=out_dir)\n        else:  # For generator training or inference\n            out = (h_feature * direction).sum(dim=1)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:29.86084Z","iopub.execute_input":"2024-11-12T15:15:29.861136Z","iopub.status.idle":"2024-11-12T15:15:29.872966Z","shell.execute_reply.started":"2024-11-12T15:15:29.861104Z","shell.execute_reply":"2024-11-12T15:15:29.872072Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torchview import draw_graph\n\n# Define input dimensions and model\nd_input_dim = 784\nsample_model = SanDiscriminator(d_input_dim=d_input_dim)\n\n# Create a dummy input tensor with the appropriate input dimensions\ndummy_input = torch.randn(1, d_input_dim)\n\n# Generate the computational graph\n# Set `flg_train` as `True` for training visualization or `False` for inference visualization\ngraph = draw_graph(sample_model, input_data=[dummy_input, True], expand_nested=True)\n\ngraph.visual_graph\ngraph.visual_graph.format = \"png\"\ngraph.visual_graph.render(\"SanDiscriminator_Architecture\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:29.874198Z","iopub.execute_input":"2024-11-12T15:15:29.874584Z","iopub.status.idle":"2024-11-12T15:15:30.257328Z","shell.execute_reply.started":"2024-11-12T15:15:29.87454Z","shell.execute_reply":"2024-11-12T15:15:30.256434Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'SanDiscriminator_Architecture.png'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import argparse\nimport json\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport tqdm\n\nfrom torch.utils.data import DataLoader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:30.258425Z","iopub.execute_input":"2024-11-12T15:15:30.258792Z","iopub.status.idle":"2024-11-12T15:15:30.265167Z","shell.execute_reply.started":"2024-11-12T15:15:30.258757Z","shell.execute_reply":"2024-11-12T15:15:30.264223Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nimport json\n\n# Define folder paths\nmnist_folder = './data/MNIST'\nlogs_folder = './logs'\nparams_path = './hparams/params.json'\n\n# Create folders if they don't exist\nos.makedirs(mnist_folder, exist_ok=True)\nos.makedirs(logs_folder, exist_ok=True)\nos.makedirs(os.path.dirname(params_path), exist_ok=True)\n\n# Define the parameters for params.json\nparams = {\n    \"dim_latent\": 100,\n    \"batch_size\": 128,\n    \"learning_rate_d\": 0.0001,\n    \"learning_rate_g\": 0.0007,    \n    \"beta_1\": 0.5,\n    \"beta_2\": 0.999,\n    \"num_epochs\": 150\n}\n\n# Write parameters to params.json\nwith open(params_path, 'w') as f:\n    json.dump(params, f, indent=4)\n\nprint(f\"Created folders '{mnist_folder}' and '{logs_folder}', and saved parameters to '{params_path}'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:30.266366Z","iopub.execute_input":"2024-11-12T15:15:30.266732Z","iopub.status.idle":"2024-11-12T15:15:30.277217Z","shell.execute_reply.started":"2024-11-12T15:15:30.266698Z","shell.execute_reply":"2024-11-12T15:15:30.276185Z"}},"outputs":[{"name":"stdout","text":"Created folders './data/MNIST' and './logs', and saved parameters to './hparams/params.json'.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"default_args = {\n    \"datadir\": \"./data/MNIST\",\n    \"params\": \"./hparams/params.json\",\n    \"model\": \"san\",\n    \"enable_class\": False,\n    \"logdir\": \"./logs\",\n    \"device\": 0\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:30.278393Z","iopub.execute_input":"2024-11-12T15:15:30.278773Z","iopub.status.idle":"2024-11-12T15:15:30.286004Z","shell.execute_reply.started":"2024-11-12T15:15:30.27874Z","shell.execute_reply":"2024-11-12T15:15:30.285064Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from argparse import Namespace\nargs = Namespace(**default_args)\n\nprint(args.datadir)        # Access './data/MNIST'\nprint(args.params)         # Access './hparams/params.json'\nprint(args.model)          # Access 'gan'\nprint(args.enable_class)   # Access False\nprint(args.logdir)         # Access './logs'\nprint(args.device)         # Access 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:30.287082Z","iopub.execute_input":"2024-11-12T15:15:30.287392Z","iopub.status.idle":"2024-11-12T15:15:30.297417Z","shell.execute_reply.started":"2024-11-12T15:15:30.287344Z","shell.execute_reply":"2024-11-12T15:15:30.296563Z"}},"outputs":[{"name":"stdout","text":"./data/MNIST\n./hparams/params.json\nsan\nFalse\n./logs\n0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Train functions\n\ndef update_discriminator(x, discriminator, generator, optimizer, params):\n    bs = x.size(0)\n    device = x.device\n\n    optimizer.zero_grad()\n\n    # for data (ground-truth) distribution\n    disc_real = discriminator(x, flg_train=True)\n    loss_real = eval('compute_loss_'+args.model)(disc_real, loss_type='real')\n\n    # for generator distribution\n    latent = torch.randn(bs, params[\"dim_latent\"], device=device)\n    img_fake = generator(latent)\n    disc_fake = discriminator(img_fake.detach(), flg_train=True)\n    loss_fake = eval('compute_loss_'+args.model)(disc_fake, loss_type='fake')\n\n\n    loss_d = loss_real + loss_fake\n    loss_d.backward()\n    optimizer.step()\n    \n    return loss_real, loss_fake\n\ndef update_generator(discriminator, generator, optimizer, params, device):\n    optimizer.zero_grad()\n\n    bs = params['batch_size']\n    latent = torch.randn(bs, params[\"dim_latent\"], device=device)\n\n    batch_fake = generator(latent)\n\n    disc_gen = discriminator(batch_fake, flg_train=False)\n    loss_g = - disc_gen.mean()\n    loss_g.backward()\n    optimizer.step()\n\n    if torch.isnan(loss_g).any():\n        print(\"NaN detected in generator loss!\")\n        return loss_g\n    return loss_g\n    \n\ndef compute_loss_gan(disc, loss_type):\n    assert (loss_type in ['real', 'fake'])\n    if 'real' == loss_type:\n        loss = (1. - disc).relu().mean() # Hinge loss\n    else: # 'fake' == loss_type\n        loss = (1. + disc).relu().mean() # Hinge loss\n\n    return loss\n\n\ndef compute_loss_san(disc, loss_type):\n    assert (loss_type in ['real', 'fake'])\n    if 'real' == loss_type:\n        loss_fun = (1. - disc['fun']).relu().mean() # Hinge loss for function h\n        loss_dir = - disc['dir'].mean() # Wasserstein loss for omega\n    else: # 'fake' == loss_type\n        loss_fun = (1. + disc['fun']).relu().mean() # Hinge loss for function h\n        loss_dir = disc['dir'].mean() # Wasserstein loss for omega\n\n    lambd = 5\n    loss = loss_fun + lambd * loss_dir\n\n    return loss\n\n\ndef save_images(imgs, idx, dirname='test'):\n    # Ensure imgs is a numpy array if it's a tensor\n    if isinstance(imgs, torch.Tensor):\n        imgs = imgs.cpu().data.numpy()\n\n    # If the images are grayscale (1 channel), repeat them to make them RGB (3 channels)\n    if imgs.shape[1] == 1:  # This checks if there is only 1 channel (grayscale)\n        imgs = np.repeat(imgs, 3, axis=1)  # Repeat the grayscale channel 3 times to make RGB\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(f'out/{dirname}/'):\n        os.makedirs(f'out/{dirname}/')\n\n    # Set up the plot\n    fig = plt.figure(figsize=(10, 10))\n    gs = gridspec.GridSpec(10, 10)\n    gs.update(wspace=0.05, hspace=0.05)\n\n    # Loop through the batch of images\n    for i, sample in enumerate(imgs):\n        ax = plt.subplot(gs[i])\n        plt.axis('off')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_aspect('equal')\n\n        # Ensure each sample has the shape (height, width, channels) for imshow\n        sample = sample.transpose((1, 2, 0))  # Convert from (C, H, W) -> (H, W, C)\n\n        # Plot the image\n        plt.imshow(sample)\n\n    # Save the plot to the directory\n    plt.savefig(f'out/{dirname}/{str(idx).zfill(3)}.png', bbox_inches=\"tight\")\n    plt.close(fig)\n\ndef plot_generated_images(generator, epoch, num_images=20):\n    \"\"\"Function to plot and display generated images from the generator.\"\"\"\n    generator.eval()\n    with torch.no_grad():\n        noise = torch.randn(num_images, 100).to(device)\n        generated_images = generator(noise)\n        generated_images = generated_images.view(-1, 1, 28, 28).cpu()\n\n        # Create two rows of 10 images\n        fig, axes = plt.subplots(2, 10, figsize=(15, 6))\n        axes = axes.flatten()  # Flatten the axes array for easy iteration\n        for ax, img in zip(axes, generated_images):\n            ax.imshow(img.squeeze(), cmap='gray')\n            ax.axis('off')\n        plt.tight_layout()\n        plt.show()\n\ndef plot_losses(G_losses, D_real_losses, D_fake_losses):\n    G_losses = [loss.detach().cpu().numpy() for loss in G_losses]\n    D_real_losses = [loss.detach().cpu().numpy() for loss in D_real_losses]\n    D_fake_losses = [loss.detach().cpu().numpy() for loss in D_fake_losses]\n    \n    plt.figure(figsize=(10, 5))\n    plt.plot(G_losses, label=\"Generator Loss\")\n    plt.plot(D_real_losses, label=\"Discriminator Real Loss\")\n    plt.plot(D_fake_losses, label=\"Discriminator Fake Loss\")\n    \n    plt.title(\"Generator and Discriminator Losses Over Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    \n    plt.ylim(-5, 5)\n    plt.legend()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:30.301096Z","iopub.execute_input":"2024-11-12T15:15:30.301707Z","iopub.status.idle":"2024-11-12T15:15:30.327665Z","shell.execute_reply.started":"2024-11-12T15:15:30.301673Z","shell.execute_reply":"2024-11-12T15:15:30.32682Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = f'cuda:{args.device}' if args.device is not None else 'cpu'\nmodel_name = args.model\nprint(model_name)\nif not model_name in ['gan', 'san']:\n    raise RuntimeError(\"A model name have to be 'gan' or 'san'.\")\n    \nexperiment_name = model_name + \"_cond\" if args.enable_class else model_name\n\ntransform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.5,), std=(0.5,))])\n\n# dataloading\nnum_class = 10\ntrain_dataset = datasets.MNIST(root=args.datadir, transform=transform, train=True, download=True)\ntrain_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], num_workers=8,\n                          pin_memory=True, persistent_workers=True, shuffle=True)\n\ntest_dataset = datasets.MNIST(root=args.datadir, transform=transform, train=False, download=True)\ntest_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], num_workers=8,\n                         pin_memory=True, persistent_workers=True, shuffle=False)\n\n# model\ngenerator = Generator(g_output_dim=784)\n\nif 'gan' == args.model:\n    discriminator = BaseDiscriminator(d_input_dim=784)\nelse: # 'san' == args.model\n    discriminator = SanDiscriminator(d_input_dim=784)\ngenerator = generator.to(device)\ndiscriminator = discriminator.to(device)\n\n# optimizer\nbetas = (params[\"beta_1\"], params[\"beta_2\"])\noptimizer_G = optim.Adam(generator.parameters(), lr=params[\"learning_rate_g\"], betas=betas)\noptimizer_D = optim.Adam(discriminator.parameters(), lr=params[\"learning_rate_d\"], betas=betas)\n\nckpt_dir = f'{args.logdir}/{experiment_name}/'\nif not os.path.exists(args.logdir):\n    os.mkdir(args.logdir)\nif not os.path.exists(ckpt_dir):\n    os.mkdir(ckpt_dir)\n\nsteps_per_epoch = len(train_loader)\n\nmsg = [\"\\t{0}: {1}\".format(key, val) for key, val in params.items()]\nprint(\"hyperparameters: \\n\" + \"\\n\".join(msg))\n\n# eval initial states\nnum_samples_per_class = 10\nwith torch.no_grad():\n    latent = torch.randn(num_samples_per_class * num_class, params[\"dim_latent\"]).cuda()\n    imgs_fake = generator(latent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:30.328777Z","iopub.execute_input":"2024-11-12T15:15:30.329067Z","iopub.status.idle":"2024-11-12T15:15:30.465066Z","shell.execute_reply.started":"2024-11-12T15:15:30.329036Z","shell.execute_reply":"2024-11-12T15:15:30.464143Z"}},"outputs":[{"name":"stdout","text":"san\nhyperparameters: \n\tdim_latent: 100\n\tbatch_size: 128\n\tlearning_rate_d: 0.0001\n\tlearning_rate_g: 0.0007\n\tbeta_1: 0.5\n\tbeta_2: 0.999\n\tnum_epochs: 150\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Save test images from test data loader\n","metadata":{}},{"cell_type":"code","source":"# Directory to save images\nmnist_image_dir = 'MNIST_images'\nos.makedirs(mnist_image_dir, exist_ok=True)\n\n# Define the inverse transform to unnormalize the images\ninv_transform = transforms.Compose([\n    transforms.Normalize(mean=[-1.0], std=[2.0])  # Undo the normalization (0.5 mean, 0.5 std)\n])\n\n# Save each image\nfor i, (image, _) in enumerate(test_dataset):\n    # Inverse normalize and convert to PIL format\n    image = inv_transform(image)\n    image = transforms.ToPILImage()(image)\n    \n    # Save as PNG\n    image.save(os.path.join(mnist_image_dir, f\"image_{i}.png\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:30.466463Z","iopub.execute_input":"2024-11-12T15:15:30.467074Z","iopub.status.idle":"2024-11-12T15:15:37.72495Z","shell.execute_reply.started":"2024-11-12T15:15:30.467036Z","shell.execute_reply":"2024-11-12T15:15:37.724135Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!ls -1 MNIST_images | wc -l","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:37.726148Z","iopub.execute_input":"2024-11-12T15:15:37.727029Z","iopub.status.idle":"2024-11-12T15:15:38.745589Z","shell.execute_reply.started":"2024-11-12T15:15:37.726982Z","shell.execute_reply":"2024-11-12T15:15:38.74441Z"}},"outputs":[{"name":"stdout","text":"10000\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Functions to evaluate PR and FID","metadata":{}},{"cell_type":"code","source":"import torchvision\nimport shutil\n\ndef generate_samples(generator):\n    directory = '/kaggle/working/samples'\n    # Remove the directory and its contents\n    shutil.rmtree(directory, ignore_errors=True)\n    \n    model = torch.nn.DataParallel(generator).cuda()\n    model.eval()\n    \n    print('Model loaded.')\n    \n    \n    print('Start Generating')\n    os.makedirs('samples', exist_ok=True)\n    \n    image_paths = []\n    batch_size = params[\"batch_size\"]\n    dim_latent = params[\"dim_latent\"]\n    \n    n_samples = 0\n    with torch.no_grad():\n        while n_samples<10000:\n            z = torch.randn(batch_size, dim_latent).cuda()\n            x = model(z)\n            x = x.reshape(batch_size, 28, 28)\n            for k in range(x.shape[0]):\n                if n_samples<10000:\n                    image_path = os.path.join('samples', f'{n_samples}.png')\n                    torchvision.utils.save_image(x[k:k+1], image_path)         \n                    image_paths.append(image_path)  # Store image path\n                    n_samples += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:38.747197Z","iopub.execute_input":"2024-11-12T15:15:38.747596Z","iopub.status.idle":"2024-11-12T15:15:38.757246Z","shell.execute_reply.started":"2024-11-12T15:15:38.747557Z","shell.execute_reply":"2024-11-12T15:15:38.75618Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import subprocess\nfrom pathlib import Path\nfrom pytorch_fid import fid_score\n\nipr = IPR(batch_size=50, k=3, num_samples=10000)\n\ndef eval_model(generator):\n    # Generate the samples\n    generate_samples(generator)\n    real_images_dir = '/kaggle/working/MNIST_images'\n    fake_images_dir = '/kaggle/working/samples'\n\n    # Calculate precision and recall\n    ipr.compute_manifold_ref(real_images_dir)\n    precision, recall = ipr.precision_and_recall(fake_images_dir)\n\n    print(\"Precision:\", precision)\n    print(\"Recall:\", recall)\n\n    # Set parameters\n    batch_size = 256  # Adjust batch size based on available memory\n    dims = 2048      # Default dimension for pool3 layer in Inception model\n    device = 'cuda'\n    \n    # Calculate FID score\n    fid_value = fid_score.calculate_fid_given_paths([real_images_dir, fake_images_dir], batch_size, device, dims)\n\n    print(\"FID score:\", fid_value)\n    \n    return precision, recall, fid_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:38.759169Z","iopub.execute_input":"2024-11-12T15:15:38.759562Z","iopub.status.idle":"2024-11-12T15:15:40.546964Z","shell.execute_reply.started":"2024-11-12T15:15:38.759516Z","shell.execute_reply":"2024-11-12T15:15:40.546013Z"}},"outputs":[{"name":"stdout","text":"loading vgg16 for improved precision and recall...","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_precision_recall_fid(precision_scores, recall_scores, fid_scores):\n    epochs = range(1, len(precision_scores) + 1)\n    \n    plt.figure(figsize=(15, 5))\n    \n    # Plot Precision\n    plt.subplot(1, 3, 1)\n    plt.plot(epochs, precision_scores, label=\"Precision\", marker='o')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Precision\")\n    plt.title(\"Precision over Epochs\")\n    plt.grid(True)\n    \n    # Plot Recall\n    plt.subplot(1, 3, 2)\n    plt.plot(epochs, recall_scores, label=\"Recall\", marker='o')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Recall\")\n    plt.title(\"Recall over Epochs\")\n    plt.grid(True)\n    \n    # Plot FID Score\n    plt.subplot(1, 3, 3)\n    plt.plot(epochs, fid_scores, label=\"FID\", marker='o')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"FID Score\")\n    plt.title(\"FID Score over Epochs\")\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:40.548298Z","iopub.execute_input":"2024-11-12T15:15:40.549157Z","iopub.status.idle":"2024-11-12T15:15:40.557754Z","shell.execute_reply.started":"2024-11-12T15:15:40.549102Z","shell.execute_reply":"2024-11-12T15:15:40.556739Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:40.55873Z","iopub.execute_input":"2024-11-12T15:15:40.559018Z","iopub.status.idle":"2024-11-12T15:15:40.571298Z","shell.execute_reply.started":"2024-11-12T15:15:40.558986Z","shell.execute_reply":"2024-11-12T15:15:40.570352Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"G_losses = []\nD_real_losses = []\nD_fake_losses = [] \nprecision_scores = []\nrecall_scores = []\nfid_scores = []\n\n# main training loop\nfor n in range(params[\"num_epochs\"]):\n    loader = iter(train_loader)\n\n    print(\"epoch: {0}/{1}\".format(n + 1, params[\"num_epochs\"]))\n    G_loss_epoch = 0\n    D_real_loss_epoch = 0\n    D_fake_loss_epoch = 0\n\n    for i in trange(steps_per_epoch):\n        x, class_ids = next(loader)\n        x = x.to(device)\n        x = x.view(x.size(0), -1)\n\n        loss_real, loss_fake = update_discriminator(x, discriminator, generator, optimizer_D, params)\n        G_loss = update_generator(discriminator, generator, optimizer_G, params, device)\n\n        D_real_loss_epoch += loss_real\n        D_fake_loss_epoch += loss_fake\n        G_loss_epoch += G_loss\n\n    if (n % 20) == 0:\n        precision, recall, fid_value = eval_model(generator)\n        precision_scores.append(precision)\n        recall_scores.append(recall)\n        fid_scores.append(fid_value)\n        plot_precision_recall_fid(precision_scores, recall_scores, fid_scores)\n\n    # Store the average loss per epoch\n    print(\"D-real:\", D_real_loss_epoch / len(train_loader))\n    print(\"D-fake:\", D_fake_loss_epoch / len(train_loader))\n    print(\"G-fake:\", G_loss_epoch / len(train_loader))\n    \n    D_real_losses.append(D_real_loss_epoch / len(train_loader))\n    D_fake_losses.append(D_fake_loss_epoch / len(train_loader))\n    \n    G_losses.append(G_loss_epoch / len(train_loader))\n    \n\n    plot_generated_images(generator, n+1)\n    \n    torch.save(generator.state_dict(), ckpt_dir + \"g.\" + str(n) + \".tmp\")\n    torch.save(discriminator.state_dict(), ckpt_dir + \"d.\" + str(n) + \".tmp\")\n\n\n    # eval\n    with torch.no_grad():\n        latent = torch.randn(num_samples_per_class * num_class, params[\"dim_latent\"]).cuda()\n        imgs_fake = generator(latent).cpu().data.numpy()\n        imgs_fake = imgs_fake.reshape(-1, 1, 28, 28)\n        save_images(imgs_fake, n, dirname=experiment_name)\n\n    plot_losses(G_losses, D_real_losses, D_fake_losses)\n\ntorch.save(generator.state_dict(), ckpt_dir + \"generator.pt\")\ntorch.save(discriminator.state_dict(), ckpt_dir + \"discriminator.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:15:40.572509Z","iopub.execute_input":"2024-11-12T15:15:40.572865Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"epoch: 1/150\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 469/469 [00:09<00:00, 50.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Model loaded.\nStart Generating\n","output_type":"stream"},{"name":"stderr","text":"extracting features of 10000 images: 100%|██████████| 200/200 [00:27<00:00,  7.18it/s]\nextracting features of 10000 images:  19%|█▉        | 38/200 [00:05<00:21,  7.38it/s]","output_type":"stream"}],"execution_count":null}]}