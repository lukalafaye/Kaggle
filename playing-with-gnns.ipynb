{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12624079,"sourceType":"datasetVersion","datasetId":7974781}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook provides an implementation for the paper EFFICIENT AND SCALABLE GRAPH GENERATION THROUGH ITERATIVE LOCAL EXPANSION: [https://arxiv.org/pdf/2312.11529](https://arxiv.org/pdf/2312.11529)","metadata":{}},{"cell_type":"code","source":"!pip3 install -q torch_geometric\n!pip3 install -q diffusers","metadata":{"_uuid":"61bd1e12-3a28-4fea-8958-826f0001255e","_cell_guid":"bfcfac55-6772-462a-aaf3-138fc1f58b11","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:10.516986Z","iopub.execute_input":"2025-08-01T14:32:10.517285Z","iopub.status.idle":"2025-08-01T14:32:18.568502Z","shell.execute_reply.started":"2025-08-01T14:32:10.517265Z","shell.execute_reply":"2025-08-01T14:32:18.566941Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":189},{"cell_type":"markdown","source":"# Import required librairies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport math\nfrom pprint import pprint\n\nimport torch\nfrom torch.nn import Dropout, LayerNorm, Linear, Module, ModuleList\nfrom torch.utils.data import IterableDataset\nfrom torch.utils.data import DataLoader as torchDataLoader\nimport torch.nn.functional as F\nfrom torch_geometric.utils import get_laplacian\n\nfrom torch_geometric.nn import \nfrom torch_geometric.data import Data, Batch, DataLoader, InMemoryDataset, download_url \nfrom torch_geometric.utils import erdos_renyi_graph, to_dense_adj, dense_to_sparse, to_networkx, from_networkx\nfrom torch_geometric.nn import GCNConv, GINConv\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport os\n\nfrom diffusers import DDPMScheduler","metadata":{"_uuid":"427e1779-e73e-4477-895e-526e1befd407","_cell_guid":"37b845cc-2ebc-49ee-89b9-32a9c821864a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.570937Z","iopub.execute_input":"2025-08-01T14:32:18.571345Z","iopub.status.idle":"2025-08-01T14:32:18.586089Z","shell.execute_reply.started":"2025-08-01T14:32:18.571306Z","shell.execute_reply":"2025-08-01T14:32:18.580210Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_35/2554226630.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    from torch_geometric.nn import\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2554226630.py, line 12)","output_type":"error"}],"execution_count":190},{"cell_type":"code","source":"print(\"torch_geometric version:\", torch_geometric.__version__)\nprint(\"torch version:\", torch.__version__)","metadata":{"_uuid":"6758ddb3-538b-4461-936d-c7dba6ceea55","_cell_guid":"e7691bee-03ab-44e1-a19a-4beda04af08d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.586685Z","iopub.status.idle":"2025-08-01T14:32:18.587335Z","shell.execute_reply.started":"2025-08-01T14:32:18.587151Z","shell.execute_reply":"2025-08-01T14:32:18.587172Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating datasets for training","metadata":{"_uuid":"bd440016-0a8f-4152-8c85-ee7622b846f6","_cell_guid":"2850c34a-b4dd-44ce-b0c0-6cf369f021d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Erdos renyi dataset","metadata":{"_uuid":"0cdf5d69-5dfd-430d-8877-ca309d20dc06","_cell_guid":"378d4133-7a74-4f1f-ac5e-4ea21d0faca7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def generate_erdos_renyi_graphData(num_nodes=5, p=0.6):\n    edge_tensor = erdos_renyi_graph(num_nodes, p, directed=False)  # shape [2, num_edges]\n\n    # Create graph nodes, values all 0\n    nodes = torch.zeros((num_nodes, 1)) # shape [num_nodes, 1], any node feature is a single scalar 0.\n    # each node in this tensor is assigned an id based on its index 0...num_nodes-1 \n    # these ids are the ones used in edge_tensor, it is possible that there are no edges... \n    \n    # Create Data object for graph\n    graphData = Data(x=nodes, edge_index=edge_tensor)\n    \n    return graphData\n\ngraphDataset = generate_erdos_renyi_graphData()\n# G = to_networkx(graph, to_undirected=True)\n# plt.figure()\n# nx.draw(G, with_labels=True, node_size=500)\n# plt.show()","metadata":{"_uuid":"aa1ad950-7259-4f51-b87a-176c4d0be256","_cell_guid":"383cf5da-3709-4b97-9d3e-d2d9b3046731","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.589766Z","iopub.status.idle":"2025-08-01T14:32:18.590130Z","shell.execute_reply.started":"2025-08-01T14:32:18.589992Z","shell.execute_reply":"2025-08-01T14:32:18.590006Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inspiration from https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html#creating-in-memory-datasets\n\nclass ErdosRenyiDataset(InMemoryDataset):\n    def __init__(self, root, num_graphs=100, transform=None, pre_transform=None, pre_filter=None):\n        self.num_graphs = num_graphs\n\n        super().__init__(root, transform, pre_transform, pre_filter)\n\n        # Generate dataset if not done already, else load it\n        processed_path = self.processed_paths[0]\n        if not os.path.exists(processed_path):\n            print(\"Creating dataset\")\n            self.process()\n        else:\n            print(\"Dataset already exists, loading it...\")\n\n        self.load(self.processed_paths[0]) # populates self.data\n        print(\"Done.\")\n\n        \"\"\"\n        self.load will create objects:\n        \n        self.slices = {\n          'x':          tensor([0, 2, 5, 6]),   # node feature offsets\n          'edge_index': tensor([0, 1, 3, 4]),   # edge index offsets\n        }\n\n        self.data = {\n          'x':          (total_num_nodes, num_node_features) # concatenation of all graphs nodes\n          'edge_index': (2, total_num_edges) # concatenation of all graph edges\n        }\n        \"\"\"\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def process(self):\n        data_list = []\n\n        for i in range(self.num_graphs):\n            data_list.append(generate_erdos_renyi_graphData())\n\n        if self.pre_filter:\n            data_list = [d for d in data_list if self.pre_filter(d)]\n            \n        if self.pre_transform:\n            data_list = [self.pre_transform(d) for d in data_list]\n\n        self.save(data_list, self.processed_paths[0]) # Runs: data, slices = cls.collate(data_list) and saves these objects","metadata":{"_uuid":"e03d14d2-cbb4-4118-b8fc-7715d3df0d82","_cell_guid":"553dc655-4da9-4490-8e4c-d9c62d79a681","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.591639Z","iopub.status.idle":"2025-08-01T14:32:18.592124Z","shell.execute_reply.started":"2025-08-01T14:32:18.591890Z","shell.execute_reply":"2025-08-01T14:32:18.591913Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/data/ER/processed\nerdos_renyi_dataset = ErdosRenyiDataset(root='data/ER', num_graphs=100)","metadata":{"_uuid":"9ee6da24-7aaa-4727-93e7-4dca384117e7","_cell_guid":"41f21cb5-f9e7-498c-88e1-6ddcde306e06","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.594965Z","iopub.status.idle":"2025-08-01T14:32:18.595934Z","shell.execute_reply.started":"2025-08-01T14:32:18.595679Z","shell.execute_reply":"2025-08-01T14:32:18.595704Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of graphs: {len(erdos_renyi_dataset)}\")\n\nfor g in range(4):\n    edge_index = erdos_renyi_dataset[g].edge_index","metadata":{"_uuid":"2b342a41-cf97-4624-a98f-c726ef909515","_cell_guid":"450d6c28-759f-418a-bf06-0db32d88e730","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.597304Z","iopub.status.idle":"2025-08-01T14:32:18.597922Z","shell.execute_reply.started":"2025-08-01T14:32:18.597674Z","shell.execute_reply":"2025-08-01T14:32:18.597700Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"edge_index[:, :1]","metadata":{"_uuid":"96ae00aa-aa4b-4d75-84b8-bf9fa2d42d98","_cell_guid":"113e4652-9a9e-4ac7-804a-9e6d2ddba079","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.601127Z","iopub.status.idle":"2025-08-01T14:32:18.603113Z","shell.execute_reply.started":"2025-08-01T14:32:18.602821Z","shell.execute_reply":"2025-08-01T14:32:18.602847Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Planar graph dataset (generated with plantri)","metadata":{"_uuid":"8ef31986-d784-49a8-afe8-ec518ed5efaa","_cell_guid":"42370f7e-4f12-4c88-a233-22caf546f008","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Command             | Triangulations Written | CPU Time (sec)\n--------------------|------------------------|---------------\n./plantri -c3 10    | 233                    | 0.00\n./plantri -c3 11    | 1,249                  | 0.02\n./plantri -c3 12    | 7,595                  | 0.04\n./plantri -c3 13    | 49,566                 | 0.16\n./plantri -c3 14    | 339,722                | 1.11\n./plantri -c3 15    | 2,406,841              | 8.01\n./plantri -c3 16    | 17,490,241             | 61.98\n./plantri -c3 17    | 129,664,753            | 504.53","metadata":{"_uuid":"bfb95a11-0606-4778-8232-a23668860e2b","_cell_guid":"e5af35de-b882-4ce3-aa80-0981efcc2638","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import networkx as nx\n\ndef graph6_to_data(g6_path):\n    print(f\"Loading graphs from {g6_path}\")\n    \n    with open(g6_path, \"rb\") as f:\n        graph_nx = nx.read_graph6(f)\n\n    print(f\"Read {len(graphs_nx)} graphs from {g6_path}\")\n    return graphs_nx","metadata":{"_uuid":"6b89a6fb-71c8-46ce-9762-38fc324607cf","_cell_guid":"97c3cba3-a8f7-4bb2-8c6c-34c1ebed205b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.604457Z","iopub.status.idle":"2025-08-01T14:32:18.604915Z","shell.execute_reply.started":"2025-08-01T14:32:18.604683Z","shell.execute_reply":"2025-08-01T14:32:18.604703Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inspiration from https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html#creating-in-memory-datasets\n\nclass PlanarDataset(InMemoryDataset):\n    def __init__(self, root, g6_dir=None, transform=None, pre_transform=None, pre_filter=None):\n\n        g6_files = [os.path.join(g6_dir, f) for f in os.listdir(g6_dir) if f.endswith(\".g6\")]\n        self.g6_files = g6_files\n        \n        super().__init__(root, transform, pre_transform, pre_filter)\n\n        # Generate dataset if not done already, else load it\n        processed_path = self.processed_paths[0]\n        if not os.path.exists(processed_path):\n            print(\"Creating dataset\")\n            self.process()\n        else:\n            print(\"Dataset already exists, loading it...\")\n\n        self.load(self.processed_paths[0]) # populates self.data\n        print(\"Done.\")\n\n        \"\"\"\n        self.load will create objects:\n        \n        self.slices = {\n          'x':          tensor([0, 2, 5, 6]),   # node feature offsets\n          'edge_index': tensor([0, 1, 3, 4]),   # edge index offsets\n        }\n\n        self.data = {\n          'x':          (total_num_nodes, num_node_features) # concatenation of all graphs nodes\n          'edge_index': (2, total_num_edges) # concatenation of all graph edges\n        }\n        \"\"\"\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def process(self):\n        data_list = []\n\n        for file in self.g6_files:\n            nx_graphs = graph6_to_data(file)\n            data_list += [from_networkx(g) for g in nx_graphs]\n\n        if self.pre_filter:\n            data_list = [d for d in data_list if self.pre_filter(d)]\n            \n        if self.pre_transform:\n            data_list = [self.pre_transform(d) for d in data_list]\n\n        self.save(data_list, self.processed_paths[0]) # Runs: data, slices = cls.collate(data_list) and saves these objects","metadata":{"_uuid":"50b156a1-9204-4113-89b5-75b0fb8b40c5","_cell_guid":"ea5d0e8a-245b-4a2b-9c2e-282774968a79","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.606152Z","iopub.status.idle":"2025-08-01T14:32:18.606553Z","shell.execute_reply.started":"2025-08-01T14:32:18.606354Z","shell.execute_reply":"2025-08-01T14:32:18.606374Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#planar_dataset = PlanarDataset(root='data/PL', g6_dir=\"/kaggle/input/planar-graphs-g6-dataset\")","metadata":{"_uuid":"c2b0dcf1-0fde-4d83-abe2-96f8769a1d01","_cell_guid":"b38241f9-c677-47b6-97fc-502693c4d701","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.610684Z","iopub.status.idle":"2025-08-01T14:32:18.611106Z","shell.execute_reply.started":"2025-08-01T14:32:18.610950Z","shell.execute_reply":"2025-08-01T14:32:18.610966Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Coarsening sampling","metadata":{"_uuid":"edc681b7-2d3e-4912-a93a-8a5edee8dec7","_cell_guid":"08f1c321-2a65-47f4-b35f-b05e09fc960b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"The below function creates a dense adjency matrix given a graph by looking at all of its edges.","metadata":{}},{"cell_type":"code","source":"def get_graph_adj(edge_index: torch.LongTensor, num_nodes: int):\n    adj = to_dense_adj(edge_index, max_num_nodes=num_nodes)\n    adj = adj[0]\n    return adj\n\n# test\ntest_graph_adj = get_graph_adj(test_graph.edge_index, test_graph.num_nodes)\n#indices = (graph_adj == 1).nonzero(as_tuple=False)\nprint(test_graph_adj)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.613180Z","iopub.status.idle":"2025-08-01T14:32:18.614005Z","shell.execute_reply.started":"2025-08-01T14:32:18.613435Z","shell.execute_reply":"2025-08-01T14:32:18.613723Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We need to sample coarsening sequences from $\\Pi_F(G)$ that only use contraction sets in $F(G) = \\mathcal{E}$, meaning each coarsening step only merges some edges of the graph. \n\nWe sample in $\\Pi_F(G)$ by sampling coarsening sequences from a \"latent\" distribution $q(\\pi | G)$. To sample from $q(\\pi | G)$, we associate a cost to each possible coarsening set at all coarsening steps, for now the uniform cost.","metadata":{}},{"cell_type":"markdown","source":"The below function extracts all possible coarsening sets at a given step (given a graph).","metadata":{}},{"cell_type":"code","source":"def get_candidate_contraction_sets(G: Data):\n    # make sure no self loop on nodes here!\n    edge_index = G.edge_index # 2, num_edges (tensor)\n    u, v       = edge_index[0], edge_index[1]\n\n    # remove self edges\n    nonself = (u != v)\n    u_ns, v_ns = u[nonself], v[nonself]\n\n    candidates = torch.stack([u_ns, v_ns], dim=1)\n    return candidates\n\n# test\ntest_graph = erdos_renyi_dataset[10]\nnx.draw(to_networkx(test_graph), with_labels=True)\n\ntest_candidates = get_candidate_contraction_sets(test_graph)\nprint(test_candidates[:10])\n\ntest_current_num_nodes = test_graph.num_nodes\nprint(\"Total number of nodes:\", test_current_num_nodes)","metadata":{"_uuid":"b68069bb-f9f0-4789-93cd-f984dbfeeaa0","_cell_guid":"b9f082c0-c8dd-4b3c-8c3b-ebf25a12aad1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.617148Z","iopub.status.idle":"2025-08-01T14:32:18.617607Z","shell.execute_reply.started":"2025-08-01T14:32:18.617365Z","shell.execute_reply":"2025-08-01T14:32:18.617380Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:red\">The uniform cost should be replaced by  local-variation cost of Loukas (2018) from the paper otherwise model won't train well!</span>","metadata":{}},{"cell_type":"code","source":"def get_uniform_cost(candidate_contractions: torch.LongTensor, graphData: Data): # can also be based on G0, P1...P{l-1}\n    # graphData not used, just used for compatibility\n    num_candidates = candidate_contractions.shape[0]\n\n    return torch.tensor([np.random.rand() for _ in range(num_candidates)], dtype=torch.float32)\n      \n# test\nget_uniform_cost(test_candidates, test_graph)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.619764Z","iopub.status.idle":"2025-08-01T14:32:18.621033Z","shell.execute_reply.started":"2025-08-01T14:32:18.620760Z","shell.execute_reply":"2025-08-01T14:32:18.620786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Better cost function from the paper, written with chatgpt","metadata":{}},{"cell_type":"code","source":"from torch.nn import Dropout, Linear, Module, ModuleList\nfrom torch_geometric.nn import GINConv\n\n\nclass SignNet(Module):\n    def __init__(\n        self,\n        num_eigenvectors: int,\n        hidden_features: int,\n        out_features: int,\n        num_layers: int,\n        dropout: float = 0.0,\n    ) -> None:\n        super().__init__()\n\n        self.in_layer = Linear(2, hidden_features)\n        self.conv_layers = ModuleList(\n            [\n                GINConv(\n                    MLP(hidden_features, [hidden_features, hidden_features]),\n                    train_eps=True,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        self.skip_layer = Linear(hidden_features * (num_layers + 1), hidden_features)\n        self.dropout = Dropout(dropout)\n        # the following corresponds to the ρ function in the paper\n        self.merge_layer = MLP(\n            in_features=num_eigenvectors * hidden_features,\n            hidden_features=[hidden_features, hidden_features],\n            out_features=out_features,\n        )\n\n    def forward(self, spectral_features, edge_index):\n        \"\"\"Forward pass of the model.\n\n        Args:\n            spectral_features (Tensor): Eigenvalues (repeated) concatenated with eigenvectors. Shape: :math:`(V, num_eigenvectors * 2)`.\n            edge_index (Adj): Adjacency matrix given as edge index or sparse tensor. Shape: :math:`(2, E)` or :math:`(V, V)`.\n\n        Returns:\n            Tensor: Node features. Shape: :math:`(V, out_features)`.\n        \"\"\"\n        # Stack spectral features\n        eigenvalues_repeated, eigenvectors = spectral_features.chunk(\n            2, dim=-1\n        )  # (V, k), (V, k)\n\n        positive_spectral_features = torch.stack(\n            [eigenvalues_repeated, eigenvectors], dim=-1\n        )  # V, k, 2\n        negative_spectral_features = torch.stack(\n            [eigenvalues_repeated, -eigenvectors], dim=-1\n        )  # V, k, 2\n        combined_spectral_features = torch.stack(\n            [positive_spectral_features, negative_spectral_features]\n        ).transpose(\n            1, 2\n        )  # 2, k, V, 2\n\n        # Apply layers\n        x = self.in_layer(combined_spectral_features)  # 2, k, V, hidden_features\n        xs = [x]\n        for conv in self.conv_layers:\n            # apply conv layer to each spectral feature independently\n            x = conv(x=x, edge_index=edge_index)\n            xs.append(x)\n\n        # Skip connection\n        x = torch.cat(xs, dim=-1)\n        x = self.dropout(x)\n        x = self.skip_layer(x)  # 2, k, V, hidden_features\n        # Make sign invariant\n        x = x.sum(dim=0)  # k, V, hidden_features\n\n        # Merge features\n        x = x.transpose(0, 1)  # V, k, hidden_features\n        x = self.merge_layer(x.reshape(x.size(0), -1))  # V, out_features\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.622020Z","iopub.status.idle":"2025-08-01T14:32:18.622355Z","shell.execute_reply.started":"2025-08-01T14:32:18.622191Z","shell.execute_reply":"2025-08-01T14:32:18.622219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_spectral_features_torch(data, k):\n    # Get Laplacian\n    lap_edge_index, lap_edge_weight = get_laplacian(data.edge_index, normalization='sym')\n    L_dense = \n\n    # Compute eigendecomposition\n    eigvals, eigvecs = torch.linalg.eigh(L_dense)  # returns all eigenpairs\n\n    # Take k smallest\n    eigvals = eigvals[:k]                   # (k,)\n    eigvecs = eigvecs[:, :k]                # (V, k)\n\n    eigvals_repeated = eigvals.unsqueeze(0).repeat(data.num_nodes, 1)  # (V, k)\n    spectral_features = torch.cat([eigvals_repeated, eigvecs], dim=-1)  # (V, 2k)\n\n    return spectral_features\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.624645Z","iopub.status.idle":"2025-08-01T14:32:18.625109Z","shell.execute_reply.started":"2025-08-01T14:32:18.624940Z","shell.execute_reply":"2025-08-01T14:32:18.624958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is the implementation of the rnd_greedy_min_cost_part algorithm 2 from the paper. Basically this algorithm runs at each step, and given number of nodes to remove, iteratively selectes the best contraction set (edges in our case) to use for coarsening, until the number of nodes to remove is obtained.","metadata":{}},{"cell_type":"code","source":"def rnd_greedy_min_cost_part(\n    candidate_contractions: torch.LongTensor, \n    cost_function, \n    current_graph, # added this parameter to compute cost, it contains current_graph on which coarsening is done\n    num_nodes_to_remove: int, \n    current_num_nodes: int,\n    rand_lambda: float = 1.0,\n):\n    \"\"\"\n    Algo 2\n    candidate_contractions [num_candidates, 2] each candidate is a pair of nodes (edge)\n    Greedily picks contraction candidates until at least num_nodes_to_remove are merged.\n    \"\"\"\n\n    num_candidates = candidate_contractions.size(0)    \n    alive = torch.ones(num_candidates, dtype=torch.bool) # check for already selected contractions, True means not selected\n    marked  = torch.zeros(current_num_nodes, dtype=torch.bool) # mark nodes already contracted...\n    \n    costs  = cost_function(candidate_contractions, current_graph)\n    \n    # each cost is a rand float between 0 and 1    \n\n    picked = [] # picked contractions idx\n    removed = 0\n\n    #print(f\"inside rnd_greedy_min_cost_part: i need to remove {num_nodes_to_remove} nodes\")\n    while (removed < num_nodes_to_remove and alive.any()):\n        masked_costs = costs.masked_fill(~alive, float('inf'))\n        idx = torch.argmin(masked_costs).item()\n        \n        if torch.rand(1).item() > rand_lambda: # simulate Bernoulli, after selecting an edge to remove, sample a Bernoulli and while it is not 0 never select it again and continue process\n            alive[idx] = False\n            continue \n\n        selected_nodes = candidate_contractions[idx] # this contains the selected edge (pair of nodes) to remove\n        \n        if marked[selected_nodes].any(): # maybe one of the two selected nodes is already used in some other edge that needs to be removed, then never select edge again and continue process\n            alive[idx] = False\n            continue\n\n        # otherwise mark it as selected set both of the nods to unalive and continue\n        \n        marked[selected_nodes] = True\n        alive[idx] = False\n        picked.append(idx)\n        #print(f\"We find a new edge to remove: {candidate_contractions[idx]}, removing it will remove: {selected_nodes.numel() - 1} nodes.\")\n        removed +=  selected_nodes.numel() - 1 # merging always keeps one node\n        #print(f\"inside rnd_greedy_min_cost_part: fo far i removed {removed} nodes.\")\n\n    #print(f\"I might have stopped because alive.any(): {alive.any()}\")\n    if picked:\n        picked_idx = torch.tensor(picked, dtype=torch.long)\n        return candidate_contractions[picked_idx]\n    else:\n        print(\"rnd_greedy_min_cost_part Error\")\n\n# test\n# for remove_n in range(1, current_num_nodes // 2): # max number of removable nodes\n#     test_partitioning = rnd_greedy_min_cost_part(test_candidates, get_cost, remove_n, current_num_nodes)\n#     print(f\"Removed {remove_n} nodes with contractions: {test_partitioning}\")\n    \ntest_num_nodes_to_remove = 2\n# uniform cost also possible with  get_uniform_cost instead of local_variation_costs\ntest_partitioning = rnd_greedy_min_cost_part(test_candidates, local_variation_costs, test_graph, test_num_nodes_to_remove, test_current_num_nodes)\nprint(test_partitioning)","metadata":{"_uuid":"ca6bee01-5e0f-4080-9a3b-ac0cc6a51f6b","_cell_guid":"2e420f0d-5e2b-4e13-ba53-fc99de809af5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.627253Z","iopub.status.idle":"2025-08-01T14:32:18.627593Z","shell.execute_reply.started":"2025-08-01T14:32:18.627453Z","shell.execute_reply":"2025-08-01T14:32:18.627468Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is a visualization function that assigns a unique color to each cluster id (as long as there are less than 34 clusters in total), and then displays the graph by coloring clusters (all nodes in a given cluster will share that cluster color). ","metadata":{}},{"cell_type":"code","source":"def visualize_clusters(G: Data, partitioning: torch.Tensor):\n    # Convert to NetworkX\n    if getattr(G, 'edge_attr', None) is not None:\n        G_nx = to_networkx(G, to_undirected=True, edge_attrs=[\"edge_attr\"])\n        raw_attrs = {\n            (u, v): int(d[\"edge_attr\"])\n            for u, v, d in G_nx.edges(data=True)\n        }\n    else:\n        G_nx = to_networkx(G, to_undirected=True)\n        raw_attrs = {}\n\n    # Create a color map for each node\n    color_map = ['lightgray'] * G.num_nodes  # default color\n\n    # Assign a unique color for each cluster\n    colors = [\n        'red', 'green', 'blue', 'orange', 'purple', 'cyan', 'yellow', 'brown',\n        'pink', 'olive', 'chocolate', 'lime', 'navy', 'teal', 'coral', 'gold',\n        'magenta', 'orchid', 'salmon', 'darkgreen', 'deepskyblue', 'slateblue',\n        'violet', 'maroon', 'turquoise', 'indigo', 'darkorange', 'crimson',\n        'steelblue', 'darkviolet', 'khaki', 'cadetblue', 'hotpink', 'mediumseagreen'\n    ]\n\n    cluster_id_to_color = {}\n    \n    for cluster_id, cluster in enumerate(partitioning):\n        color = colors[cluster_id % len(colors)]\n        cluster_id_to_color[cluster_id] = color\n        for node in cluster.tolist():\n            color_map[node] = color\n        print(f\"Cluster {cluster_id} has color {colors[cluster_id % len(colors)]}\")\n\n    edge_colors = []\n    for u, v in G_nx.edges():\n        a = raw_attrs.get((u, v), None)\n        edge_colors.append('red' if a == -1 else 'black')\n\n    # Draw the graph\n    pos = nx.spring_layout(G_nx, seed=42)\n    nx.draw(G_nx, pos, node_color=color_map, with_labels=True, edge_color=edge_colors)\n    plt.show()\n\n    return cluster_id_to_color","metadata":{"_uuid":"4bc1543a-9d19-447b-85ae-940fc31ab97d","_cell_guid":"4876d36b-c9bc-49a1-9bb7-4923d66a4005","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.629403Z","iopub.status.idle":"2025-08-01T14:32:18.630134Z","shell.execute_reply.started":"2025-08-01T14:32:18.629865Z","shell.execute_reply":"2025-08-01T14:32:18.629914Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_cluster_id_to_color = visualize_clusters(test_graph, test_partitioning)\nprint(f\"Cluster 0 has color {test_cluster_id_to_color[0]}\")","metadata":{"_uuid":"cff014b3-6aa1-46e0-b380-b7f337da1d55","_cell_guid":"b2733eaa-1869-46f2-910f-f9f0ed7baa70","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.632670Z","iopub.status.idle":"2025-08-01T14:32:18.633125Z","shell.execute_reply.started":"2025-08-01T14:32:18.632858Z","shell.execute_reply":"2025-08-01T14:32:18.632900Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def has_edge_undirected(edge_index, u, v):\n    src, dst = edge_index\n    return ((src == u) & (dst == v)).any() or ((src == v) & (dst == u)).any()\n\nhas_edge_undirected(test_graph.edge_index, 3, 5)","metadata":{"_uuid":"435f64d3-bd56-4222-abc0-2f7018d2421b","_cell_guid":"edf68720-6375-4481-b44c-c2d0571eb4f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.635383Z","iopub.status.idle":"2025-08-01T14:32:18.635853Z","shell.execute_reply.started":"2025-08-01T14:32:18.635688Z","shell.execute_reply":"2025-08-01T14:32:18.635711Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function completes a partitioning tensor containing all coarsening sets (pairs of nodes forming an edge that need to be merged) with the remaining sets that will not be merged for future code compatibility. The remaining sets not to be merged will all contain leftover nodes stacked with themselves, and concatenated at the end of the partitioning tensor. For instance one left over node i will be concatenated as [i, i].","metadata":{}},{"cell_type":"code","source":"def augment_with_singletons(partitioning: torch.LongTensor, num_nodes: int, debug=False):\n    \"\"\"\n    Given partitioning of shape [P,2]\n    return a new LongTensor of shape [P+U, K'] where rows 0..P-1\n    are your old clusters, and rows P..P+U-1 each contain exactly\n    one leftover node as a single cluster.\n    \"\"\"\n    # flatten and find which nodes were clustered\n    clustered = partitioning.unique() # no dim specified -> unique of the flattened partitioning\n    num_initial_clusters = partitioning.shape[0]\n    \n    all_nodes = torch.arange(num_nodes, device=partitioning.device)\n    \n    mask = torch.ones(num_nodes, dtype=torch.bool, device=partitioning.device)\n    mask[clustered] = False\n    \n    leftover = all_nodes[mask]  # shape [U]\n\n    # now make each leftover its own cluster of size 1, with itself, little trick\n    singletons = torch.stack([leftover, leftover], dim=1) # [U, 2]\n    \n    if singletons.numel() > 0:\n        partitioning = torch.cat([partitioning, singletons], dim=0)\n\n        if debug:\n            for i, node_id in enumerate(leftover.tolist()):\n                cluster_id = num_initial_clusters + i\n                print(f\"Leftover node {node_id} assigned to singleton cluster {cluster_id}\")\n\n    return partitioning # ideally also explain what id each solo cluster got for coloring later!\n\n# test \nprint(f\"Number of clusters before: {test_partitioning.shape[0]}\")\ntest_augmented_partitioning = augment_with_singletons(test_partitioning, test_current_num_nodes, debug=True)\nprint(f\"Number of clusters after: {test_augmented_partitioning.shape[0]}\")","metadata":{"_uuid":"7be62fdd-7eed-4370-a519-17525ad8227c","_cell_guid":"d34a0124-c2c6-4ffb-b0c8-e8e0cb9328ef","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.637797Z","iopub.status.idle":"2025-08-01T14:32:18.638508Z","shell.execute_reply.started":"2025-08-01T14:32:18.638204Z","shell.execute_reply":"2025-08-01T14:32:18.638228Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function builds a matrix P mapping all nodes to their clusters (left over nodes are considered as single node clusters) returns [num_clusters, N] for N nodes.","metadata":{}},{"cell_type":"code","source":"def build_P(num_nodes: int,\n            partitioning: torch.Tensor):\n    \"\"\"\n    num_nodes: N\n    partitioning: list of LongTensors, each contains original node‑indices cluster (max 2 nodes per cluster since we only consider edges).\n    Returns P of shape [num_clusters, N], where P[c,i]=1 if i belongs to coarse‑node c.\n    \"\"\"\n    num_clusters = len(partitioning)\n\n    P = torch.zeros((num_clusters, num_nodes), dtype=torch.float32)\n    for c, nodes in enumerate(partitioning):\n        P[c, nodes] = 1.0\n    return P\n\n# test\ntest_P = build_P(test_current_num_nodes, test_augmented_partitioning)\nprint(test_P)","metadata":{"_uuid":"7c3c0997-5129-4d41-9d51-bde0709421ee","_cell_guid":"970dc061-7e69-4367-942b-8e45fee05a64","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.640277Z","iopub.status.idle":"2025-08-01T14:32:18.640695Z","shell.execute_reply.started":"2025-08-01T14:32:18.640497Z","shell.execute_reply":"2025-08-01T14:32:18.640515Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Each row corresponds to one cluster id, each column corresponds to one node. This can be reused to compute the expansion step GT as each cluster super node will need to be expanded back to however many nodes it contains.","metadata":{"_uuid":"ff09fc72-468e-49bc-91d6-359be37e2413","_cell_guid":"39377340-43ca-46ee-a1c2-217122648bdb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"The below function creates an adjency matrix for clusters. Two clusters which initially contained nodes connected to each other will end up connected. Self loops for one cluster are removed by filling the diagonal of this matrix with 0s.","metadata":{}},{"cell_type":"code","source":"def get_cluster_adj(P: torch.FloatTensor,\n                        A: torch.FloatTensor):\n    # P: [C, N]; A: [N, N]\n    M = P @ A @ P.t()       # [C, C], computes inter cluster links, if any two clusters initially had nodes connected they will be connected\n    M.fill_diagonal_(0)\n    return (M > 0).to(torch.float32)\n\n# test\ntest_cluster_adj = get_cluster_adj(test_P, test_graph_adj)\nprint(test_cluster_adj)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.642559Z","iopub.status.idle":"2025-08-01T14:32:18.642971Z","shell.execute_reply.started":"2025-08-01T14:32:18.642747Z","shell.execute_reply":"2025-08-01T14:32:18.642766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function coarsens a graph given a partitioning containing coarsening sets (a set can be a cluster containing two nodes to merge, or a left over node copied twice that shouldn't be merged). It uses previous functions to returned the coarsened graph as well as the node-cluster mapping matrix that was used to build it.","metadata":{}},{"cell_type":"code","source":"def coarsen_graph(G: Data, partitioning: torch.LongTensor):\n    \"\"\"\n    Merge each row of partitioning into a single super‐node.\n    Unclustered nodes are nodes j not appearing in any row of partitioning.\n    Edges between any two super‐nodes exist iff any member nodes in both were previously connected in G.\n    G{l}->G{l+1}\n    \"\"\"\n    \n    N = G.num_nodes\n\n    augmented_partitioning = augment_with_singletons(partitioning, N)\n    \n    P = build_P(N, augmented_partitioning) # map nodes to clusters\n    graph_adj = get_graph_adj(G.edge_index, N) # get initial graph adjency\n    cluster_adj = get_cluster_adj(P, graph_adj) # get clusters adjency\n\n    edge_index, edge_attr = dense_to_sparse(cluster_adj) # convert cluster adj to list of edges\n    num_clusters = cluster_adj.size(0)\n    \n    x = torch.zeros((num_clusters, 1), dtype=torch.float32) # creates a list of cluster nodes\n    return Data(x=x, edge_index=edge_index, num_nodes=num_clusters), P # returns a new graph using list of cluster nodes and how they should be connected + the node-cluster mapping matrix \n\n# test\ntest_coarsened_graph, test_P = coarsen_graph(test_graph, test_partitioning)\n\ndef get_coarse_node_colors(num_clusters: int, cluster_id_to_color: dict) -> list[str]:\n    \"\"\"\n    Returns a list of colors for super-nodes in the coarsened graph.\n    Each super-node corresponds to a cluster ID.\n    \"\"\"\n    color_map = []\n    for cluster_id in range(num_clusters):\n        color = cluster_id_to_color.get(cluster_id, 'lightgray')\n        color_map.append(color)\n    return color_map\n\ntest_num_clusters = test_coarsened_graph.num_nodes\ntest_coarse_colors = get_coarse_node_colors(test_num_clusters, test_cluster_id_to_color)\n\n# Step 3: Plot the coarsened graph\nnx.draw(to_networkx(test_coarsened_graph), with_labels=True, node_color=test_coarse_colors)","metadata":{"_uuid":"9502285e-1ca6-43d4-a5a0-5281a1d881f9","_cell_guid":"858d2530-f405-4599-b18c-705feb5484dd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.644899Z","iopub.status.idle":"2025-08-01T14:32:18.645994Z","shell.execute_reply.started":"2025-08-01T14:32:18.645493Z","shell.execute_reply":"2025-08-01T14:32:18.645584Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A sequence of coarsening steps will always reach a single node graph eventually since the starting graph is fully connected and the only way to reach one node is to merge a graph containing two nodes (one edge).","metadata":{}},{"cell_type":"code","source":"curr = 1\nfor i in range(10):\n    print(curr)\n    curr = curr-math.ceil(0.10*curr) # math.ceil(0.10*curr) represents the number of nodes to remove, it is computed in the algorithm that follows.","metadata":{"_uuid":"9a72116c-0026-4a15-91ab-2409e38f1954","_cell_guid":"a37dd7d7-a5b2-4d57-b4df-01fc9f3f8adf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.650156Z","iopub.status.idle":"2025-08-01T14:32:18.650537Z","shell.execute_reply.started":"2025-08-01T14:32:18.650379Z","shell.execute_reply":"2025-08-01T14:32:18.650393Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function implements rnd_red_seq (algorithm 1) from the paper. It takes as input the initial graph and returns the list of coarsened graphs at each step up to the final single node graph, as well as the contractions history (the node-cluster mappings for building each intermediate coarsened graph).","metadata":{}},{"cell_type":"code","source":"def rnd_red_seq(G0: Data, min_red_frac=0.1, max_red_frac=0.3, debug=False): # it is not possible to remove more than current_num_nods//2 with edge contraction\n    \"\"\"\n    Algo 1\n    Returns random coarsening sequence π = (G0, . . . , GL) ∈ ΠF (G) \n    \"\"\"\n\n    current_graph = G0\n    \n    if debug: print(f\"Initial graph has {G0.num_nodes} nodes.\")\n    coarsening_seq = [current_graph] # π, will end up containing (G0...GL)\n    l = 0\n\n    contractions_history = [] # will contain (P1...PL-1) where Pi is the partitioning matrix used to coarsen Gi into G{i+1}\n    \n    while current_graph.num_nodes != 1: # removing % of current_num_nods to current_num_nods will always be bigger than 0 and math.ceil \n        # will end up being one until current_num_nods becomes 1 -> last node will be removed \n        l += 1\n        reduction_fraction = torch.tensor([1.0])\n        reduction_fraction.uniform_(min_red_frac, max_red_frac)  # ρ\n\n        # f already defined as get_cost, for now does not depend on G0/contractions_history\n        num_nodes_to_remove = math.ceil(reduction_fraction * current_graph.num_nodes) # m\n\n        candidates = get_candidate_contraction_sets(current_graph) # F(G{l−1}) fetch all possible contractions\n        if candidates.numel() == 0:\n            print(\"Error no more edges to contract in rnd_red_seq\")\n            break\n        \n        partitioning_l = rnd_greedy_min_cost_part(\n            candidate_contractions=candidates,\n            cost_function=get_cost,\n            current_graph=current_graph,\n            num_nodes_to_remove=num_nodes_to_remove,\n            current_num_nodes=current_graph.num_nodes\n        ) # tensor [C0, C1, …, Ck] # [num_clusters, 2]\n\n        if partitioning_l.numel() == 0:\n            print(\"No contractions picked\")\n            continue\n\n        # coarsen graph\n        if debug: print(f\"Coarsening graph and trying to remove {num_nodes_to_remove} nodes.\") # sometimes in some shapes such as S3 (star 4 nodes, one central) we cannot remove two nods\n            # if we pick the center one...\n        current_graph, P = coarsen_graph(current_graph, partitioning_l)\n        if debug: print(f\"Coarsened graph has {current_graph.num_nodes} nodes.\")\n\n        coarsening_seq.append(current_graph)\n        contractions_history.append(P) \n\n    return coarsening_seq, contractions_history\n\n# test\ntest_coarsening_seq, test_contractions_history = rnd_red_seq(test_graph, debug=True)\nprint(len(test_coarsening_seq), len(test_contractions_history))\n\nprint(\"\\nCoarsening sequence:\")\npprint(test_coarsening_seq)\n\nprint(\"\\nContractions history:\")\npprint(test_contractions_history)","metadata":{"_uuid":"c728b321-2556-4334-9ddd-1097df09cfb0","_cell_guid":"d1e16645-c4d1-4659-b043-0c4743eee22d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.652518Z","iopub.status.idle":"2025-08-01T14:32:18.652861Z","shell.execute_reply.started":"2025-08-01T14:32:18.652704Z","shell.execute_reply":"2025-08-01T14:32:18.652718Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Retrieving data needed for expansion","metadata":{"_uuid":"eba9d668-0b19-415c-926d-00e7acd939e7","_cell_guid":"4b791cea-f78a-46dc-8b22-3f39b6a1e169","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"for g in erdos_renyi_dataset[0:5]:\n    print(g)","metadata":{"_uuid":"845aa34c-36cf-4af8-a933-f16840c416e7","_cell_guid":"d051e53c-023b-4b8b-9ab9-e0136e5d156b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.654363Z","iopub.status.idle":"2025-08-01T14:32:18.654938Z","shell.execute_reply.started":"2025-08-01T14:32:18.654700Z","shell.execute_reply":"2025-08-01T14:32:18.654723Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function computes the cluster counts (v) given a node/cluster mapping matrix. Since we only use clusters of 1/2 nodes -> we reformulate the problem as classification between -1 if node shouldn't be expanded (cluster size 1) , 1 if node should be expanded (cluster size 2) ","metadata":{}},{"cell_type":"code","source":"def compute_node_embeddings(P):\n    cluster_counts = P.sum(dim=1).long() # v_l\n    assign = P.argmax(dim=0)\n    v = cluster_counts[assign]\n    v = v-2\n    return v\n\n# test\nprint(\"P:\")\nprint(test_P)\n\ntest_v = compute_node_embeddings(test_P)\nprint(\"Result:\\n\", test_v)","metadata":{"_uuid":"fe3b9df4-04c7-441e-a134-c2b1360229b1","_cell_guid":"5d0db520-6c75-4b9c-b52f-f3ba4fa4b673","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.656273Z","iopub.status.idle":"2025-08-01T14:32:18.656595Z","shell.execute_reply.started":"2025-08-01T14:32:18.656459Z","shell.execute_reply":"2025-08-01T14:32:18.656472Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function expands a coarsened graph based on the node/cluster mapping matrix that was used to obtain that coarsened graph Gc, it returns the adjency matrix of the expanded graph.","metadata":{}},{"cell_type":"code","source":"def expand_coarsened(Gc: Data, P):\n    C, N = P.shape\n    expanded_adj = np.zeros((N, N), dtype=int)\n    \n    coarsened_adj = get_graph_adj(Gc.edge_index, Gc.num_nodes) # [C, C]\n\n    # for each node i find its cluster id\n    clusters = P.argmax(dim=0)  # [N]  each in 0..C-1\n\n    # # intercluster connections \n    # for i in range(N):\n    #     for j in range(N):\n    #         if clusters[i] != clusters[j]:\n    #             ci, cj = clusters[i], clusters[j]\n    #             if coarsened_adj[ci, cj] == 1:\n    #                 expanded_adj[i, j] = 1\n    #                 expanded_adj[j, i] = 1\n    #         else:\n    #             expanded_adj[i, j] = 1\n    #             expanded_adj[j, i] = 1\n\n    # np.fill_diagonal(expanded_adj, 0)\n    # return expanded_adj\n    \n    # chatgpt optimized version using broadcasting\n    idx_i = clusters.unsqueeze(1)  # [N,1]\n    idx_j = clusters.unsqueeze(0)  # [1,N]\n\n    # intercluster mask\n    inter_mask = coarsened_adj[idx_i, idx_j].to(torch.int) # [N, N] \n    \n    # intracluster mask\n    intra_mask = idx_i == idx_j.to(torch.int) # [N, N]\n\n    # combine and zero out diagonal no self loop\n    expanded = (inter_mask | intra_mask)\n    expanded.fill_diagonal_(False)\n\n    return expanded\n    \n# test\ntest_expanded_graph_adj = expand_coarsened(test_coarsened_graph, test_P)\nprint(test_expanded_graph_adj)","metadata":{"_uuid":"76d2418d-1c10-45ff-a60a-51213995d501","_cell_guid":"8b409003-57ba-4d96-812c-1dc0a298eb85","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.659265Z","iopub.status.idle":"2025-08-01T14:32:18.660269Z","shell.execute_reply.started":"2025-08-01T14:32:18.660042Z","shell.execute_reply":"2025-08-01T14:32:18.660068Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:red\">Should we realy remove self loops here? expanded.fill_diagonal_(False) in the paper diagram for PPGN self loops are kept eventhough the model should learn to remove them of course...</span>","metadata":{}},{"cell_type":"markdown","source":"Below is the original test graph that we coarsen using the colored clusters.","metadata":{}},{"cell_type":"code","source":"print(\"Original clustering\")\ntest_cluster_id_to_color = visualize_clusters(test_graph, test_partitioning)","metadata":{"_uuid":"9915deee-894e-4ed5-825b-17f1660ddd43","_cell_guid":"04db82d4-2965-499d-b54b-c5f0f816d618","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.662801Z","iopub.status.idle":"2025-08-01T14:32:18.663296Z","shell.execute_reply.started":"2025-08-01T14:32:18.663082Z","shell.execute_reply":"2025-08-01T14:32:18.663101Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_cluster_id_to_color","metadata":{"_uuid":"f3cb5799-ecc7-49c9-82b7-ce3d54aa3962","_cell_guid":"f42e4461-bc72-4bbb-bef0-b1ca8180b2a9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.665801Z","iopub.status.idle":"2025-08-01T14:32:18.666643Z","shell.execute_reply.started":"2025-08-01T14:32:18.666385Z","shell.execute_reply":"2025-08-01T14:32:18.666401Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is the coarsened graph obtained from the previous clusters + original graph.","metadata":{}},{"cell_type":"code","source":"print(\"Original coarsened graph\")\nnx.draw(to_networkx(test_coarsened_graph), with_labels=True, node_color=test_coarse_colors)","metadata":{"_uuid":"1be47c76-31e7-4448-a337-a6ef52f56563","_cell_guid":"73d4db0a-fe6d-4c64-86e3-60cc6ccfbc60","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.667771Z","iopub.status.idle":"2025-08-01T14:32:18.668312Z","shell.execute_reply.started":"2025-08-01T14:32:18.668127Z","shell.execute_reply":"2025-08-01T14:32:18.668148Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is a function which given the adjency matrix of some expanded graph $\\tilde{G}_l$  creates the refined graph Data object $G_l$.  $\\tilde{G}_l$ was obtained after expanding some $\\tilde{G}_{l+1}$ using $v_{l+1}$. The new graph should contain the same edges as the expanded graph, but with labels indicating whether we should keep (1) or remove/refine them (-1). All edges can be represented as $e_l$. Node embeddings will then contain for each node the future expansion counts required, all node embeddings can therefore be represented as $v_l$ later on.","metadata":{}},{"cell_type":"code","source":"def refine_expanded_graph(expanded_adj: torch.Tensor, original_adj: torch.Tensor, v: torch.Tensor) -> Data:\n    \"\"\"\n    expanded_adj: [N, N] dense adjacency (0/1)\n    expanded_adj: [N, N] dense adjacency (0/1)\n    \n    returns: Data(x=[N,1], edge_index=[2, E])\n    \"\"\"\n    edge_index, edge_attr = dense_to_sparse(expanded_adj)\n\n    edge_labels = original_adj[edge_index[0],edge_index[1]].long() # [E]\n    edge_labels = edge_labels * 2 - 1 # remap 0/1 to -1/1\n\n    #x = torch.zeros((expanded_adj.size(0), 1), dtype=torch.float)\n    x = torch.zeros((expanded_adj.size(0), 1), dtype=torch.float)\n    y = v.float()\n\n    return Data(x=x, edge_index=edge_index, edge_attr=edge_labels, y=y)\n\ntest_refinedGraphData = refine_expanded_graph(test_expanded_graph_adj, test_graph_adj, test_v)\nprint(test_refinedGraphData)\nprint(\".x:\", test_refinedGraphData.x)\nprint(\".y:\", test_refinedGraphData.y)\nprint(\"edge_attr:\", test_refinedGraphData.edge_attr)","metadata":{"_uuid":"39785a57-49ad-4ab0-916c-2a74e2454eab","_cell_guid":"005394de-6db7-4192-8243-304b6fe7b265","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.670141Z","iopub.status.idle":"2025-08-01T14:32:18.670731Z","shell.execute_reply.started":"2025-08-01T14:32:18.670377Z","shell.execute_reply":"2025-08-01T14:32:18.670390Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Expanded and refined (red) graph coarsened graph\")\ntest_cluster_id_to_color = visualize_clusters(test_refinedGraphData, test_partitioning)","metadata":{"_uuid":"bcb82d50-1869-4289-9307-17eb6b40a84c","_cell_guid":"0d2940e1-0bad-4f6d-a457-a879966a0738","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.672741Z","iopub.status.idle":"2025-08-01T14:32:18.673144Z","shell.execute_reply.started":"2025-08-01T14:32:18.672962Z","shell.execute_reply":"2025-08-01T14:32:18.672980Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can extract from the refined graph the vector $e_l$ and for each edge print its -1/1 value.","metadata":{}},{"cell_type":"code","source":"edge_index = test_refinedGraphData.edge_index   # shape [2, E]\nedge_attr  = test_refinedGraphData.edge_attr    # shape [E] or [E,1]\n\nfor i in range(edge_index.size(1)):\n    u = edge_index[0, i].item()\n    v = edge_index[1, i].item()\n    a = edge_attr[i].item() if edge_attr.ndim == 1 else edge_attr[i,0].item()\n    print(f\"Edge ({u:2d}, {v:2d})  → attr = {a}\")","metadata":{"_uuid":"bafa8104-e935-4ecf-a099-c0b56c7f4db9","_cell_guid":"41366e98-6977-46f3-8e21-edbe13d25a1a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.674798Z","iopub.status.idle":"2025-08-01T14:32:18.675198Z","shell.execute_reply.started":"2025-08-01T14:32:18.675059Z","shell.execute_reply":"2025-08-01T14:32:18.675074Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating SignNet node embeddings from graph for conditional diffusion","metadata":{}},{"cell_type":"markdown","source":"This code is adapted from their implementation.","metadata":{}},{"cell_type":"code","source":"class MLP(Module):\n    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n\n    Args:\n        in_features (int): Number of features of the input.\n        hidden_features (list[int]): List of the hidden features dimensions.\n        out_features (int, optional): If not `None` a projection layer is added at the end of the MLP. Defaults to `None`.\n        bias (bool, optional): Whether to use bias in the linear layers. Defaults to `True`.\n        norm_layer (Module, optional): Normalization layer to use. Defaults to `norm_layer`.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: list[int],\n        out_features: int | None = None,\n        bias: bool = True,\n        norm_layer=LayerNorm,\n    ):\n        super().__init__()\n        lin_layers = []\n        norm_layers = []\n        hidden_in_features = in_features\n        for hidden_dim in hidden_features:\n            lin_layers.append(Linear(hidden_in_features, hidden_dim, bias=bias))\n            norm_layers.append(norm_layer(hidden_dim))\n            hidden_in_features = hidden_dim\n\n        self.out_layer = (\n            Linear(hidden_in_features, out_features, bias=bias)\n            if out_features is not None\n            else None\n        )\n\n        self.lin_layers = ModuleList(lin_layers)\n        self.norm_layers = ModuleList(norm_layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for lin, norm in zip(self.lin_layers, self.norm_layers):\n            x = lin(x)\n            x = norm(x)\n            x = torch.relu(x)\n\n        if self.out_layer is not None:\n            x = self.out_layer(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.678444Z","iopub.status.idle":"2025-08-01T14:32:18.678847Z","shell.execute_reply.started":"2025-08-01T14:32:18.678701Z","shell.execute_reply":"2025-08-01T14:32:18.678716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SignNet(Module):\n    def __init__(\n        self,\n        num_eigenvectors: int,\n        hidden_features: int,\n        out_features: int,\n        num_layers: int,\n        dropout: float = 0.0,\n    ) -> None:\n        super().__init__()\n\n        self.in_layer = Linear(2, hidden_features)\n        self.conv_layers = ModuleList(\n            [\n                GINConv(\n                    MLP(hidden_features, [hidden_features, hidden_features]),\n                    train_eps=True,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        self.skip_layer = Linear(hidden_features * (num_layers + 1), hidden_features)\n        self.dropout = Dropout(dropout)\n        # the following corresponds to the ρ function in the paper\n        self.merge_layer = MLP(\n            in_features=num_eigenvectors * hidden_features,\n            hidden_features=[hidden_features, hidden_features],\n            out_features=out_features,\n        )\n\n    def forward(self, spectral_features, edge_index):\n        \"\"\"Forward pass of the model.\n\n        Args:\n            spectral_features (Tensor): Eigenvalues (repeated) concatenated with eigenvectors. Shape: :math:`(V, num_eigenvectors * 2)`.\n            edge_index (Adj): Adjacency matrix given as edge index or sparse tensor. Shape: :math:`(2, E)` or :math:`(V, V)`.\n\n        Returns:\n            Tensor: Node features. Shape: :math:`(V, out_features)`.\n        \"\"\"\n        # Stack spectral features\n        eigenvalues_repeated, eigenvectors = spectral_features.chunk(\n            2, dim=-1\n        )  # (V, k), (V, k)\n\n        positive_spectral_features = torch.stack(\n            [eigenvalues_repeated, eigenvectors], dim=-1\n        )  # V, k, 2\n        negative_spectral_features = torch.stack(\n            [eigenvalues_repeated, -eigenvectors], dim=-1\n        )  # V, k, 2\n        combined_spectral_features = torch.stack(\n            [positive_spectral_features, negative_spectral_features]\n        ).transpose(\n            1, 2\n        )  # 2, k, V, 2\n\n        # Apply layers\n        x = self.in_layer(combined_spectral_features)  # 2, k, V, hidden_features\n        xs = [x]\n        for conv in self.conv_layers:\n            # apply conv layer to each spectral feature independently\n            x = conv(x=x, edge_index=edge_index)\n            xs.append(x)\n\n        # Skip connection\n        x = torch.cat(xs, dim=-1)\n        x = self.dropout(x)\n        x = self.skip_layer(x)  # 2, k, V, hidden_features\n        # Make sign invariant\n        x = x.sum(dim=0)  # k, V, hidden_features\n\n        # Merge features\n        x = x.transpose(0, 1)  # V, k, hidden_features\n        x = self.merge_layer(x.reshape(x.size(0), -1))  # V, out_features\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.680046Z","iopub.status.idle":"2025-08-01T14:32:18.680450Z","shell.execute_reply.started":"2025-08-01T14:32:18.680253Z","shell.execute_reply":"2025-08-01T14:32:18.680273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_spectral_features_torch(data, k):\n    # Get Laplacian\n    lap_edge_index, lap_edge_weight = get_laplacian(data.edge_index, normalization='sym')\n    L_dense = get_graph_adj(data.edge_index, data.num_nodes)\n\n    # Compute eigendecomposition\n    eigvals, eigvecs = torch.linalg.eigh(L_dense)  # returns all eigenpairs\n\n    # Take k smallest\n    eigvals = eigvals[:k]                   # (k,)\n    eigvecs = eigvecs[:, :k]                # (V, k)\n\n    eigvals_repeated = eigvals.unsqueeze(0).repeat(data.num_nodes, 1)  # (V, k)\n    spectral_features = torch.cat([eigvals_repeated, eigvecs], dim=-1)  # (V, 2k)\n\n    return spectral_features\n\n\n# test\nget_spectral_features_torch(test_graph, k=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.681464Z","iopub.status.idle":"2025-08-01T14:32:18.682271Z","shell.execute_reply.started":"2025-08-01T14:32:18.681661Z","shell.execute_reply":"2025-08-01T14:32:18.681678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inference\n\nglobal_k = 32 # means the model takes as input 2x 32 = 64\n\nV = test_graph.num_nodes\nk_cur = min(global_k, V - 1)\n\nembedding_model = SignNet(\n    num_eigenvectors=global_k,\n    hidden_features=64,\n    out_features=128,\n    num_layers=3,\n    dropout=0.1\n)\n\nspectral_features = get_spectral_features_torch(test_graph, k=k_cur)\nprint(spectral_features.shape)\n\ntarget_width = 2 * global_k\ncur_width = spectral_features.shape[1]\n\nif cur_width < target_width:\n    pad_size = target_width - cur_width  # = 2*global_k − 2*k_cur\n    padding = torch.zeros((V, pad_size))\n    spectral_features = torch.cat([spectral_features, padding], dim=1)\n\nprint(spectral_features.shape)\n\nembedding_model.eval()\nwith torch.no_grad():\n    out = embedding_model(spectral_features, test_graph.edge_index)\n\nprint(out.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.684974Z","iopub.status.idle":"2025-08-01T14:32:18.685396Z","shell.execute_reply.started":"2025-08-01T14:32:18.685228Z","shell.execute_reply":"2025-08-01T14:32:18.685244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spectral_features.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.687864Z","iopub.status.idle":"2025-08-01T14:32:18.688398Z","shell.execute_reply.started":"2025-08-01T14:32:18.688256Z","shell.execute_reply":"2025-08-01T14:32:18.688274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fill_H_embeddings(G_coarsened: Data, G_refined: Data, P_l, embedding_model, v, k=0):\n    num_nodes = G_coarsened.num_nodes\n    if k==0:\n        #H = torch.randn(num_nodes)\n        x = torch.randn(2*k)\n        G_refined.x = x\n        return G_refined\n\n    else:\n        k_eig = min(k, num_nodes - 1)\n        spectral_feats = get_spectral_features_torch(G_coarsened, k=k_eig)\n    \n        if k > k_eig:\n            pad_cols = 2 * (k - k_eig)\n            padding = torch.zeros((num_nodes, pad_cols))\n            spectral_feats = torch.cat([spectral_feats, padding], dim=1)\n    \n        with torch.no_grad():\n            H_coarsened = embedding_model(spectral_feats, G_coarsened.edge_index)\n\n        print(H_coarsened.shape)\n\n        # broadcast using P_l that was used to obtain G_coarsened from G\n        cluster_id = P_l.argmax(dim=0)\n        H_expanded = H_coarsened[cluster_id]\n\n        G_refined.x = H_expanded\n        \n        return G_refined\n\n# test\ntest_graph = fill_H_embeddings(test_coarsened_graph, test_graph, test_P, embedding_model, test_v, k=2)\nprint(test_graph.num_nodes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.691015Z","iopub.status.idle":"2025-08-01T14:32:18.691551Z","shell.execute_reply.started":"2025-08-01T14:32:18.691219Z","shell.execute_reply":"2025-08-01T14:32:18.691232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating the training dataset","metadata":{}},{"cell_type":"markdown","source":"The diffusion model will learn to denoise e_l (refine current graph), and v_l future expansions after refinement to get Gl-1, given the following parameters:\n\nnoised el and vl obtained from Gl˜ (noised T times using ddpm scheduler)\nT\nGl˜ -> make diffusion conditional, that is the refined graph... Gl˜.x should contain the H embeddings replicated according to algo 5 in the paper, .y contains future expansion labels v -1/1, and edges attr contain -1/1\ntarget size of G0?\nsomething else?\n\nIt should output denoised el and vl (one step?)","metadata":{}},{"cell_type":"code","source":"class CoarsenDataset(IterableDataset): # torch dataset\n    def __init__(self, graphsDataset, min_red=0.1, max_red=0.3, start=0, end=None):\n        super(CoarsenDataset).__init__()\n        self.graphsDataset = graphsDataset\n        self.start = start\n        self.end = len(graphs_dataset) if end is None else end\n        \n    def __iter__(self):\n        iter_start = self.start\n        iter_end = self.end\n        graphDataset = self.graphsDataset\n\n        selected_graphs = graphDataset[iter_start: iter_end]\n        \n        for G0 in selected_graphs:\n            coarsening_seq, contractions_history = rnd_red_seq(G0, self.min_red, self.max_red)\n            # coarsening_seq = [G0, G1, ..., G_L]\n            # contractions_history = [P1, P2, ..., P_{L}]  where P_l is used for G_{l-1}->G_l\n            # P_l are the node-cluster mappings...\n\n            num_initial_nodes = G0.num_nodes # target num nodes\n            \n            original_adj = get_graph_adj(G0.edge_index, G0.num_nodes)\n            seq_len = len(coarsening_seq)\n            print(seq_len)\n            break\n            \n            for G_l, G_lp1, P_l in zip(coarsening_seq[:], coarsening_seq[1:], contractions_history):\n                # to construct each sample we need:\n                # from a sequence G{l-1} ---(Pl)---> Gl ----(Plp1)---> Glp1\n                \n                # first to expand Glp1 using Plp1 -> and obtain Gl˜ before refinement\n                # compare Gl˜ and Gl to get refined graph and extract e_l -> add that to graph Gl˜ to obtain refined graph with proper edge attributes which is denoted as Gl\n                # Use Pl to compute next node expansion for Gl (the expansion that will lead to G{l-1}˜)\n\n                # for last node we know v=[2]\n                # then given refined graphs + node embeddings/edgeattributes resp. G1,G2....,G{l-1} we can learn to create resp. G0, ..., Gl-2\n                # we don't need to learn how to create Gl-1 since given any single node graph GL we just duplicate it into two nodes and that gives Gl-1...\n                # therefore we can reconstruct any sequence backwards if model learns to reproduce the refinement+future expansion steps...\n\n                coarsening_red_factor = G_lp1.num_nodes / G_l.num_nodes # reduction factor, proportion of nodes removed\n                # during coarsening\n                \n                expanded_graph_adj = expand_coarsened(G_lp1, P_l) # Gl tilde, using actual cluster values\n                v_l = compute_node_embeddings(P_l) # extract v_l counts of each cluster nodes\n                # will be node embeddings\n\n                refined_graph = refine_expanded_graph(expanded_graph_adj, original_adj, v_l) # data object, represents some G_l containing e_l to refine it... as well as future v_l labels in its .y\n                ...\n                \n            \n                yield \n\n\ndef coarsen_collate(batch):\n","metadata":{"_uuid":"96d9fedb-fdcf-4c43-b033-119ae466dc0b","_cell_guid":"39565894-d41e-4223-981e-324bb036d0b9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.694567Z","iopub.status.idle":"2025-08-01T14:32:18.694994Z","shell.execute_reply.started":"2025-08-01T14:32:18.694802Z","shell.execute_reply":"2025-08-01T14:32:18.694816Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import IterableDataset\nfrom torch_geometric.data import Data, Batch\n\nclass CoarsenDataset(IterableDataset):\n    \"\"\"\n    Yields training triples (graph, v_clean, e_clean, globals)\n      graph.x       : [N, d_H+1]   –  concat(H , v_bin)\n      graph.edge_attr: [E]         –  e_bin\n      v_clean        : [N]         –  {-1,+1}\n      e_clean        : [E]         –  {-1,+1}\n      globals        : dict(ρ=float , target=int)\n    \"\"\"\n\n    def __init__(\n        self,\n        graphs_dataset,                     # any PyG dataset\n        signnet_model,                      # instantiated SignNet\n        global_k      = 8,                  # eigenvectors for SignNet\n        min_red       = 0.10,\n        max_red       = 0.30,\n        start         = 0,\n        end           = None,\n    ):\n        super().__init__()\n        self.graphs  = graphs_dataset\n        self.model   = signnet_model\n        self.k       = global_k\n        self.min_red = min_red\n        self.max_red = max_red\n        self.start   = start\n        self.end     = len(graphs_dataset) if end is None else end\n\n    # ---------- helper -----------------------------------------------------\n    def _broadcast_H(self, H_parent, P_l):          # P_l: [C,N]\n        cluster_id = P_l.argmax(dim=0)              # [N]\n        return H_parent[cluster_id]                 # [N, d_H]\n\n    # ---------- iterator ---------------------------------------------------\n    def __iter__(self):\n        for G0 in self.graphs[self.start : self.end]:\n            # 1. coarsen once per epoch\n            seq, Ps = rnd_red_seq(G0, self.min_red, self.max_red)\n            # seq = [G0,…, G_L]     Ps = [P1,…, P_L]\n\n            target_size = G0.num_nodes\n            for l in range(len(Ps)):                # iterate over levels\n                G_l      = seq[l]                   # fine  graph at level l\n                G_lp1    = seq[l+1]                 # coarse graph\n                P_l      = Ps[l]                    # map nodes(G_l) -> nodes(G_{l+1})\n\n                # 2. build expanded graph ẐG_l and labels\n                expanded_adj = expand_coarsened(G_lp1, P_l)\n                v_counts     = compute_node_embeddings(P_l)  # [N] (1/2)\n                v_bin        = 2*v_counts - 3                # {-1,+1}\n\n                e_bin_graph  = refine_expanded_graph(\n                                    expanded_adj,\n                                    get_graph_adj(G_l.edge_index, G_l.num_nodes),\n                                    v_counts)\n                e_bin = e_bin_graph.edge_attr                 # {-1,+1}\n\n                # 3. SignNet embedding on G_{l+1}, then broadcast to ẐG_l\n                H_parent = get_embeddings(G_lp1,\n                                           self.model,\n                                           v_counts,          # dummy arg\n                                           k=self.k)          # [C, d_H]\n                H_full   = self._broadcast_H(H_parent, P_l)   # [N, d_H]\n\n                # 4. pack node features  [N, d_H+1]\n                node_x = torch.cat([H_full, v_bin.unsqueeze(1).float()], dim=1)\n                e_bin_graph.x         = node_x\n                e_bin_graph.y_v_clean = v_bin          # supervision\n                e_bin_graph.y_e_clean = e_bin\n\n                # 5. globals\n                rho = G_lp1.num_nodes / G_l.num_nodes  # reduction fraction\n                globals = dict(rho=torch.tensor([rho], dtype=torch.float32),\n                               target=torch.tensor([target_size], dtype=torch.long))\n\n                yield e_bin_graph, globals\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.696937Z","iopub.status.idle":"2025-08-01T14:32:18.697312Z","shell.execute_reply.started":"2025-08-01T14:32:18.697170Z","shell.execute_reply":"2025-08-01T14:32:18.697186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def coarsen_collate(samples, scheduler, device=\"cpu\"):\n    \"\"\"\n    samples   : list[(Data, globals)]\n    scheduler : your DDPM / VP-SDE scheduler with   .sample_timesteps(B)\n                and   .add_noise(x, t)\n    returns\n        batch_graph         – PyG Batch with *noisy* node / edge features\n        clean_v , clean_e   – tensors for the loss\n        timesteps, globals  – tensors\n    \"\"\"\n    graphs, globals = zip(*samples)\n    batch    = Batch.from_data_list(graphs).to(device)\n\n    # extract clean targets before adding noise\n    clean_v = torch.cat([g.y_v_clean  for g in graphs]).to(device)  # [ΣN]\n    clean_e = torch.cat([g.y_e_clean  for g in graphs]).to(device)  # [ΣE]\n\n    # DDPM time-steps\n    B        = len(graphs)\n    t        = scheduler.sample_timesteps(B).to(device)             # [B]\n\n    # node-wise noise\n    node_feat = batch.x                                             # [ΣN,d+1]\n    noisy_node_feat = scheduler.add_noise(node_feat, t.repeat_interleave(\n                          batch.__slices__['x'][1:] - batch.__slices__['x'][:-1]))\n    batch.x = noisy_node_feat\n\n    # edge-wise noise\n    edge_feat = batch.edge_attr.float().unsqueeze(1)                # [ΣE,1]\n    noisy_edge_feat = scheduler.add_noise(edge_feat, t.repeat_interleave(\n                          batch.__slices__['edge_index'][1:] - batch.__slices__['edge_index'][:-1]))\n    batch.edge_attr = noisy_edge_feat.squeeze(1)\n\n    # stack globals in the same order\n    rhos    = torch.cat([g['rho']    for g in globals]).to(device)\n    tgt_szs = torch.cat([g['target'] for g in globals]).to(device)\n\n    return batch, clean_v, clean_e, t, rhos, tgt_szs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.698587Z","iopub.status.idle":"2025-08-01T14:32:18.699024Z","shell.execute_reply.started":"2025-08-01T14:32:18.698807Z","shell.execute_reply":"2025-08-01T14:32:18.698825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = CoarsenDataset(graphs_dataset=erdos_renyi_dataset,\n                         signnet_model=embedding_model,\n                         global_k=32)\n\nloader  = torchDataLoader(dataset,\n                          batch_size=32,\n                          collate_fn=lambda b: coarsen_collate(b, scheduler),\n                          num_workers=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.700024Z","iopub.status.idle":"2025-08-01T14:32:18.700378Z","shell.execute_reply.started":"2025-08-01T14:32:18.700197Z","shell.execute_reply":"2025-08-01T14:32:18.700216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for ex in loader:\n    print(ex)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.702117Z","iopub.status.idle":"2025-08-01T14:32:18.702436Z","shell.execute_reply.started":"2025-08-01T14:32:18.702303Z","shell.execute_reply":"2025-08-01T14:32:18.702316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = CoarsenDataset(erdos_renyi_dataset, 0.1, 0.3, start=0, end=100)\nloader = torchDataLoader(ds, batch_size=4, collate_fn=coarsen_collate)\n\nfor G_l, G_lp1, P_l in loader:\n    print(\"G_l: \", G_l)\n    print(\"G_lp1: \", G_lp1)\n    print(\"P_l is of length: \", len(P_l))\n    break\n\nprint(\"\\nEach batch contains four graphs:\")\nfor i in range(4):\n    print(G_l.get_example(i))\n\n# In a batch all nodes and edge_index are stacked but .batch keeps indices to filter for specific graphs","metadata":{"_uuid":"977ae6c2-30b7-4c92-b20f-392e9153919c","_cell_guid":"26fc6c66-9163-486e-a594-1ceffa6ea312","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.703822Z","iopub.status.idle":"2025-08-01T14:32:18.705039Z","shell.execute_reply.started":"2025-08-01T14:32:18.704075Z","shell.execute_reply":"2025-08-01T14:32:18.704235Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Noising v_l and e_l for diffusion ","metadata":{}},{"cell_type":"code","source":"scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\nscheduler.set_timesteps(1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.707808Z","iopub.status.idle":"2025-08-01T14:32:18.708373Z","shell.execute_reply.started":"2025-08-01T14:32:18.708191Z","shell.execute_reply":"2025-08-01T14:32:18.708218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"v = torch.rand((4, 10))\nt = torch.randint(0, 1000, (4,), dtype=torch.long)  # random timesteps per sample\nnoise = torch.randn_like(v)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.710693Z","iopub.status.idle":"2025-08-01T14:32:18.711059Z","shell.execute_reply.started":"2025-08-01T14:32:18.710901Z","shell.execute_reply":"2025-08-01T14:32:18.710923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"v_t = scheduler.add_noise(v, noise=noise, timesteps=t)\nprint(v_t)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.713110Z","iopub.status.idle":"2025-08-01T14:32:18.713514Z","shell.execute_reply.started":"2025-08-01T14:32:18.713318Z","shell.execute_reply":"2025-08-01T14:32:18.713338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"G_l.batch","metadata":{"_uuid":"e913683e-b771-4886-b9e7-ce48dbdde0ca","_cell_guid":"fa5037a1-3040-4dab-8ef9-5d3408ef2bb8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.715670Z","iopub.status.idle":"2025-08-01T14:32:18.716135Z","shell.execute_reply.started":"2025-08-01T14:32:18.715939Z","shell.execute_reply":"2025-08-01T14:32:18.715957Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch_geometric.nn import GCNConv\nimport torch.nn.functional as F\n\nclass SimpleGCN(torch.nn.Module):\n    def __init__(self, in_ch, hidden_ch, out_ch):\n        super().__init__()\n        self.conv1 = GCNConv(in_ch, hidden_ch)\n        self.conv2 = GCNConv(hidden_ch, out_ch)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        return x","metadata":{"_uuid":"211315ce-a085-44fb-9cae-cd37ef95581c","_cell_guid":"2ad51523-5aeb-4f95-98c4-f2bfb66d5e52","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.717627Z","iopub.status.idle":"2025-08-01T14:32:18.718327Z","shell.execute_reply.started":"2025-08-01T14:32:18.718131Z","shell.execute_reply":"2025-08-01T14:32:18.718153Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SimpleGCN(in_ch=1, hidden_ch=16, out_ch=8)\nout = model(G_l)\nG_l.x = out\ngraphs = G_l.to_data_list()","metadata":{"_uuid":"19f0491b-dde2-47ba-92cf-6c6955f47a5a","_cell_guid":"6d6338e5-87d9-4d2e-ad4e-aeaa64419640","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.719907Z","iopub.status.idle":"2025-08-01T14:32:18.720245Z","shell.execute_reply.started":"2025-08-01T14:32:18.720103Z","shell.execute_reply":"2025-08-01T14:32:18.720116Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"graphs[0].x","metadata":{"_uuid":"6e62a662-e927-4485-bb8f-a2091f5090b0","_cell_guid":"fafe1be9-c421-4105-a98b-d041871ae5ae","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T14:32:18.721742Z","iopub.status.idle":"2025-08-01T14:32:18.722126Z","shell.execute_reply.started":"2025-08-01T14:32:18.721932Z","shell.execute_reply":"2025-08-01T14:32:18.721953Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}