{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12624079,"sourceType":"datasetVersion","datasetId":7974781}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook provides an implementation for the paper EFFICIENT AND SCALABLE GRAPH GENERATION THROUGH ITERATIVE LOCAL EXPANSION: [https://arxiv.org/pdf/2312.11529](https://arxiv.org/pdf/2312.11529)","metadata":{}},{"cell_type":"code","source":"!pip3 install -q torch_geometric\n!pip3 install -q diffusers\n!pip3 install torch_scatter","metadata":{"_uuid":"61bd1e12-3a28-4fea-8958-826f0001255e","_cell_guid":"bfcfac55-6772-462a-aaf3-138fc1f58b11","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-03T22:42:03.710970Z","iopub.execute_input":"2025-08-03T22:42:03.711238Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Import required librairies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport math\nfrom pprint import pprint\n\nimport torch\nfrom torch.nn import Dropout, LayerNorm, Linear, Module, ModuleList, Sequential, ReLU\nfrom torch.utils.data import IterableDataset\nfrom torch.utils.data import DataLoader as torchDataLoader\nimport torch.nn.functional as F\n\n\nfrom torch_geometric.nn import TransformerConv          # edge-aware\nfrom torch_scatter import scatter_mean             # for e→v summary\n\n\nimport torch_geometric\nfrom torch_geometric.utils import get_laplacian\nfrom torch_geometric.data import Data, Batch, DataLoader, InMemoryDataset, download_url \nfrom torch_geometric.utils import erdos_renyi_graph, to_dense_adj, dense_to_sparse, to_networkx, from_networkx\nfrom torch_geometric.nn import GCNConv, GINConv, GATConv\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport os\n\nfrom diffusers import DDPMScheduler\nfrom tqdm.auto import trange","metadata":{"_uuid":"427e1779-e73e-4477-895e-526e1befd407","_cell_guid":"37b845cc-2ebc-49ee-89b9-32a9c821864a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"torch_geometric version:\", torch_geometric.__version__)\nprint(\"torch version:\", torch.__version__)","metadata":{"_uuid":"6758ddb3-538b-4461-936d-c7dba6ceea55","_cell_guid":"e7691bee-03ab-44e1-a19a-4beda04af08d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating datasets for training","metadata":{"_uuid":"bd440016-0a8f-4152-8c85-ee7622b846f6","_cell_guid":"2850c34a-b4dd-44ce-b0c0-6cf369f021d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Erdos renyi dataset","metadata":{"_uuid":"0cdf5d69-5dfd-430d-8877-ca309d20dc06","_cell_guid":"378d4133-7a74-4f1f-ac5e-4ea21d0faca7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def generate_erdos_renyi_graphData(num_nodes=10, p=0.5):\n    edge_tensor = erdos_renyi_graph(num_nodes, p, directed=False)  # shape [2, num_edges]\n\n    # Create graph nodes, values all 0\n    nodes = torch.zeros((num_nodes, 1)) # shap4 [num_nodes, 1], any node feature is a single scalar 0.\n    # each node in this tensor is assigned an id based on its index 0...num_nodes-1 \n    # these ids are the ones used in edge_tensor, it is possible that there are no edges... \n    \n    # Create Data object for graph\n    graphData = Data(x=nodes, edge_index=edge_tensor)\n    \n    return graphData\n\ngraphDataset = generate_erdos_renyi_graphData()\n# G = to_networkx(graph, to_undirected=True)\n# plt.figure()\n# nx.draw(G, with_labels=True, node_size=500)\n# plt.show()","metadata":{"_uuid":"aa1ad950-7259-4f51-b87a-176c4d0be256","_cell_guid":"383cf5da-3709-4b97-9d3e-d2d9b3046731","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inspiration from https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html#creating-in-memory-datasets\n\nclass ErdosRenyiDataset(InMemoryDataset):\n    def __init__(self, root, num_graphs=100, transform=None, pre_transform=None, pre_filter=None):\n        self.num_graphs = num_graphs\n\n        super().__init__(root, transform, pre_transform, pre_filter)\n\n        # Generate dataset if not done already, else load it\n        processed_path = self.processed_paths[0]\n        if not os.path.exists(processed_path):\n            print(\"Creating dataset\")\n            self.process()\n        else:\n            print(\"Dataset already exists, loading it...\")\n\n        self.load(self.processed_paths[0]) # populates self.data\n        print(\"Done.\")\n\n        \"\"\"\n        self.load will create objects:\n        \n        self.slices = {\n          'x':          tensor([0, 2, 5, 6]),   # node feature offsets\n          'edge_index': tensor([0, 1, 3, 4]),   # edge index offsets\n        }\n\n        self.data = {\n          'x':          (total_num_nodes, num_node_features) # concatenation of all graphs nodes\n          'edge_index': (2, total_num_edges) # concatenation of all graph edges\n        }\n        \"\"\"\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def process(self):\n        data_list = []\n\n        for i in range(self.num_graphs):\n            data_list.append(generate_erdos_renyi_graphData())\n\n        if self.pre_filter:\n            data_list = [d for d in data_list if self.pre_filter(d)]\n            \n        if self.pre_transform:\n            data_list = [self.pre_transform(d) for d in data_list]\n\n        self.save(data_list, self.processed_paths[0]) # Runs: data, slices = cls.collate(data_list) and saves these objects","metadata":{"_uuid":"e03d14d2-cbb4-4118-b8fc-7715d3df0d82","_cell_guid":"553dc655-4da9-4490-8e4c-d9c62d79a681","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/data/ER/processed\nerdos_renyi_dataset = ErdosRenyiDataset(root='data/ER', num_graphs=10000)","metadata":{"_uuid":"9ee6da24-7aaa-4727-93e7-4dca384117e7","_cell_guid":"41f21cb5-f9e7-498c-88e1-6ddcde306e06","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of graphs: {len(erdos_renyi_dataset)}\")\n\nfor g in range(4):\n    edge_index = erdos_renyi_dataset[g].edge_index","metadata":{"_uuid":"2b342a41-cf97-4624-a98f-c726ef909515","_cell_guid":"450d6c28-759f-418a-bf06-0db32d88e730","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"edge_index[:, :1]","metadata":{"_uuid":"96ae00aa-aa4b-4d75-84b8-bf9fa2d42d98","_cell_guid":"113e4652-9a9e-4ac7-804a-9e6d2ddba079","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Planar graph dataset (generated with plantri)","metadata":{"_uuid":"8ef31986-d784-49a8-afe8-ec518ed5efaa","_cell_guid":"42370f7e-4f12-4c88-a233-22caf546f008","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Command             | Triangulations Written | CPU Time (sec)\n--------------------|------------------------|---------------\n./plantri -c3 10    | 233                    | 0.00\n./plantri -c3 11    | 1,249                  | 0.02\n./plantri -c3 12    | 7,595                  | 0.04\n./plantri -c3 13    | 49,566                 | 0.16\n./plantri -c3 14    | 339,722                | 1.11\n./plantri -c3 15    | 2,406,841              | 8.01\n./plantri -c3 16    | 17,490,241             | 61.98\n./plantri -c3 17    | 129,664,753            | 504.53","metadata":{"_uuid":"bfb95a11-0606-4778-8232-a23668860e2b","_cell_guid":"e5af35de-b882-4ce3-aa80-0981efcc2638","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import networkx as nx\n\ndef graph6_to_data(g6_path):\n    print(f\"Loading graphs from {g6_path}\")\n    \n    with open(g6_path, \"rb\") as f:\n        graph_nx = nx.read_graph6(f)\n\n    print(f\"Read {len(graphs_nx)} graphs from {g6_path}\")\n    return graphs_nx","metadata":{"_uuid":"6b89a6fb-71c8-46ce-9762-38fc324607cf","_cell_guid":"97c3cba3-a8f7-4bb2-8c6c-34c1ebed205b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inspiration from https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html#creating-in-memory-datasets\n\nclass PlanarDataset(InMemoryDataset):\n    def __init__(self, root, g6_dir=None, transform=None, pre_transform=None, pre_filter=None):\n\n        g6_files = [os.path.join(g6_dir, f) for f in os.listdir(g6_dir) if f.endswith(\".g6\")]\n        self.g6_files = g6_files\n        \n        super().__init__(root, transform, pre_transform, pre_filter)\n\n        # Generate dataset if not done already, else load it\n        processed_path = self.processed_paths[0]\n        if not os.path.exists(processed_path):\n            print(\"Creating dataset\")\n            self.process()\n        else:\n            print(\"Dataset already exists, loading it...\")\n\n        self.load(self.processed_paths[0]) # populates self.data\n        print(\"Done.\")\n\n        \"\"\"\n        self.load will create objects:\n        \n        self.slices = {\n          'x':          tensor([0, 2, 5, 6]),   # node feature offsets\n          'edge_index': tensor([0, 1, 3, 4]),   # edge index offsets\n        }\n\n        self.data = {\n          'x':          (total_num_nodes, num_node_features) # concatenation of all graphs nodes\n          'edge_index': (2, total_num_edges) # concatenation of all graph edges\n        }\n        \"\"\"\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def process(self):\n        data_list = []\n\n        for file in self.g6_files:\n            nx_graphs = graph6_to_data(file)\n            data_list += [from_networkx(g) for g in nx_graphs]\n\n        if self.pre_filter:\n            data_list = [d for d in data_list if self.pre_filter(d)]\n            \n        if self.pre_transform:\n            data_list = [self.pre_transform(d) for d in data_list]\n\n        self.save(data_list, self.processed_paths[0]) # Runs: data, slices = cls.collate(data_list) and saves these objects","metadata":{"_uuid":"50b156a1-9204-4113-89b5-75b0fb8b40c5","_cell_guid":"ea5d0e8a-245b-4a2b-9c2e-282774968a79","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#planar_dataset = PlanarDataset(root='data/PL', g6_dir=\"/kaggle/input/planar-graphs-g6-dataset\")","metadata":{"_uuid":"c2b0dcf1-0fde-4d83-abe2-96f8769a1d01","_cell_guid":"b38241f9-c677-47b6-97fc-502693c4d701","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Coarsening sampling","metadata":{"_uuid":"edc681b7-2d3e-4912-a93a-8a5edee8dec7","_cell_guid":"08f1c321-2a65-47f4-b35f-b05e09fc960b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"We need to sample coarsening sequences from $\\Pi_F(G)$ that only use contraction sets in $F(G) = \\mathcal{E}$, meaning each coarsening step only merges some edges of the graph. \n\nWe sample in $\\Pi_F(G)$ by sampling coarsening sequences from a \"latent\" distribution $q(\\pi | G)$. To sample from $q(\\pi | G)$, we associate a cost to each possible coarsening set at all coarsening steps, for now the uniform cost.","metadata":{}},{"cell_type":"markdown","source":"The below function extracts all possible coarsening sets at a given step (given a graph).","metadata":{}},{"cell_type":"code","source":"def get_candidate_contraction_sets(G: Data):\n    # make sure no self loop on nodes here!\n    edge_index = G.edge_index # 2, num_edges (tensor)\n    u, v       = edge_index[0], edge_index[1]\n\n    # remove self edges\n    nonself = (u != v)\n    u_ns, v_ns = u[nonself], v[nonself]\n\n    candidates = torch.stack([u_ns, v_ns], dim=1)\n    return candidates\n\n# test\ntest_graph = erdos_renyi_dataset[10]\nnx.draw(to_networkx(test_graph), with_labels=True)\n\ntest_candidates = get_candidate_contraction_sets(test_graph)\nprint(test_candidates[:10])\n\ntest_current_num_nodes = test_graph.num_nodes\nprint(\"Total number of nodes:\", test_current_num_nodes)","metadata":{"_uuid":"b68069bb-f9f0-4789-93cd-f984dbfeeaa0","_cell_guid":"b9f082c0-c8dd-4b3c-8c3b-ebf25a12aad1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function creates a dense adjency matrix given a graph by looking at all of its edges.","metadata":{}},{"cell_type":"code","source":"def get_graph_adj(edge_index: torch.LongTensor, num_nodes: int):\n    adj = to_dense_adj(edge_index, max_num_nodes=num_nodes)\n    adj = adj[0]\n    return adj\n\n# test\ntest_graph_adj = get_graph_adj(test_graph.edge_index, test_graph.num_nodes)\n#indices = (graph_adj == 1).nonzero(as_tuple=False)\nprint(test_graph_adj)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:red\">The uniform cost should be replaced by  local-variation cost of Loukas (2018) from the paper otherwise model won't train well!</span>","metadata":{}},{"cell_type":"code","source":"def get_uniform_cost(candidate_contractions: torch.LongTensor, graphData: Data): # can also be based on G0, P1...P{l-1}\n    # graphData not used, just used for compatibility\n    num_candidates = candidate_contractions.shape[0]\n\n    return torch.tensor([np.random.rand() for _ in range(num_candidates)], dtype=torch.float32)\n\n# test\nget_uniform_cost(test_candidates, test_graph)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Better cost function inspired from the paper. Try to conserve structure. The idea is that after computing eigenvectors of the laplacian, in each vector, the difference between coordinates i (first node) and j (second node) is either high or low. If low, this means the two nodes diffuse signals similarly? if high, they shouldnt be removed. We compute the sum of squared differences and   ","metadata":{}},{"cell_type":"code","source":"def get_improved_cost(\n    candidate_contractions: torch.LongTensor,   # [E', 2]  candidate contractions\n    graphData: Data,\n    k:int = 8,\n) -> torch.Tensor:                  # → [E']  cost per edge\n    \"\"\"\n    Cheap surrogate of Loukas' local-variation cost.\n    Computes ‖A[u]-A[v]‖₂ for every candidate edge (u,v).\n    \"\"\"\n    edge_index = graphData.edge_index\n    num_nodes = graphData.num_nodes\n    \n    adj = get_graph_adj(edge_index, num_nodes)\n    deg = adj.sum(1)\n    L   = torch.diag(deg) - adj\n\n    # First-k eigenvectors\n    evals, evecs = torch.linalg.eigh(L)\n    F = evecs[:, :min(k, num_nodes-1)] # [N, k]\n\n    u, v = candidate_contractions.t() # [2, E'] -> [E'] and [E']\n    diffs = F[u] - F[v] # [E',k] - [E',k]\n    \n    cost  = (diffs**2).sum(dim=1) #[E']\n\n    return cost\n\n# test\nget_improved_cost(test_candidates, test_graph)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is the implementation of the rnd_greedy_min_cost_part algorithm 2 from the paper. Basically this algorithm runs at each step, and given number of nodes to remove, iteratively selectes the best contraction set (edges in our case) to use for coarsening, until the number of nodes to remove is obtained.","metadata":{}},{"cell_type":"code","source":"def rnd_greedy_min_cost_part(\n    candidate_contractions: torch.LongTensor, \n    cost_function, \n    current_graph, # added this parameter to compute cost, it contains current_graph on which coarsening is done\n    num_nodes_to_remove: int, \n    current_num_nodes: int,\n    rand_lambda: float = 1.0,\n):\n    \"\"\"\n    Algo 2\n    candidate_contractions [num_candidates, 2] each candidate is a pair of nodes (edge)\n    Greedily picks contraction candidates until at least num_nodes_to_remove are merged.\n    \"\"\"\n\n    num_candidates = candidate_contractions.size(0)    \n    alive = torch.ones(num_candidates, dtype=torch.bool) # check for already selected contractions, True means not selected\n    marked  = torch.zeros(current_num_nodes, dtype=torch.bool) # mark nodes already contracted...\n    \n    costs  = cost_function(candidate_contractions, current_graph)\n    \n    # each cost is a rand float between 0 and 1    \n\n    picked = [] # picked contractions idx\n    removed = 0\n\n    #print(f\"inside rnd_greedy_min_cost_part: i need to remove {num_nodes_to_remove} nodes\")\n    while (removed < num_nodes_to_remove and alive.any()):\n        masked_costs = costs.masked_fill(~alive, float('inf'))\n        idx = torch.argmin(masked_costs).item()\n        \n        if torch.rand(1).item() > rand_lambda: # simulate Bernoulli, after selecting an edge to remove, sample a Bernoulli and while it is not 0 never select it again and continue process\n            alive[idx] = False\n            continue \n\n        selected_nodes = candidate_contractions[idx] # this contains the selected edge (pair of nodes) to remove\n        \n        if marked[selected_nodes].any(): # maybe one of the two selected nodes is already used in some other edge that needs to be removed, then never select edge again and continue process\n            alive[idx] = False\n            continue\n\n        # otherwise mark it as selected set both of the nods to unalive and continue\n        \n        marked[selected_nodes] = True\n        alive[idx] = False\n        picked.append(idx)\n        #print(f\"We find a new edge to remove: {candidate_contractions[idx]}, removing it will remove: {selected_nodes.numel() - 1} nodes.\")\n        removed +=  selected_nodes.numel() - 1 # merging always keeps one node\n        #print(f\"inside rnd_greedy_min_cost_part: fo far i removed {removed} nodes.\")\n\n    #print(f\"I might have stopped because alive.any(): {alive.any()}\")\n    if picked:\n        picked_idx = torch.tensor(picked, dtype=torch.long)\n        return candidate_contractions[picked_idx]\n    else:\n        print(\"rnd_greedy_min_cost_part Error\")\n\n# test\n# for remove_n in range(1, current_num_nodes // 2): # max number of removable nodes\n#     test_partitioning = rnd_greedy_min_cost_part(test_candidates, get_cost, remove_n, current_num_nodes)\n#     print(f\"Removed {remove_n} nodes with contractions: {test_partitioning}\")\n    \ntest_num_nodes_to_remove = 2\n# uniform cost also possible with  get_uniform_cost instead of get_improved_cost\ntest_partitioning = rnd_greedy_min_cost_part(test_candidates, get_improved_cost, test_graph, test_num_nodes_to_remove, test_current_num_nodes)\nprint(test_partitioning)","metadata":{"_uuid":"ca6bee01-5e0f-4080-9a3b-ac0cc6a51f6b","_cell_guid":"2e420f0d-5e2b-4e13-ba53-fc99de809af5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is a visualization function that assigns a unique color to each cluster id (as long as there are less than 34 clusters in total), and then displays the graph by coloring clusters (all nodes in a given cluster will share that cluster color). ","metadata":{}},{"cell_type":"code","source":"def visualize_clusters(G: Data, partitioning: torch.Tensor):\n    # Convert to NetworkX\n    if getattr(G, 'edge_attr', None) is not None:\n        G_nx = to_networkx(G, to_undirected=True, edge_attrs=[\"edge_attr\"])\n        raw_attrs = {\n            (u, v): int(d[\"edge_attr\"])\n            for u, v, d in G_nx.edges(data=True)\n        }\n    else:\n        G_nx = to_networkx(G, to_undirected=True)\n        raw_attrs = {}\n\n    # Create a color map for each node\n    color_map = ['lightgray'] * G.num_nodes  # default color\n\n    # Assign a unique color for each cluster\n    colors = [\n        'red', 'green', 'blue', 'orange', 'purple', 'cyan', 'yellow', 'brown',\n        'pink', 'olive', 'chocolate', 'lime', 'navy', 'teal', 'coral', 'gold',\n        'magenta', 'orchid', 'salmon', 'darkgreen', 'deepskyblue', 'slateblue',\n        'violet', 'maroon', 'turquoise', 'indigo', 'darkorange', 'crimson',\n        'steelblue', 'darkviolet', 'khaki', 'cadetblue', 'hotpink', 'mediumseagreen'\n    ]\n\n    cluster_id_to_color = {}\n    \n    for cluster_id, cluster in enumerate(partitioning):\n        color = colors[cluster_id % len(colors)]\n        cluster_id_to_color[cluster_id] = color\n        for node in cluster.tolist():\n            color_map[node] = color\n        print(f\"Cluster {cluster_id} has color {colors[cluster_id % len(colors)]}\")\n\n    edge_colors = []\n    for u, v in G_nx.edges():\n        a = raw_attrs.get((u, v), None)\n        edge_colors.append('red' if a == -1 else 'black')\n\n    # Draw the graph\n    pos = nx.spring_layout(G_nx, seed=42)\n    nx.draw(G_nx, pos, node_color=color_map, with_labels=True, edge_color=edge_colors)\n    plt.show()\n\n    return cluster_id_to_color","metadata":{"_uuid":"4bc1543a-9d19-447b-85ae-940fc31ab97d","_cell_guid":"4876d36b-c9bc-49a1-9bb7-4923d66a4005","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_cluster_id_to_color = visualize_clusters(test_graph, test_partitioning)\nprint(f\"Cluster 0 has color {test_cluster_id_to_color[0]}\")","metadata":{"_uuid":"cff014b3-6aa1-46e0-b380-b7f337da1d55","_cell_guid":"b2733eaa-1869-46f2-910f-f9f0ed7baa70","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def has_edge_undirected(edge_index, u, v):\n    src, dst = edge_index\n    return ((src == u) & (dst == v)).any() or ((src == v) & (dst == u)).any()\n\nhas_edge_undirected(test_graph.edge_index, 3, 5)","metadata":{"_uuid":"435f64d3-bd56-4222-abc0-2f7018d2421b","_cell_guid":"edf68720-6375-4481-b44c-c2d0571eb4f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function completes a partitioning tensor containing all coarsening sets (pairs of nodes forming an edge that need to be merged) with the remaining sets that will not be merged for future code compatibility. The remaining sets not to be merged will all contain leftover nodes stacked with themselves, and concatenated at the end of the partitioning tensor. For instance one left over node i will be concatenated as [i, i].","metadata":{}},{"cell_type":"code","source":"def augment_with_singletons(partitioning: torch.LongTensor, num_nodes: int, debug=False):\n    \"\"\"\n    Given partitioning of shape [P,2]\n    return a new LongTensor of shape [P+U, K'] where rows 0..P-1\n    are your old clusters, and rows P..P+U-1 each contain exactly\n    one leftover node as a single cluster.\n    \"\"\"\n    # flatten and find which nodes were clustered\n    clustered = partitioning.unique() # no dim specified -> unique of the flattened partitioning\n    num_initial_clusters = partitioning.shape[0]\n    \n    all_nodes = torch.arange(num_nodes, device=partitioning.device)\n    \n    mask = torch.ones(num_nodes, dtype=torch.bool, device=partitioning.device)\n    mask[clustered] = False\n    \n    leftover = all_nodes[mask]  # shape [U]\n\n    # now make each leftover its own cluster of size 1, with itself, little trick\n    singletons = torch.stack([leftover, leftover], dim=1) # [U, 2]\n    \n    if singletons.numel() > 0:\n        partitioning = torch.cat([partitioning, singletons], dim=0)\n\n        if debug:\n            for i, node_id in enumerate(leftover.tolist()):\n                cluster_id = num_initial_clusters + i\n                print(f\"Leftover node {node_id} assigned to singleton cluster {cluster_id}\")\n\n    return partitioning # ideally also explain what id each solo cluster got for coloring later!\n\n# test \nprint(f\"Number of clusters before: {test_partitioning.shape[0]}\")\ntest_augmented_partitioning = augment_with_singletons(test_partitioning, test_current_num_nodes, debug=True)\nprint(f\"Number of clusters after: {test_augmented_partitioning.shape[0]}\")","metadata":{"_uuid":"7be62fdd-7eed-4370-a519-17525ad8227c","_cell_guid":"d34a0124-c2c6-4ffb-b0c8-e8e0cb9328ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function builds a matrix P mapping all nodes to their clusters (left over nodes are considered as single node clusters) returns [num_clusters, N] for N nodes.","metadata":{}},{"cell_type":"code","source":"def build_P(num_nodes: int,\n            partitioning: torch.Tensor):\n    \"\"\"\n    num_nodes: N\n    partitioning: list of LongTensors, each contains original node‑indices cluster (max 2 nodes per cluster since we only consider edges).\n    Returns P of shape [num_clusters, N], where P[c,i]=1 if i belongs to coarse‑node c.\n    \"\"\"\n    num_clusters = len(partitioning)\n\n    P = torch.zeros((num_clusters, num_nodes), dtype=torch.float32)\n    for c, nodes in enumerate(partitioning):\n        P[c, nodes] = 1.0\n    return P\n\n# test\ntest_P = build_P(test_current_num_nodes, test_augmented_partitioning)\nprint(test_P)","metadata":{"_uuid":"7c3c0997-5129-4d41-9d51-bde0709421ee","_cell_guid":"970dc061-7e69-4367-942b-8e45fee05a64","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Each row corresponds to one cluster id, each column corresponds to one node. This can be reused to compute the expansion step GT as each cluster super node will need to be expanded back to however many nodes it contains.","metadata":{"_uuid":"ff09fc72-468e-49bc-91d6-359be37e2413","_cell_guid":"39377340-43ca-46ee-a1c2-217122648bdb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"The below function creates an adjency matrix for clusters. Two clusters which initially contained nodes connected to each other will end up connected. Self loops for one cluster are removed by filling the diagonal of this matrix with 0s.","metadata":{}},{"cell_type":"code","source":"def get_cluster_adj(P: torch.FloatTensor,\n                        A: torch.FloatTensor):\n    # P: [C, N]; A: [N, N]\n    M = P @ A @ P.t()       # [C, C], computes inter cluster links, if any two clusters initially had nodes connected they will be connected\n    M.fill_diagonal_(0)\n    return (M > 0).to(torch.float32)\n\n# test\ntest_cluster_adj = get_cluster_adj(test_P, test_graph_adj)\nprint(test_cluster_adj)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function coarsens a graph given a partitioning containing coarsening sets (a set can be a cluster containing two nodes to merge, or a left over node copied twice that shouldn't be merged). It uses previous functions to returned the coarsened graph as well as the node-cluster mapping matrix that was used to build it.","metadata":{}},{"cell_type":"code","source":"def coarsen_graph(G: Data, partitioning: torch.LongTensor):\n    \"\"\"\n    Merge each row of partitioning into a single super‐node.\n    Unclustered nodes are nodes j not appearing in any row of partitioning.\n    Edges between any two super‐nodes exist iff any member nodes in both were previously connected in G.\n    G{l}->G{l+1}\n    \"\"\"\n    \n    N = G.num_nodes\n\n    augmented_partitioning = augment_with_singletons(partitioning, N)\n    \n    P = build_P(N, augmented_partitioning) # map nodes to clusters\n    graph_adj = get_graph_adj(G.edge_index, N) # get initial graph adjency\n    cluster_adj = get_cluster_adj(P, graph_adj) # get clusters adjency\n\n    edge_index, edge_attr = dense_to_sparse(cluster_adj) # convert cluster adj to list of edges\n    num_clusters = cluster_adj.size(0)\n    \n    x = torch.zeros((num_clusters, 1), dtype=torch.float32) # creates a list of cluster nodes\n    return Data(x=x, edge_index=edge_index, num_nodes=num_clusters), P # returns a new graph using list of cluster nodes and how they should be connected + the node-cluster mapping matrix \n\n# test\ntest_coarsened_graph, test_P = coarsen_graph(test_graph, test_partitioning)\n\ndef get_coarse_node_colors(num_clusters: int, cluster_id_to_color: dict) -> list[str]:\n    \"\"\"\n    Returns a list of colors for super-nodes in the coarsened graph.\n    Each super-node corresponds to a cluster ID.\n    \"\"\"\n    color_map = []\n    for cluster_id in range(num_clusters):\n        color = cluster_id_to_color.get(cluster_id, 'lightgray')\n        color_map.append(color)\n    return color_map\n\ntest_num_clusters = test_coarsened_graph.num_nodes\ntest_coarse_colors = get_coarse_node_colors(test_num_clusters, test_cluster_id_to_color)\n\n# Step 3: Plot the coarsened graph\nnx.draw(to_networkx(test_coarsened_graph), with_labels=True, node_color=test_coarse_colors)","metadata":{"_uuid":"9502285e-1ca6-43d4-a5a0-5281a1d881f9","_cell_guid":"858d2530-f405-4599-b18c-705feb5484dd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A sequence of coarsening steps will always reach a single node graph eventually since the starting graph is fully connected and the only way to reach one node is to merge a graph containing two nodes (one edge).","metadata":{}},{"cell_type":"code","source":"curr = 1\nfor i in range(10):\n    print(curr)\n    curr = curr-math.ceil(0.10*curr) # math.ceil(0.10*curr) represents the number of nodes to remove, it is computed in the algorithm that follows.","metadata":{"_uuid":"9a72116c-0026-4a15-91ab-2409e38f1954","_cell_guid":"a37dd7d7-a5b2-4d57-b4df-01fc9f3f8adf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function implements rnd_red_seq (algorithm 1) from the paper. It takes as input the initial graph and returns the list of coarsened graphs at each step up to the final single node graph, as well as the contractions history (the node-cluster mappings for building each intermediate coarsened graph).","metadata":{}},{"cell_type":"code","source":"def rnd_red_seq(G0: Data, min_red_frac=0.1, max_red_frac=0.3, debug=False): # it is not possible to remove more than current_num_nods//2 with edge contraction\n    \"\"\"\n    Algo 1\n    Returns random coarsening sequence π = (G0, . . . , GL) ∈ ΠF (G) \n    \"\"\"\n\n    current_graph = G0\n    \n    if debug: print(f\"Initial graph has {G0.num_nodes} nodes.\")\n    coarsening_seq = [current_graph] # π, will end up containing (G0...GL)\n    l = 0\n\n    contractions_history = [] # will contain (P1...PL-1) where Pi is the partitioning matrix used to coarsen Gi into G{i+1}\n    \n    while current_graph.num_nodes != 1: # removing % of current_num_nods to current_num_nods will always be bigger than 0 and math.ceil \n        # will end up being one until current_num_nods becomes 1 -> last node will be removed \n        l += 1\n        reduction_fraction = torch.tensor([1.0])\n        reduction_fraction.uniform_(min_red_frac, max_red_frac)  # ρ\n\n        # f already defined as get_cost, for now does not depend on G0/contractions_history\n        num_nodes_to_remove = math.ceil(reduction_fraction * current_graph.num_nodes) # m\n\n        candidates = get_candidate_contraction_sets(current_graph) # F(G{l−1}) fetch all possible contractions\n        if candidates.numel() == 0:\n            print(\"Error no more edges to contract in rnd_red_seq\")\n            break\n        \n        partitioning_l = rnd_greedy_min_cost_part(\n            candidate_contractions=candidates,\n            cost_function=get_improved_cost,\n            current_graph=current_graph,\n            num_nodes_to_remove=num_nodes_to_remove,\n            current_num_nodes=current_graph.num_nodes\n        ) # tensor [C0, C1, …, Ck] # [num_clusters, 2]\n\n        if partitioning_l.numel() == 0:\n            print(\"No contractions picked\")\n            continue\n\n        # coarsen graph\n        if debug: print(f\"Coarsening graph and trying to remove {num_nodes_to_remove} nodes.\") # sometimes in some shapes such as S3 (star 4 nodes, one central) we cannot remove two nods\n            # if we pick the center one...\n        current_graph, P = coarsen_graph(current_graph, partitioning_l)\n        if debug: print(f\"Coarsened graph has {current_graph.num_nodes} nodes.\")\n\n        coarsening_seq.append(current_graph)\n        contractions_history.append(P) \n\n    return coarsening_seq, contractions_history\n\n# test\ntest_coarsening_seq, test_contractions_history = rnd_red_seq(test_graph, debug=True)\nprint(len(test_coarsening_seq), len(test_contractions_history))\n\nprint(\"\\nCoarsening sequence:\")\npprint(test_coarsening_seq)\n\nprint(\"\\nContractions history:\")\npprint(test_contractions_history)","metadata":{"_uuid":"c728b321-2556-4334-9ddd-1097df09cfb0","_cell_guid":"d1e16645-c4d1-4659-b043-0c4743eee22d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Retrieving data needed for expansion","metadata":{"_uuid":"eba9d668-0b19-415c-926d-00e7acd939e7","_cell_guid":"4b791cea-f78a-46dc-8b22-3f39b6a1e169","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"for g in erdos_renyi_dataset[0:5]:\n    print(g)","metadata":{"_uuid":"845aa34c-36cf-4af8-a933-f16840c416e7","_cell_guid":"d051e53c-023b-4b8b-9ab9-e0136e5d156b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function computes the cluster counts (v) given a node/cluster mapping matrix. Since we only use clusters of 1/2 nodes -> we reformulate the problem as classification between -1 if node shouldn't be expanded (cluster size 1) , 1 if node should be expanded (cluster size 2) ","metadata":{}},{"cell_type":"code","source":"def compute_node_embeddings(P):\n    # Gl is coarsened into Glp1 using Pl\n    # Pl is given as input\n    # output is the expansion vector v used to come back to Gl from Glp1\n    # so it should have as many nodes as coarsened graph Gl!\n    cluster_counts = P.sum(dim=1).long() # v_l\n    #assign = P.argmax(dim=0)\n    #v = cluster_counts[assign]\n    v = cluster_counts-2\n    return v\n\n# test\nprint(\"P:\")\nprint(test_P.shape)\n\ntest_v = compute_node_embeddings(test_P)\nprint(\"Result:\\n\", test_v.shape)","metadata":{"_uuid":"fe3b9df4-04c7-441e-a134-c2b1360229b1","_cell_guid":"5d0db520-6c75-4b9c-b52f-f3ba4fa4b673","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function expands a coarsened graph based on the node/cluster mapping matrix that was used to obtain that coarsened graph Gc, it returns the adjency matrix of the expanded graph.","metadata":{}},{"cell_type":"code","source":"def expand_coarsened(Gc: Data, P):\n    \"\"\"\n    Takes Gc coarsened graph as input and\n    P the node-cluster mapping matrix used to obtain Gc\n    \"\"\"\n    device = P.device\n    \n    C, N = P.shape\n    \n    coarsened_adj = get_graph_adj(Gc.edge_index, Gc.num_nodes) # [C, C]\n\n    # for each node i find its cluster id\n    clusters = P.argmax(dim=0)  # [N]  each in 0..C-1\n\n    # # intercluster connections \n    # for i in range(N):\n    #     for j in range(N):\n    #         if clusters[i] != clusters[j]:\n    #             ci, cj = clusters[i], clusters[j]\n    #             if coarsened_adj[ci, cj] == 1:\n    #                 expanded_adj[i, j] = 1\n    #                 expanded_adj[j, i] = 1\n    #         else:\n    #             expanded_adj[i, j] = 1\n    #             expanded_adj[j, i] = 1\n\n    # np.fill_diagonal(expanded_adj, 0)\n    # return expanded_adj\n    \n    # chatgpt optimized version using broadcasting\n    idx_i = clusters.unsqueeze(1)  # [N,1]\n    idx_j = clusters.unsqueeze(0)  # [1,N]\n\n    # intercluster mask\n    inter_mask = coarsened_adj[idx_i, idx_j].to(torch.int) # [N, N] \n    \n    # intracluster mask\n    intra_mask = idx_i == idx_j.to(torch.int) # [N, N]\n\n    # combine and zero out diagonal no self loop\n    expanded = (inter_mask | intra_mask).to(device) \n    expanded.fill_diagonal_(False)\n\n    return expanded\n    \n# test\ntest_expanded_graph_adj = expand_coarsened(test_coarsened_graph, test_P)\nprint(test_expanded_graph_adj)\nprint(test_P.shape)\nprint(test_coarsened_graph.num_nodes)\nprint(test_expanded_graph_adj.shape)","metadata":{"_uuid":"76d2418d-1c10-45ff-a60a-51213995d501","_cell_guid":"8b409003-57ba-4d96-812c-1dc0a298eb85","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:red\">Should we realy remove self loops here? expanded.fill_diagonal_(False) in the paper diagram for PPGN self loops are kept eventhough the model should learn to remove them of course...</span>","metadata":{}},{"cell_type":"markdown","source":"Below is the original test graph that we coarsen using the colored clusters.","metadata":{}},{"cell_type":"code","source":"print(\"Original clustering\")\ntest_cluster_id_to_color = visualize_clusters(test_graph, test_partitioning)","metadata":{"_uuid":"9915deee-894e-4ed5-825b-17f1660ddd43","_cell_guid":"04db82d4-2965-499d-b54b-c5f0f816d618","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_cluster_id_to_color","metadata":{"_uuid":"f3cb5799-ecc7-49c9-82b7-ce3d54aa3962","_cell_guid":"f42e4461-bc72-4bbb-bef0-b1ca8180b2a9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is the coarsened graph obtained from the previous clusters + original graph.","metadata":{}},{"cell_type":"code","source":"print(\"Original coarsened graph\")\nnx.draw(to_networkx(test_coarsened_graph), with_labels=True, node_color=test_coarse_colors)","metadata":{"_uuid":"1be47c76-31e7-4448-a337-a6ef52f56563","_cell_guid":"73d4db0a-fe6d-4c64-86e3-60cc6ccfbc60","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is a function which given the adjency matrix of some expanded graph $\\tilde{G}_l$  creates the refined graph Data object $G_l$.  $\\tilde{G}_l$ was obtained after expanding some $\\tilde{G}_{l+1}$ using $v_{l+1}$. The new graph should contain the same edges as the expanded graph, but with labels indicating whether we should keep (1) or remove/refine them (-1). All edges can be represented as $e_l$. Node embeddings will then contain for each node the future expansion counts required, all node embeddings can therefore be represented as $v_l$ later on.","metadata":{}},{"cell_type":"code","source":"def refine_expanded_graph(expanded_adj: torch.Tensor, original_adj: torch.Tensor, v: torch.Tensor) -> Data:\n    \"\"\"\n    expanded_adj: [N, N] dense adjacency (0/1)\n    expanded_adj: [N, N] dense adjacency (0/1)\n    v are the expansion counts for all nodes in graph Gl tilde, used to obtain Glm1\n    \n    returns: Data(x=[N,1], edge_index=[2, E])\n    \"\"\"\n    edge_index, edge_attr = dense_to_sparse(expanded_adj)\n\n    edge_labels = original_adj[edge_index[0],edge_index[1]].long() # [E], 1 if exists in original 0 otherwise\n    edge_labels = edge_labels * 2 - 1 # remap 0/1 to -1/1, -1 means should be removed...\n\n    #x = torch.zeros((expanded_adj.size(0), 1), dtype=torch.float)\n    x = torch.zeros((expanded_adj.size(0), 1), dtype=torch.float)\n    y = v.float()\n\n    return Data(x=x, edge_index=edge_index, edge_attr=edge_labels, y=y)\n\ntest_refinedGraphData = refine_expanded_graph(test_expanded_graph_adj, test_graph_adj, test_v)\nprint(test_refinedGraphData.num_nodes)\nprint(\".x:\", test_refinedGraphData.x.shape)\nprint(\".y:\", test_refinedGraphData.y.shape)\nprint(\"edge_attr:\", test_refinedGraphData.edge_attr)","metadata":{"_uuid":"39785a57-49ad-4ab0-916c-2a74e2454eab","_cell_guid":"005394de-6db7-4192-8243-304b6fe7b265","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Expanded and refined (red) graph coarsened graph\")\ntest_cluster_id_to_color = visualize_clusters(test_refinedGraphData, test_partitioning)","metadata":{"_uuid":"bcb82d50-1869-4289-9307-17eb6b40a84c","_cell_guid":"0d2940e1-0bad-4f6d-a457-a879966a0738","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can extract from the refined graph the vector $e_l$ and for each edge print its -1/1 value.","metadata":{}},{"cell_type":"code","source":"edge_index = test_refinedGraphData.edge_index   # shape [2, E]\nedge_attr  = test_refinedGraphData.edge_attr    # shape [E] or [E,1]\n\nfor i in range(edge_index.size(1)):\n    u = edge_index[0, i].item()\n    v = edge_index[1, i].item()\n    a = edge_attr[i].item() if edge_attr.ndim == 1 else edge_attr[i,0].item()\n    print(f\"Edge ({u:2d}, {v:2d})  → attr = {a}\")","metadata":{"_uuid":"bafa8104-e935-4ecf-a099-c0b56c7f4db9","_cell_guid":"41366e98-6977-46f3-8e21-edbe13d25a1a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating SignNet node embeddings from graph for conditional diffusion","metadata":{}},{"cell_type":"markdown","source":"This code is adapted from their implementation.","metadata":{}},{"cell_type":"code","source":"class MLP(Module):\n    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n\n    Args:\n        in_features (int): Number of features of the input.\n        hidden_features (list[int]): List of the hidden features dimensions.\n        out_features (int, optional): If not `None` a projection layer is added at the end of the MLP. Defaults to `None`.\n        bias (bool, optional): Whether to use bias in the linear layers. Defaults to `True`.\n        norm_layer (Module, optional): Normalization layer to use. Defaults to `norm_layer`.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: list[int],\n        out_features: int | None = None,\n        bias: bool = True,\n        norm_layer=LayerNorm,\n    ):\n        super().__init__()\n        lin_layers = []\n        norm_layers = []\n        hidden_in_features = in_features\n        for hidden_dim in hidden_features:\n            lin_layers.append(Linear(hidden_in_features, hidden_dim, bias=bias))\n            norm_layers.append(norm_layer(hidden_dim))\n            hidden_in_features = hidden_dim\n\n        self.out_layer = (\n            Linear(hidden_in_features, out_features, bias=bias)\n            if out_features is not None\n            else None\n        )\n\n        self.lin_layers = ModuleList(lin_layers)\n        self.norm_layers = ModuleList(norm_layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for lin, norm in zip(self.lin_layers, self.norm_layers):\n            x = lin(x)\n            x = norm(x)\n            x = torch.relu(x)\n\n        if self.out_layer is not None:\n            x = self.out_layer(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SignNet(Module):\n    def __init__(\n        self,\n        num_eigenvectors: int,\n        hidden_features: int,\n        out_features: int,\n        num_layers: int,\n        dropout: float = 0.0,\n    ) -> None:\n        super().__init__()\n\n        self.in_layer = Linear(2, hidden_features)\n        self.conv_layers = ModuleList(\n            [\n                GINConv(\n                    MLP(hidden_features, [hidden_features, hidden_features]),\n                    train_eps=True,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        self.skip_layer = Linear(hidden_features * (num_layers + 1), hidden_features)\n        self.dropout = Dropout(dropout)\n        # the following corresponds to the ρ function in the paper\n        self.merge_layer = MLP(\n            in_features=num_eigenvectors * hidden_features,\n            hidden_features=[hidden_features, hidden_features],\n            out_features=out_features,\n        )\n\n    def forward(self, spectral_features, edge_index):\n        \"\"\"Forward pass of the model.\n\n        Args:\n            spectral_features (Tensor): Eigenvalues (repeated) concatenated with eigenvectors. Shape: :math:`(V, num_eigenvectors * 2)`.\n            edge_index (Adj): Adjacency matrix given as edge index or sparse tensor. Shape: :math:`(2, E)` or :math:`(V, V)`.\n\n        Returns:\n            Tensor: Node features. Shape: :math:`(V, out_features)`.\n        \"\"\"\n        # Stack spectral features\n        eigenvalues_repeated, eigenvectors = spectral_features.chunk(\n            2, dim=-1\n        )  # (V, k), (V, k)\n\n        positive_spectral_features = torch.stack(\n            [eigenvalues_repeated, eigenvectors], dim=-1\n        )  # V, k, 2\n        negative_spectral_features = torch.stack(\n            [eigenvalues_repeated, -eigenvectors], dim=-1\n        )  # V, k, 2\n        combined_spectral_features = torch.stack(\n            [positive_spectral_features, negative_spectral_features]\n        ).transpose(\n            1, 2\n        )  # 2, k, V, 2\n\n        # Apply layers\n        x = self.in_layer(combined_spectral_features)  # 2, k, V, hidden_features\n        xs = [x]\n        for conv in self.conv_layers:\n            # apply conv layer to each spectral feature independently\n            x = conv(x=x, edge_index=edge_index)\n            xs.append(x)\n\n        # Skip connection\n        x = torch.cat(xs, dim=-1)\n        x = self.dropout(x)\n        x = self.skip_layer(x)  # 2, k, V, hidden_features\n        # Make sign invariant\n        x = x.sum(dim=0)  # k, V, hidden_features\n\n        # Merge features\n        x = x.transpose(0, 1)  # V, k, hidden_features\n        x = self.merge_layer(x.reshape(x.size(0), -1))  # V, out_features\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define model\nglobal_k = 8 # means the model takes as input 2x 32 = 64, we select first 32 eigenval/eigenvectors for all graphs\n# and pad with 0s if needed, one node i will get [lambda 1, ..., lambda num_nodes-1, 0 ,..., u1(i), ..., un-1(i), 0, ..., 0]\n\n# model input should be (V,2k) where k eigenvectors were selected, since some graphs might have less nodes than global_k we pad them with 0s...\n# ideally with a big k we always have k>V and for each node the input vector will be 2k with first k values : [lambda 1, ..., lambda V, 0...0,     eigevec_0, ...., eigevec_V, 0...0]\n\n# basically for each node, \nembedding_model = SignNet(\n    num_eigenvectors=global_k,\n    hidden_features=64,\n    out_features=128,\n    num_layers=3,\n    dropout=0.1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_spectral_features_torch(data, k=global_k):\n    N = data.num_nodes\n\n    if N==1: return torch.randn((1, 2 * k), device=data.x.device)\n\n    # Get Laplacian\n    lap_edge_index, lap_edge_weight = get_laplacian(data.edge_index, num_nodes=N, normalization='sym') # computes laplacian and returns as edge_index with weights on each edge (pair of nodes)\n    \n    L_dense = to_dense_adj(lap_edge_index, edge_attr=lap_edge_weight, max_num_nodes=N).squeeze(0) # convert to matrix form\n    # max_num_nodes guarantees the dense matrix is N×N even when some rows/columns are completely zero.\n    \n    # Compute eigendecomposition\n    eigvals, eigvecs = torch.linalg.eigh(L_dense)  # eigenvalues sorted in ascending order, computes all 50 + their corresponding eigen vectors\n    # shapes [num_nodes] and [num_nodes, num_nodes] resp. one column = one eigen vector\n    \n    # Take k smallest eigen values and corresponding vectors\n    eigvals = eigvals[1:k+1] # [k,] (remove the 0 eigen value) / [num_nodes-1,] if num_nodes-1<k...\n    eigvals = eigvals.unsqueeze(0) #[1,k]\n    eigvals = eigvals.repeat(data.num_nodes, 1) # num_nodes, k / num_nodes, num_nodes-1  if num_nodes-1<k...\n\n    eigvecs = eigvecs[:, 1:k+1] # [num_nodes, k] remove eigen vector linked with 0 eigen value /  [num_nodes, num_nodes-1] if num_nodes-1<k\n    \n    num_nodes = eigvecs.shape[0]\n    num_vecs = eigvecs.shape[1] # could be num_nodes-1 and not k if  num_nodes-1<k    \n\n    if num_vecs<k:\n        # prepare padding matrix for both eigen values and eigen vectors\n        diff = k-num_vecs\n        pad_vals = torch.zeros((N, diff), device=eigvals.device)\n        pad_vecs = torch.zeros((N, diff), device=eigvals.device)\n        \n        eigvals = torch.cat([eigvals, pad_vals], dim=1) # num_nodes, k\n        eigvecs = torch.cat([eigvecs, pad_vecs], dim=1) # num_nodes, k\n\n    try:\n        return torch.cat([eigvals, eigvecs], dim=1) # (N, 2k)\n    except RuntimeError as err:\n        print(\"\\n!!! spectral cat() failed !!!\")\n        print(\"graph id        :\", data)\n        print(\"data.num_nodes  :\", data.num_nodes)\n        print(\"eigvals shape   :\", eigvals.shape)\n        print(\"eigvecs shape   :\", eigvecs.shape)\n        print(\"full traceback follows\\n\")\n        raise\n\n# test\nget_spectral_features_torch(test_graph, 100).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spectral_features = get_spectral_features_torch(test_graph, k=global_k)\nprint(spectral_features.shape)\n\nembedding_model.eval()\nwith torch.no_grad():\n    out = embedding_model(spectral_features, test_graph.edge_index)\n\nprint(out.shape) # for each node outputs some 128 dimensional embedding","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below function is O(n3)! Can be optimized using scipy.sparse.linalg.eigsh.","metadata":{}},{"cell_type":"code","source":"def fill_H_embeddings(G_expanded: Data, G_coarsened: Data, Plp1, embedding_model, rand=False):\n    \"\"\"\n    Basically calculates embedding from Glp1 graph using embedding_model Signnet and algo5 from paper, then replicates these embeddings inside G_expanded\n    all nodes belonging to a coarsened cluster will get that cluster embedding...\n    Plp1 is the cluster/node mapping for transition Gl->Glp1\n\n    G_expanded is the graph that needs to be filled!\n    \"\"\"\n    \n    num_nodes = G_coarsened.num_nodes\n    if rand:\n        #H = torch.randn(num_nodes)\n        x = torch.randn((G_expanded.num_nodes, 128), G_expanded.x.device)\n        G_expanded.x = x\n        return G_expanded\n\n    else:\n        dev = next(embedding_model.parameters()).device\n        \n        spectral_feats = get_spectral_features_torch(G_coarsened, k=global_k).to(dev)\n        edge_idx_dev   = G_coarsened.edge_index.to(dev)\n\n        with torch.no_grad():\n            H_coarsened = embedding_model(spectral_feats, edge_idx_dev)\n\n        # broadcast using P_l that was used to obtain G_coarsened from G\n        cluster_id = Plp1.argmax(dim=0).to(dev)\n        H_expanded = H_coarsened[cluster_id] # Basically all nodes i in some cluster j with embedding Hj, will end up with embedding Hj \n        G_expanded.x = H_expanded.to(G_expanded.x.device) # set node embeddings\n        \n        return G_expanded\n\n# test\ntest_graph = fill_H_embeddings(test_graph, test_coarsened_graph, test_P, embedding_model)\nprint(test_graph)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating the training dataset","metadata":{}},{"cell_type":"markdown","source":"The diffusion model will learn to denoise e_l (refine current graph), and v_l future expansions after refinement to get Gl-1, given the following parameters most contained in refined graph Gl:\n\n* noised el and vl obtained from Gl˜ (noised T times using ddpm scheduler) (from refined graph which edges attr contain -1/1, and .y contains -1/1 for each potential node expansion)\n* T\n* Gl˜ node embeddings containing info obtained using coarsened Glp1 (.x inside refined graph)","metadata":{}},{"cell_type":"code","source":"class CoarsenDataset(IterableDataset): # torch dataset\n    def __init__(self, graphsDataset, embedding_model, min_red=0.1, max_red=0.3, start=0, end=None, debug=False):\n        super(CoarsenDataset).__init__()\n        self.min_red = min_red\n        self.max_red = max_red\n\n        self.graphsDataset = graphsDataset\n        self.embedding_model = embedding_model\n        self.start = start\n        self.end = len(graphsDataset) if end is None else end\n        self.debug = debug\n\n    def __iter__(self):\n        iter_start = self.start\n        iter_end = self.end\n        graphsDataset = self.graphsDataset\n        debug = self.debug\n\n        selected_graphs = graphsDataset[iter_start: iter_end]\n        \n        for G0 in selected_graphs:\n            coarsening_seq, contractions_history = rnd_red_seq(G0, self.min_red, self.max_red)\n\n            if debug: print(\"coarsening_seq, contractions_history\", len(coarsening_seq), len(contractions_history))\n            # coarsening_seq = [G0, G1, ..., G_L]\n            # contractions_history = [P1, P2, ..., P_{L}]  where P_l is used for G_{l-1}->G_l\n            # P_l are the node-cluster mappings...\n\n            num_initial_nodes = G0.num_nodes # target num nodes\n            \n            original_adj = get_graph_adj(G0.edge_index, G0.num_nodes)\n        \n            # to construct each sample we need:\n            # from a sequence G{l-1} ---(Pl)---> Gl ----(Plp1)---> Glp1\n            \n            # first expand Glp1 using Plp1 -> and obtain Gl˜ before refinement\n            # compare Gl˜ and Gl to get refined graph and extract e_l -> add that to graph Gl˜ to obtain refined graph with proper edge attributes which is denoted as Gl\n            # Use Pl to compute next node expansion for Gl (the expansion that will lead to G{l-1}˜)\n\n            # for last node we know v=[2]\n            # then given refined graphs + node embeddings/edgeattributes resp. G1,G2....,G{l-1} we can learn to create resp. G0, ..., Gl-2\n            # we don't need to learn how to create Gl-1 since given any single node graph GL we just duplicate it into two nodes and that gives Gl-1...\n            # therefore we can reconstruct any sequence backwards if model learns to reproduce the refinement+future expansion steps...\n\n            for i in range(1, len(contractions_history)-1): # 1, ..., L-2\n                Glm1 = coarsening_seq[i-1] # G0, G1, ..., Gl-3\n                Gl = coarsening_seq[i] #  G1, G2, ..., Gl-2\n                Glp1 = coarsening_seq[i+1] # G2, G3, ..., Gl-1\n                Pl = contractions_history[i-1] # used to coarsen Glm1 into Gl,    P1...Pl-2\n                Plp1 = contractions_history[i] # used to coarsen Gl into Glp1,   P2, ..., Pl-1\n\n                original_graph_adj_Gl = get_graph_adj(Gl.edge_index, Gl.num_nodes) # original Gl adjency matrix\n                expanded_graph_adj_Gl_tilde = expand_coarsened(Glp1, Plp1) # Gl tilde, using actual cluster values, #G1...Gl-2 tilde\n                # we expand using Plp1 (that is vlp1 expansion counts for all nodes)\n                \n                if debug: print(\"Gl.num_nodes\", Gl.num_nodes)\n                \n                #vlp1 = compute_node_embeddings(Plp1) # extract cluster sizes -1/1 if need to expand, of Glp1\n                v_parent = compute_node_embeddings(Pl) # future node expansions of Gl tilde, used to obtain Glm1\n                if debug: print(\"v_parent.shape: \", v_parent.shape)\n\n                refined_graph = refine_expanded_graph(expanded_graph_adj_Gl_tilde, original_graph_adj_Gl, v_parent) # Gl refined version ,  G1, G2, ..., Gl-2\n                if debug: print(\"refined_graph.num_nodes: \", refined_graph.num_nodes)\n\n                # data object, is refined G_l containing e_l edge attr 1 keep /-1 remove; future vl expansion labels on each node in .y, missing node embeddings obtained from Glp1 and replicated \n                # across all cluster nodes\n                refined_graph = fill_H_embeddings(refined_graph, Glp1, Plp1, embedding_model) # adds all node embeddings inside Gl refined graph according to algo 5, using its later coarsened version Glp1\n                # as well as Plp1 transition and the embedding model\n\n                pho = (Glm1.num_nodes - Gl.num_nodes) / Gl.num_nodes # proportion of nodes that were removed\n                # from Glm1 to obtain Gl\n            \n                yield refined_graph, num_initial_nodes, pho","metadata":{"_uuid":"96d9fedb-fdcf-4c43-b033-119ae466dc0b","_cell_guid":"39565894-d41e-4223-981e-324bb036d0b9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def coarsen_collate(samples):\n    \"\"\"\n    samples: list of tuples (refined_graph, init_num_nodes, level_l)\n    returns batch_graph PyG Batch object with all graphs merged;\n    init_N      : LongTensor [B]  –  original |V(G0)| for all graphs in batch\n    pho_l     : LongTensor [B]  –  coarsening prop. % of nodes removed to obtain current graph\n    \"\"\"\n    graphs, init_nodes, levels = zip(*samples)\n\n    batch_graph = Batch.from_data_list(graphs)  # automatic concat\n    init_N = torch.tensor(init_nodes, dtype=torch.long)\n    pho_l = torch.tensor(levels, dtype=torch.long)\n\n    return batch_graph, init_N, pho_l","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = CoarsenDataset(graphsDataset=erdos_renyi_dataset,embedding_model=embedding_model)\nloader  = torchDataLoader(dataset,batch_size=32,collate_fn=coarsen_collate,num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch_graph, init_N, pho_l in loader:\n    print(\"batch_graph: \", batch_graph)\n    print(\"init_N: \", init_N)\n    print(\"pho_l \", pho_l)\n    break\n\ngraphs = batch_graph.to_data_list()\n\nprint(\"\\nNum nodes in each graph of batch:\")\nfor g in graphs:\n    print(g.num_nodes, g.y.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(batch_graph.x.shape)\nprint(batch_graph.y.shape)\nprint(batch_graph.edge_index.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining denoising model","metadata":{}},{"cell_type":"code","source":"class Denoiser(Module):\n    \"\"\"\n    Simple edge-aware denoiser for (v_l, e_l).\n\n    • Each node input  : [H_node | v_noisy | t_emb | scalar_emb]\n    • Each edge message: concatenates (h_src,h_dst) and feeds e_noisy to\n                         TransformerConv via edge_attr.\n    \"\"\"\n    def __init__(self, in_dim, hid_dim=128,\n                 t_emb_dim=32, n_heads=4):\n        super().__init__()\n\n        # --- tiny embeddings for timestep -------------------------------\n        self.t_embed   = Linear(1, t_emb_dim)   # γ(t)\n        self.scalar_ml = Linear(2, t_emb_dim)   # γ(|V(G₀)|, φ_l)\n\n        # --- 2 × TransformerConv (edge_dim=1 → uses e_noisy) ------------\n        self.conv1 = TransformerConv(in_channels  = in_dim + 1 + 2*t_emb_dim,\n                                     out_channels = hid_dim,\n                                     heads        = n_heads,\n                                     edge_dim     = 1,\n                                     concat=False)\n\n        self.conv2 = TransformerConv(in_channels  = hid_dim,\n                                     out_channels = hid_dim,\n                                     heads        = n_heads,\n                                     edge_dim     = 1,\n                                     concat=False)\n\n        # --- prediction heads -------------------------------------------\n        self.node_head = Sequential(Linear(hid_dim, hid_dim),\n                                     ReLU(),\n                                     Linear(hid_dim, 1))\n\n        self.edge_head = Sequential(Linear(2*hid_dim, hid_dim),\n                                     ReLU(),\n                                     Linear(hid_dim, 1))\n\n    # ------------------------------------------------------------------ #\n    def forward(self, data, t_node, t_edge, init_N, pho_l):\n        \"\"\"\n        data.y         : v_noisy   (ΣN,)\n        data.edge_attr : e_noisy   (ΣE,)\n        \"\"\"\n\n        # -------- build node-feature tensor ------------------------------\n        v_feat = data.y.unsqueeze(1)                             # (ΣN,1)\n\n        t_feat = self.t_embed(t_node.unsqueeze(1).float())       # (ΣN,t)\n        scalars = torch.stack([init_N[data.batch].float(),\n                               pho_l[data.batch].float()], dim=1)\n        s_feat  = self.scalar_ml(scalars)                        # (ΣN,t)\n\n        x0 = torch.cat([data.x, v_feat, t_feat, s_feat], dim=1)  # (ΣN,d₀)\n\n        # -------- message passing (edge_dim = 1 uses e_noisy) ------------\n        h = self.conv1(x0, data.edge_index, data.edge_attr.unsqueeze(-1))\n        h = self.conv2(h,  data.edge_index, data.edge_attr.unsqueeze(-1))\n\n        # -------- node noise prediction ----------------------------------\n        node_eps = self.node_head(h).squeeze(-1)                 # (ΣN,)\n\n        # -------- edge noise prediction ----------------------------------\n        src, dst = data.edge_index\n        edge_h   = torch.cat([h[src], h[dst]], dim=1)            # (ΣE,2H)\n        edge_eps = self.edge_head(edge_h).squeeze(-1)            # (ΣE,)\n\n        return node_eps, edge_eps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model  = Denoiser(in_dim=batch_graph.x.size(1)).to(device)\nopt    = torch.optim.Adam(model.parameters(), lr=10e-4) # same as paper\n\nscheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\nscheduler.set_timesteps(1000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"code","source":"T = scheduler.config.num_train_timesteps   # e.g. 1000\nall_losses = [] \n\nfor batch_graph, init_N, pho_l in loader:\n    batch_graph = batch_graph.to(device, non_blocking=True) \n    B = init_N.size(0) # batch size\n\n    t_graph = torch.randint(0, T, (B,) , device=device) # per batch T sampling\n\n    # batch_graph.x.size(0), contains for each graph in batch its num_nodes\n    # we sample coordinate-wise (for each node/edge) time levels, each node gets noised using a different ti, which we \n    # will of course provide to the model after...\n    \n    #t_node = torch.randint(0, T, (batch_graph.x.size(0),),  device=device)\n    #t_edge = torch.randint(0, T, (batch_graph.edge_attr.size(0),), device=device)\n\n    t_node = t_graph[batch_graph.batch]                         # [ΣN]\n    src    = batch_graph.edge_index[0]                          # src node of each edge\n    t_edge = t_graph[batch_graph.batch[src]]                    # [ΣE]\n\n    # expansion v_l and refinement e_l vectors, will be noised soon\n    clean_v = batch_graph.y.float() # [Σ num_nodes]\n    clean_e = batch_graph.edge_attr.float() # [Σ num_edges]\n\n    # prepare random gaussian noise to add to each node/edge using same sized vectors\n    noise_v = torch.randn_like(clean_v)\n    noise_e = torch.randn_like(clean_e)\n\n    # add noise using hf ddpm scheduler using all different sampled time steps\n    v_noisy = scheduler.add_noise(clean_v.unsqueeze(-1),  # [Σ num_nodes,1]\n                                  noise_v.unsqueeze(-1),\n                                  t_node).squeeze(-1)     # back to [num_nodes]\n\n    # for e_l refinement vectors too\n    e_noisy = scheduler.add_noise(clean_e.unsqueeze(-1),  # [Σ num_edges,1]\n                                  noise_e.unsqueeze(-1),\n                                  t_edge).squeeze(-1)     # back to [Σ num_edges]\n\n    data_t = batch_graph.clone()\n    data_t.y = v_noisy \n    data_t.edge_attr = e_noisy\n\n    # predict noise\n    pred_v_eps, pred_e_eps = model(data_t,\n                                   t_node, t_edge,\n                                   init_N.to(device),\n                                   pho_l.to(device))\n\n    # l2 loss\n    loss  = (pred_v_eps - noise_v).pow(2).mean() + \\\n            (pred_e_eps - noise_e).pow(2).mean()\n    all_losses.append(loss.item()) \n\n    # backprop\n    opt.zero_grad()\n    loss.backward()\n    opt.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plot loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(all_losses[1000:])\nplt.xlabel(\"mini-batch\")\nplt.ylabel(\"MSE loss\")\nplt.title(\"Training loss – per-graph timestep noise\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generating new graphs","metadata":{}},{"cell_type":"markdown","source":"Seed GL graph. Create GLm1 from GL single node graph and initialize its embeddings properly.","metadata":{}},{"cell_type":"code","source":"def seed_graph(feat_dim: int = 128) -> Data:\n    \"\"\"Returns GL (one node self edge? expansion true), GL-1 two nodes one edge\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    edge_index_GL = torch.empty((2, 0), dtype=torch.long)\n    GL_graph = Data(x=torch.randn(1), edge_index=edge_index_GL, num_nodes=1) # single node graph, only created to apply fill_H_embeddings\n    # PL\n    PL = torch.tensor([[1, 1]]) # both nodes of GLm1 belong to same cluster node, the only in GL\n\n    # create GLm1seed_graph\n    edge_index = torch.tensor([[0, 1],\n                               [1, 0]], dtype=torch.long, device=device)\n    x   = torch.zeros((2, feat_dim), device=device)\n    # need to pass through embedding func here instead?\n    \n    y   = torch.ones(2, device=device) # v_l 1s only keep\n    e_a = torch.ones(2, device=device) # e_l one only keep, 2 because undirected edge\n    \n    GLm1 = Data(x=x, edge_index=edge_index,\n                edge_attr=e_a,  y=y)\n\n    GLm1 = fill_H_embeddings(GLm1, GL_graph, PL, embedding_model)\n    return GLm1\n\n# test\ntest_GLm1 = seed_graph()\nprint(test_GLm1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_GLm1.edge_index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Obtain model predictions on GLm1 for a single denoising step","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(device)\nT = scheduler.config.num_train_timesteps \n\ntest_init_N = torch.tensor([20], device=device)\ntest_pho_l = torch.tensor([0.15], device=device) # during generation we will sample pho randomly\n\n# create a batch containing single graph GLm\nG_batched  = Batch.from_data_list([test_GLm1]).to(device) #, device=device)\nt_graph = torch.randint(0, T, (1,), device=device) # noise number of timesteps\n\nt_node  = t_graph.expand(test_GLm1.num_nodes)                    # [N]\nsrc     = test_GLm1.edge_index[0]\nt_edge  = t_graph.expand(src.size(0))                     # [E]\n\n\n# expansion v_l and refinement e_l vectors, will be noised soon\nclean_v = test_GLm1.y.float().to(device) # [num_nodes]\nclean_e = test_GLm1.edge_attr.float().to(device) # [num_edges]\n\n# prepare random gaussian noise to add to each node/edge using same sized vectors\nnoise_v = torch.randn_like(clean_v, device=device)\nnoise_e = torch.randn_like(clean_e, device=device)\n\n# add noise using hf ddpm scheduler using all different sampled time steps\nv_noisy = scheduler.add_noise(clean_v.unsqueeze(-1),  # [num_nodes,1]\n                              noise_v.unsqueeze(-1),\n                              t_node).squeeze(-1)     # back to [num_nodes]\n\n# for e_l refinement vectors too\ne_noisy = scheduler.add_noise(clean_e.unsqueeze(-1),  # [num_edges,1]\n                              noise_e.unsqueeze(-1),\n                              t_edge).squeeze(-1)     # back to [num_edges]\n\n\ntest_pred_v_Lm1, test_pred_e_Lm1 = model(\n        G_batched,\n        t_node, t_edge,\n        test_init_N, test_pho_l)\n\ntest_pred_v_Lm1[1] += 10e-7\nprint(test_pred_v_Lm1)\nprint(test_pred_e_Lm1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, torch_geometric\nprint(\"torch:\", torch.__version__, \"cuda?\", torch.cuda.is_available())\nprint(\"pyg  :\", torch_geometric.__version__)\n\n# where do the model parameters live?\nprint(next(model.parameters()).device)              # should say cuda:0\n\n# but the aggregated messages kernel tells the truth:\nfrom torch_geometric.nn.conv.gat_conv import GATConv\nprint(\"GATConv uses fused CUDA kernels? \",\n      GATConv.__dict__.get('_FUSED_KERNEL', False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Discretize model predictions","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef discretise(v_cont: torch.Tensor, e_cont: torch.Tensor,\n               n_dup: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Turn continuous outputs into {-1,+1}.\n    - v_cont (N,)  higher and closer to 1 means we should expand\n    - e_cont – (E,)  positive means we should keep, negative means we should refine\n    - n_dup number of nodes to expand (duplicate since edge contraction)\n    \"\"\"\n    \n    # discretize v_l\n    thr_v, _ = torch.topk(v_cont, k=n_dup, largest=True) # fetches top k=n_dup values, closest to +1 should be expanded\n    \n    # returns values and indices in two tensors\n    \n    v_bin = torch.full_like(v_cont, -1.) # by default all nodes should not be duplicated\n    v_bin[v_cont >= thr_v[-1]] = 1.0 # all the top-k are set to 1 to be expanded\n    # here if \n\n     # discretize e_l\n    e_bin = torch.where(e_cont > 0, torch.tensor(1.0, device=e_cont.device),\n                                    torch.tensor(-1.0,device=e_cont.device)) # edge refinement classification\n    return v_bin, e_bin\n\n# test\ntest_disc_v_Lm1, test_disc_e_Lm1 = discretise(test_pred_v_Lm1, test_pred_e_Lm1, 1) # duplicate max 1 node for instance...\nprint(test_disc_v_Lm1)\nprint(test_disc_e_Lm1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First apply the refinement step using discretized e_l vector.","metadata":{}},{"cell_type":"code","source":"def drop_edges(G: Data, disc_e: torch.Tensor) -> Data:\n    keep = disc_e > 0 # [E] booleans\n\n    if keep.sum() == 0:  # keep at least one edge\n        keep[disc_e.argmax()] = True\n\n    # G.edge_index [2, E]\n    \n    # slice edge_index & edge_attr\n    edge_index_ref = G.edge_index[:, keep]  # (2 , E_keep)\n    edge_attr_ref  = G.edge_attr[keep]      # (E_keep,)\n\n    G_refined = Data(\n        x          = G.x,\n        edge_index = edge_index_ref,\n        edge_attr  = edge_attr_ref,\n        y          = G.y\n    )\n\n    return G_refined\n\n# test using old graph\nvisualize_clusters(test_refinedGraphData, test_partitioning)\ntest_edges_removed_graph = drop_edges(test_refinedGraphData, test_refinedGraphData.edge_attr) # first refine graph Gltilde\nvisualize_clusters(test_edges_removed_graph, test_partitioning)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Building P matrix from discretized v to expand graph. We have GLm1tilde right now, to expand it we can reuse PLm1 (proj matrix used to create GLm1) but we need to create it using sampled vLm1.","metadata":{}},{"cell_type":"code","source":"def build_Pl_from_vl(v_bin: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    build Pl from vl where Pl is used to obtain Gl from Glm1, and vl is the expansion counts of Gl nodes to obtain Glm1\n\n    v_bin discretized version (N,) (-1 keep, +1 duplicate)\n    returns P [N, Ne] where Ne is the number of nodes after expansion\n    P is the cluster-node mapping, nodes are created in expanded graph and mapped to cluster nodes from current graph\n    cluster nodes are represented by v_bin\n\n    each row sums to 1 or 2 (solo cluster / cluster that needs expansion to two nodes)\n    each column has exactly one 1 in some cluster row, for each new node in expanded graph (Glm1)\n    \"\"\"\n    \n    N = v_bin.numel()\n    copies_per_node = (v_bin > 0).long() + 1 # {-1,+1} → {1,2} (N,)\n    Ne = copies_per_node.sum().item() # total #nodes after expand\n\n    P = torch.zeros((N, Ne), dtype=torch.float32)\n\n    col = 0                                            # current fine-node id\n    for cluster_id, n_copies in enumerate(copies_per_node.tolist()): # for each cluster node i=0...N-1\n        for _ in range(n_copies): # 1× or 2×\n            P[cluster_id, col] = 1.0 # add 1 to represent node in cluster \n            col += 1 # next free column\n\n    return P\n\n# test\ntest_PLm1 = build_Pl_from_vl(test_disc_v_Lm1)\nprint(test_PLm1) # PLm1 is used to coarsen GLm2 into GLm1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Expand graph and fill with new embeddings","metadata":{}},{"cell_type":"code","source":"test_GLm2_adj = expand_coarsened(test_edges_removed_graph, test_PLm1) # then expand graph GLm1 into GLm2 using PLm2\n\n# convert to graph Data object\nedge_index, edge_attr = dense_to_sparse(test_GLm2_adj)\nx = torch.zeros((test_GLm2_adj.size(0), 1), dtype=torch.float)\ntest_GLm2 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr) # edge_attr and .y do not matter since they are predicted\n# however it is missing embeddings for next model prediction...\n\nGLm2 = fill_H_embeddings(test_GLm2, test_GLm1, test_PLm1, embedding_model) # this function is given some Gl, and its vl \n# and it returns embeddings for Glm1 in the paper, here we use Plm1 instead of vl and input GLm1 to calculate \n# GLm2's embeddings...\n\nprint(GLm2.num_nodes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implementing algo 7","metadata":{}},{"cell_type":"code","source":"def denoise_one_step(G_t: Data, t: int, model, scheduler, init_N: int, pho_l: float, device: torch.device):\n    \"\"\"\n    Computes a single reverse DDPM step on v_l and e_l.\n    Returns the denoised G_t graph.\n    \"\"\"\n\n    # use same timestep for all nodes/edges\n    t_node = torch.full((G_t.x.size(0),), t, dtype=torch.long, device=device) # [num_nodes] contains repeated t value\n    t_edge = torch.full((G_t.edge_attr.size(0),), t, dtype=torch.long, device=device) # [num_edges] contains repeated t value\n\n    init_N_tensor = torch.tensor([init_N], dtype=torch.long, device=device) # target size of G0\n    pho_tensor    = torch.tensor([pho_l],  dtype=torch.long, device=device) # sampled reduction frac\n\n    batch = Batch.from_data_list([G_t]).to(device) # wrap graph in batch\n\n    eps_v, eps_e = model(batch, t_node, t_edge, init_N_tensor, pho_tensor) # predicted noise for one step\n\n    v_in   = G_t.y.unsqueeze(-1) # (num_nodes,1) \n    step_v = scheduler.step(eps_v.unsqueeze(-1), t, v_in) # noise v\n    v_tm1  = step_v.prev_sample.squeeze(-1) # (num_nodes,) applies denoising\n\n    e_in   = G_t.edge_attr.unsqueeze(-1) # (num_edges,1)\n    step_e = scheduler.step(eps_e.unsqueeze(-1),\n                            t, e_in)\n    e_tm1  = step_e.prev_sample.squeeze(-1) # (num_edges,) applies denoising\n\n    G_tm1 = Data(x = G_t.x, edge_index = G_t.edge_index, edge_attr  = e_tm1, y = v_tm1)\n\n    return G_tm1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_graph(init_N: int, model, scheduler, min_red: float = 0.10, max_red: float = 0.30, feat_dim: int = 128) -> Data:\n    \"\"\"\n    Generates a graph with ≈ init_N nodes using the trained denoiser.\n    \"\"\"\n    device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model   = model.to(device).eval()\n    T       = scheduler.config.num_train_timesteps\n\n    G_l = seed_graph(feat_dim) #GLm1 with two vertices, one edge\n    \n    # ----------------------------------------------------------------------\n    # loop over levels  l = L-1 , … , 1 (stop when #V >= init_N)\n    # ----------------------------------------------------------------------\n    while G_l.num_nodes < init_N:\n\n        pho_l = float(torch.empty(1).uniform_(min_red, max_red)) # sample between min_red and max_red\n        # represents reduction % of previous coarsening step\n\n        # calculate future duplication count to match pho_l\n        Nc = G_l.num_nodes\n        Ne = Nc / (1 - pho_l)\n\n        n_dup = (Ne-Nc) # since each expansion adds 1 to node count, converts tensor\n        n_dup   = int(max(round(Ne - Nc), 1)) # at least one duplication \n\n        G_t = G_l.clone()\n        G_t.y = torch.randn(G_l.num_nodes, device=device)\n        G_t.edge_attr = torch.randn(G_l.edge_attr.size(0), device=device)\n\n        for t in trange(T-1, -1, -1, leave=False): # reverse diffusion process\n            G_t = denoise_one_step(G_t, t, model, scheduler, init_N, pho_l, device)\n\n        v_bin, e_bin = discretise(G_t.y, G_t.edge_attr, n_dup) # discretize and make sure to only keep n_dup\n\n        G_ref = drop_edges(G_l, e_bin).to(device) # drop selected edges\n\n        # expand graph\n        P_l   = build_Pl_from_vl(v_bin).to(device) # [Nc,Ne] cluster-node mapping\n        adj   = expand_coarsened(G_ref, P_l).to(device) \n        ei, _ = dense_to_sparse(adj)\n\n        G_fine = Data(x=torch.zeros((adj.size(0), feat_dim), device=device), edge_index=ei, \n                      edge_attr=torch.ones(ei.size(1), device=device)*-1, y = torch.ones(adj.size(0), device=device)*-1)\n\n        # G_fine is expanded version of G_ref \n        G_fine = fill_H_embeddings(G_fine, G_ref, P_l, embedding_model) # fill new embeddings from coarsened graph\n\n        G_l = G_fine\n        print(G_l.num_nodes)\n\n    return G_l.to(\"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_G = sample_graph(10, model, scheduler)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nx.draw(to_networkx(sample_G), with_labels=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}