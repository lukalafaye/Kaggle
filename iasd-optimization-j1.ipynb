{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lukalafaye/iasd-optimization-j1?scriptVersionId=201553443\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Optimization for Machine Learning</span>\n\n### <span style=\"color:rgb(199,21,11)\">Master 2 IASD \\& Master 2 MASH 2024-2025</span>\n\n\n# <span style=\"color:rgb(199,21,11)\">Lab 01 - Basics of gradient descent</span>","metadata":{}},{"cell_type":"markdown","source":"This lab session aims at illustrating the behavior of gradient descent on convex and strongly convex problems. We will first look at a toy quadratic problem, then switch to regression problems based on synthetic data.\n\n<!--\nLecture notes for this course will be available [here](https://www.lamsade.dauphine.fr/~croyer/ensdocs/OIM/PolyOIM.pdf).\n-->\n\nThis Jupyter notebook can be obtained [here](https://www.lamsade.dauphine.fr/~croyer/ensdocs/OIM/SourcesLabOIM01.zip).\n\nFor any comment regarding this notebook (including typos), please send an email to: **clement.royer@lamsade.dauphine.fr**.","metadata":{}},{"cell_type":"markdown","source":"#### <span style=\"color:rgb(199,21,11)\">Preliminary remarks</span>\n\n- This notebook and the subsequent ones used in this course mix Python code and text/LaTeX blocks. They can be run offline on any computer where Python and Jupyter are installed, or online using Google Colab (requires a Google account).\n\n- All code blocks from this notebook are meant to be run in the order that they are given. In particular, the first block below must be run first in order to import the necessary toolboxes.\n\n- All the notebooks from this course rely on Python and the NumPy library. A basic yet very useful tutorial on NumPy is freely available\n[here](https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf).","metadata":{}},{"cell_type":"code","source":"# Preamble: useful toolboxes, librairies, functions, etc.\n\n# Display\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfrom math import sqrt # Square root\nfrom math import ceil # Ceil integer operator\nfrom math import log # Logarithm function\n\n# NumPy - Matrix and vector structures\nimport numpy as np # NumPy library\nfrom numpy.random import multivariate_normal, randn, uniform # Probability distributions\n\n# SciPy - Efficient mathematical calculation\nfrom scipy.linalg import toeplitz # Toeplitz matrices\nfrom scipy.linalg import norm # Euclidean norm\nfrom scipy.linalg import svdvals # Singular value decomposition\nfrom scipy.linalg import qr # QR decomposition from linear algebra\nfrom scipy.optimize import check_grad # Numerical check of derivatives\nfrom scipy.optimize import fmin_l_bfgs_b # An efficient minimization routine in moderate dimensions","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:37.560388Z","iopub.execute_input":"2024-09-26T14:31:37.560845Z","iopub.status.idle":"2024-09-26T14:31:37.8457Z","shell.execute_reply.started":"2024-09-26T14:31:37.560804Z","shell.execute_reply":"2024-09-26T14:31:37.844184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Useful NumPy routines (check documentation for more info)**\n\n* *transpose* matrix (i.e. two-dimensional NumPy array) transpose operator.\n* *matmul* matrix-matrix product (the two matrices should have the same number of rows/columns).\n* *dot* matrix-vector product (dimensions permitting), can also be used as inner product operator between two vectors with samle length.\n* *np.ones((m,n))* m-by-n matrix with all components equal to 1. \n* *np.zeros((m,n))* m-by-n matrix with all components equal to 1. \n* *np.identity(n)* square identity matrix of size n.\n* *np.diag(v)* diagonal matrix defined by an array representing its diagonal elements.\n* *np.pi* $\\pi$ constant.\n* *np.inf* represents an infinite number in memory.\n* *np.log* componentwise logarithm for NumPy arrays.\n* *np.exp* componentwise exponential for NumPy arrays.\n* *np.sum* sum of the components of a NumPy array (for matrices, works along one dimension\n* *np.maximum(u,v)* returns a NumPy array as output. The components of this array are $max(u_i,v_i)$, with $u_i$ and $v_i$ being the components of $u$ and $v$.\n* *np.concatenate* concatenates NumPy arrays (vectors, matrices) of appropriate dimensions.\n* If t is a NumPy array, the function *t.shape* returns the dimension(s) of that array (useful when one wants to define an array with the same dimensions.)","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:rgb(199,21,11)\">Part 1 - Gradient descent on convex and strongly convex quadratic functions</span>","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\"> 1.1 - Toy quadratic problem</span>","metadata":{}},{"cell_type":"markdown","source":"To begin this notebook, we will study the following quadratic optimization problem:\n$$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x}):=\\tfrac{1}{2}\\mathbf{x}^T \\mathbf{C} \\mathbf{x},\n$$\nwhere $\\mathbf{C} \\in \\mathbb{R}^d$ is a symmetric, positive semidefinite matrix with eigenvalues\n$$\n    0 \\le \\lambda_1 \\le \\dots \\le \\lambda_d.\n$$\nThis simple setting will allow us to illustrate the performance of gradient-type methods in a convex setting. ","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Observations (cf September 23 lecture)</span> \n\n- The function is polynomial (even quadratic) in every variable, therefore it is (in particular) continuously differentiable.\n- For any $\\mathbf{x} \\in \\mathbb{R}^d$, the gradient of $f$ at $\\mathbf{x}$ is given \nby $\\nabla f(\\mathbf{x})=\\mathbf{C} \\mathbf{x}$.\n- The function $f$ is convex, and $\\lambda_1$-strongly convex when $\\lambda_1 > 0$.\n- The function $f$ is $\\lambda_d$-smooth, i.e. $f$ has an $\\lambda_d$-Lipschitz continuous gradient.\n- $\\min_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x})=0$.\n- $\\mathrm{argmin}_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x})$ always contains the zero vector, and this is the only minimum when $\\lambda_1 >0$.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\"> 1.2 - Two instances of the toy problem</span>","metadata":{}},{"cell_type":"markdown","source":"We will experiment on two different functions (hence two matrices) constructed as follows:\n\n1) We generate a diagonal matrix $\\mathbf{D}$ with eigenvalues randomly distributed in [$\\mu$,$L$], where $L$ and $\\mu$ are fixed, positive parameters, so as to guarantee that $\\mu = \\lambda_1 \\le \\cdots \\le \\lambda_d = L$.\n\n2) We then obtain a diagonal matrix $\\mathbf{D}_0$ with its $d/2$ largest eigenvalues corresponding to that of $\\mathbf{D}$ and the remaining eigenvalues being equal to $0$. \n\n3) We generate a random orthogonal matrix $\\mathbf{Q}$ and define \n$$\n    \\mathbf{C}_{\\mu}=\\mathbf{Q}^T \\mathbf{D} \\mathbf{Q} \n    \\quad \\mathrm{and} \\quad\n    \\mathbf{C}_0=\\mathbf{Q}^T \\mathbf{D}_0 \\mathbf{Q}.\n$$\n\nIt follows that $\\mathbf{C}_{\\mu} \\succeq \\mu \\mathbf{I}$, i.e. this matrix is positive definite, while $\\mathbf{C}_0$ is only positive semidefinite.","metadata":{}},{"cell_type":"code","source":"# Defining two quadratic problems through (random) matrices\n\n# Problem size\nd=100\n\n# Lipschitz constant\nL=10\n\n# Strong convexity constant\nmu=0.01\n\n# Fix random seed for reproducibility\nnp.random.seed(1)\n\n# Generate a random orthogonal matrix Q\nM = np.random.multivariate_normal(np.zeros(d),np.identity(d),size=d)\nQ,R = qr(M) # Q is orthogonal by definition of the QR factorization\n\n# Generate random eigenvalues between mu and L (procedure used for a better spread of the eigenvalues)\nD = np.random.uniform(mu,L,d)\nD = 10.**D\nD = (D-min(D))/(max(D)-min(D))\nD = mu+(L-mu)*D\n\nD0 = D.copy()\nD0[ceil(d/2):] = 0\n\n\n# Produce matrix Cmu\nCmu = Q.T @ np.diag(D) @ Q\n\n# Produce matrix A0 by setting the last eigenvalue to 0\nC0 = Q.T @ np.diag(D0) @ Q","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:37.848854Z","iopub.execute_input":"2024-09-26T14:31:37.849556Z","iopub.status.idle":"2024-09-26T14:31:37.919536Z","shell.execute_reply.started":"2024-09-26T14:31:37.8495Z","shell.execute_reply":"2024-09-26T14:31:37.918201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\">1.2 - Gradient descent</span>","metadata":{}},{"cell_type":"markdown","source":"We now define a gradient descent algorithm dedicated to our problem of interest. For simplicity, the method will use a constant stepsize, to be given as an input. Rather than aiming for a point with a gradient norm smaller than a given tolerance, we run the algorithm for a fixed number of iterations.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Question 1</span> \n\n**Fill out the missing parts in the code below. Make sure the variables you use are consistent with that of the overall code.**","metadata":{}},{"cell_type":"code","source":"def gd_quad(x0,C,stepsize,n_iter=100, verbose=False): \n    \"\"\"\n        A code for gradient descent on quadratic functions.\n        \n        Inputs:\n            x0: Initial vector\n            A: Square matrix of same\n            stepsize: Value of the (constant) stepsize\n            n_iter: Number of iterations\n            verbose: Boolean value indicating whether the outcome of every iteration should be displayed\n      \n        Outputs:\n            w_output: Final iterate of the method\n            objvals: History of function values (output as a Numpy array of length n_iter+1)\n            \n    \"\"\"\n    \n    ############\n    # Initial step: Compute and plot some initial quantities\n\n    # objective history\n    objvals = []\n    \n    # Initial value of current iterate   \n    x = x0.copy()\n\n    # Initialize iteration counter\n    k=0    \n    \n    ##########################################\n    # FILL OUT THE MISSING LINE\n    ##########################################\n    \n    # Current value for the objective function\n    obj = 0.5*x.dot(C.dot(x))\n    \n    objvals.append(obj);\n\n    # Plot the initial values if asked to\n    if verbose:\n        print(\"Gradient Descent:\")\n        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\"]]))\n        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8)]))\n    \n    ####################\n    # Main loop\n    while (k < n_iter):\n        \n        ##################################\n        # FILL OUT THE MISSING PIECES!\n        ##################################\n        \n        # Compute the new iterate through a gradient step\n        x[:] = x - stepsize * C.dot(x)\n        \n        # Compute the new objective value \n        obj = 0.5*x.dot(C.dot(x))\n        \n        ##################################\n        # END FILL-OUT SECTION\n        ##################################\n        objvals.append(obj)\n        \n        # Plot relevant information if asked to\n        if verbose:\n            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8)]))       \n        \n        # Increment the iteration count\n        k += 1\n    \n    # End main loop\n    ######################\n    \n    # Output\n    x_output = x.copy()\n    return x_output, np.array(objvals)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:37.921331Z","iopub.execute_input":"2024-09-26T14:31:37.921881Z","iopub.status.idle":"2024-09-26T14:31:37.946465Z","shell.execute_reply.started":"2024-09-26T14:31:37.921829Z","shell.execute_reply":"2024-09-26T14:31:37.944912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Question 2 - Tests on the strongly convex quadratic problem</span>\n\n**In this question, we consider the matrix $\\mathbf{C}_{\\mu}$ generated above, and the associated quadratic problem.**\n\n**a) Run gradient descent using a fixed initial point (e.g. the vector of all ones) for 100 iterations using 5 different values for the \nstepsize (suggested: a value > 1, 1, 3 values <1 including). Can you find a stepsize greater than $1$ for which the method seems to converge/diverge? Can you find a stepsize smaller than $1$ for which the method does not appear to converge? What about the value $1$?**\n\nFor step size > 2, it will diverge.\nFor step size = 2, it converges.\nFor 0.001, it does not appear to converge but it will after lots of iterations.\nFor 1 it has not converged in 100 iterations but performs very well so far\n\nRanking of variants:\n1, 0.1, 2, 2.01, 0.01, 0.001\n\n**b) Repeat the experiment of question a) using $1000$ iterations instead. Do you observe any change in the ranking of all variants?**\n\nRanking of variants:\n1, 0.1, 0.01, 2, 0.001, 2.01\n\n**c) The choice $\\frac{2}{\\mu+L}$ is known to be optimal stepsize for quadratics that are both $L$-smooth and $\\mu$-strongly convex. Do you observe this in your experiments?**\n\n$\\frac{2}{\\mu+L} = \\frac{2}{0.01 + 1} = 1.9801980198$ approx, yes...","metadata":{}},{"cell_type":"code","source":"# Answer to question 2.a) - Suggested code template\n\n#########################################################\n# Running the variants with same initial point and budget\n#########################################################\n\n######################################\n# PICK AN INITIAL POINT AND AN ITERATION BUDGET\nx_0 = np.ones(d)\nn_iter = 1000\n######################################\n# CHOOSE YOUR PREFERRED VALUES FOR THE STEPSIZE\nvals_stepsize = [2, 1, 0.001, 0.01, 0.1, 2.01, 2, 1.9801980198]\n#\nnvals = len(vals_stepsize)\n\nobjs = np.zeros((n_iter+1,nvals))\n\nfor i_val in range(nvals):\n    _, objs[:,i_val] = gd_quad(x_0,Cmu,vals_stepsize[i_val],n_iter, verbose=False)\n\n\n########################\n# Plotting the comparison\n#########################\n# The x-axis corresponds to the number of iterations\n# The y-axis corresponds to the function value\nplt.figure(figsize=(7,5))\nplt.set_cmap(\"RdPu\")\nfor i_val in range(nvals):\n    plt.semilogy(objs[:,i_val], label=\"GD(\"+str(vals_stepsize[i_val])+\")\", lw=2)\nplt.title(\"Convergence plot (\"+str(n_iter)+\" its)\", fontsize=16)\nplt.xlabel(\"#Iterations\", fontsize=14)\nplt.ylabel(\"Objective function (log)\", fontsize=14)\nplt.legend(loc=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:37.948922Z","iopub.execute_input":"2024-09-26T14:31:37.95067Z","iopub.status.idle":"2024-09-26T14:31:39.139991Z","shell.execute_reply.started":"2024-09-26T14:31:37.950607Z","shell.execute_reply":"2024-09-26T14:31:39.138313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Answer to question 2.b (changing the number of iterations)\n\nYes...\n\nRanking of variants for 100 iter:\n1, 0.1, 2, 2.01, 0.01, 0.001\n\nRanking of variants for 1000 iter:\n1, 0.1, 0.01, 2, 0.001, 2.01","metadata":{"execution":{"iopub.status.busy":"2024-09-26T13:00:21.878998Z","iopub.execute_input":"2024-09-26T13:00:21.879468Z","iopub.status.idle":"2024-09-26T13:00:21.887931Z","shell.execute_reply.started":"2024-09-26T13:00:21.879418Z","shell.execute_reply":"2024-09-26T13:00:21.886062Z"}}},{"cell_type":"markdown","source":"### Answer to question 2.c\n\n$\\frac{2}{\\mu+L} = \\frac{2}{0.01 + 1} = 1.9801980198$ approx, yes it has the best perf...","metadata":{"execution":{"iopub.status.busy":"2024-09-26T12:33:58.658688Z","iopub.status.idle":"2024-09-26T12:33:58.660666Z","shell.execute_reply.started":"2024-09-26T12:33:58.660331Z","shell.execute_reply":"2024-09-26T12:33:58.660369Z"}}},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Question 3 - Tests on the convex quadratic problem</span>\n\n**In this question, we consider the matrix $\\mathbf{C}_0$ generated above, and the associated quadratic problem.**\n\n**a) Run gradient descent using a fixed initial point (e.g. the vector of all ones) for 100 iterations using 5 different multiples of $1/L$, including $1/L$ itself. What value emerges as the best one?**\n\n**b) Compare the rate of convergence on gradient descent with stepsize $1/L$ on this example with that of gradient descent with the same stepsize choice on the strongly convex problem defined by $\\mathbf{C}_{\\mu}$.**","metadata":{}},{"cell_type":"code","source":"# Answer to question 3.a\n\n######################################\n# PICK AN INITIAL POINT AND AN ITERATION BUDGET\nx_0 = np.ones(d)\nn_iter = 100\n######################################\n# CHOOSE YOUR PREFERRED VALUES FOR THE STEPSIZE\nvals_stepsize = [1/L, 0.1/L, 0.5/L, 0.7/L, 0.85/L, 0.99/L]\n\nnvals = len(vals_stepsize)\n\nobjs = np.zeros((n_iter+1,nvals))\n\nfor i_val in range(nvals):\n    _, objs[:,i_val] = gd_quad(x_0,C0,vals_stepsize[i_val],n_iter, verbose=False)\n\n########################\n# Plotting the comparison\n#########################\n# The x-axis corresponds to the number of iterations\n# The y-axis corresponds to the function value\nplt.figure(figsize=(7,5))\nplt.set_cmap(\"RdPu\")\nfor i_val in range(nvals):\n    plt.semilogy(objs[:,i_val], label=\"GD(\"+str(vals_stepsize[i_val])+\")\", lw=2)\nplt.title(\"Convergence plot (\"+str(n_iter)+\" its)\", fontsize=16)\nplt.xlabel(\"#Iterations\", fontsize=14)\nplt.ylabel(\"Objective function (log)\", fontsize=14)\nplt.legend(loc=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:39.14426Z","iopub.execute_input":"2024-09-26T14:31:39.145002Z","iopub.status.idle":"2024-09-26T14:31:39.837629Z","shell.execute_reply.started":"2024-09-26T14:31:39.144932Z","shell.execute_reply":"2024-09-26T14:31:39.835938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Answer to question 3.b\n0.99/L performs best\n\n$\\frac{2}{\\mu+L}$ was best now looks like $\\frac{L-\\mu}{L}$ is.","metadata":{"execution":{"iopub.status.busy":"2024-09-26T12:33:58.669051Z","iopub.status.idle":"2024-09-26T12:33:58.671184Z","shell.execute_reply.started":"2024-09-26T12:33:58.67085Z","shell.execute_reply":"2024-09-26T12:33:58.670887Z"}}},{"cell_type":"markdown","source":"# <span style=\"color:rgb(199,21,11)\">Part 2 - Convex Empirical Risk Minimization</span>","metadata":{}},{"cell_type":"markdown","source":"We now consider empirical risk minimization problems, that are at the center of this course. We will study the performance of gradient descent on two finite-sum problems based on learning a linear model from synthetic data. This will require a more generic implementation of gradient than that from the first part of this notebook.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\">2.1 - Generating the data</span>","metadata":{}},{"cell_type":"markdown","source":"We consider a dataset $\\{(\\mathbf{a}_i,y_i)\\}_{i=1,\\dots,n}$, where $\\mathbf{a}_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$, available in the form of:\n\n- a feature matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times d}$;\n- and a vector of labels $\\mathbf{y} \\in \\mathbb{R}^n$. \n\nGiven this dataset, we will seek a linear model parameterized by a vector $\\mathbf{x}$ that explains the data according to a certain loss function $\\ell$. This results in the following formulation:\n$$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^n f_i(\\mathbf{x}), \\qquad f_i(\\mathbf{x}) = \\ell(\\mathbf{a}_i^T\\mathbf{x},y_i) + \\frac{\\lambda}{2}\\|\\mathbf{x}\\|^2.\n$$\nwhere $\\lambda \\ge 0$ is an optional regularization parameter *(more on this in the lectures on regularization)*.\n\nThe dataset will be produced according to the procedure below.","metadata":{}},{"cell_type":"code","source":"# Data generation.\n# This code is inspired by a generator proposed by A. Gramfort from INRIA.\n\ndef simu_linmodel(x, n, std=1., corr=0.5):\n    \"\"\"\n    Simulation values obtained by a linear model with additive noise\n    \n    Parameters\n    ----------\n    x : np.ndarray, shape=(d,)\n        The coefficients of the model\n    \n    n : int\n        Sample size\n    \n    std : float, default=1.\n        Standard-deviation of the noise\n\n    corr : float, default=0.5\n        Correlation of the feature matrix\n    \"\"\"    \n    d = x.shape[0]\n    cov = toeplitz(corr ** np.arange(0, d))\n    A = multivariate_normal(np.zeros(d), cov, size=n)\n    noise = std * randn(n)\n    y = A.dot(x) + noise\n    return A, y","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:39.840548Z","iopub.execute_input":"2024-09-26T14:31:39.841112Z","iopub.status.idle":"2024-09-26T14:31:39.853305Z","shell.execute_reply.started":"2024-09-26T14:31:39.841056Z","shell.execute_reply":"2024-09-26T14:31:39.851544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is thus produced from a linear model corrupted with noise: $\\mathbf{y} = \\mathbf{A} \\mathbf{x}^* + \\mathbf{\\epsilon}$, where $\\mathbf{\\epsilon}$ follows a Gaussian distribution. Our goal will thus be to learn a linear model from the data.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\">2.2 Linear regression</span>","metadata":{}},{"cell_type":"markdown","source":"In *linear regression*, we seek a linear model that explains the data based on minimizing a least-squares objective:\n$$\n    \\mathrm{minimize}_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x}) \n    := \\frac{1}{2 n} \\|\\mathbf{A} \\mathbf{x} - \\mathbf{y}\\|^2 + \\frac{\\lambda}{2}\\|\\mathbf{x}\\|^2 \n    = \\frac{1}{n} \\left(\\frac{1}{2}\\sum_{i=1}^n (\\mathbf{a}_i^T \\mathbf{x} - y_i)^2 + \\frac{\\lambda}{2}\\|\\mathbf{x}\\|^2 \\right)\n$$ \nfor some $\\lambda \\ge 0$.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Observations</span> \n\n- The function $f$ is quadratic.\n- The function $f$ is $L$-smooth with $L = \\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|}{n}+\\lambda$. \n- The function $f$ is convex, and $(\\sigma_{\\min}(A)+\\lambda)$-strongly convex when $\\sigma_{\\min}(A)+\\lambda>0$, where $\\sigma_{\\min}(A)$ denotes the minimum singular value of $A$.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\">2.3 Logistic regression</span>","metadata":{}},{"cell_type":"markdown","source":"In *logistic regression*, we still consider a linear model, but for classification purposes (here $y_i \\in \\{-1,1\\}$). We use a loss function better suited for classification:\n$$\n    \\mathrm{minimize}_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x}) \n    := \\frac{1}{n} \\sum_{i=1}^n f_i(\\mathbf{x}), \\qquad \n    f_i(\\mathbf{x})=\\log(1+\\exp(-y_i \\mathbf{a}_i^T \\mathbf{x}))+\\frac{\\lambda}{2}\\|\\mathbf{x}\\|^2,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Observations</span> \n\n- For any $\\mathbf{x} \\in \\mathbb{R}^d$, one has\n$$\n\\nabla f(\\mathbf{x}) = \\frac{1}{n}\\sum_{i=1}^n  -\\frac{y_i}{1 + \\exp(y_i \\mathbf{a}_i^T \\mathbf{x})} \\mathbf{a}_i + \\lambda \\mathbf{x}.\n$$\n- The function $f$ is convex and even $\\lambda$-strongly convex when $\\lambda>0$.\n- The function $f$ is $L$-smooth with $L =\\frac{\\|\\mathbf{A}^T \\mathbf{A}\\|}{4n}+\\lambda$.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\">2.4 Python class for regression problems</span>","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Question 4</span> \n\n**Fill out the missing parts in the code below to encode the objective and the gradient of both regression problems, using the formulae from Sections 2.2 and 2.3. Run the next two blocks to check correctness of your implementation.**\n\n*Useful functions:* For any matrix/2-way Numpy array A,\n\n- A.dot(v) is the matrix-vector product $A v$ (if A is a vector, this returns the scalar product);\n- A.T is the transpose matrix $A^T$.\n\n- np.log : logarithm applied componentwise on Numpy arrays.\n- np.mean : average of the components of a NumPy array","metadata":{}},{"cell_type":"code","source":"# Python class for regression problems\nclass RegPb(object):\n    '''\n        A class for regression problems with linear models.\n        \n        Attributes:\n            A: Data matrix (features)\n            y: Data vector (labels)\n            n,d: Dimensions of A\n            loss: Loss function to be considered in the regression\n                'l2': Least-squares loss\n                'logit': Logistic loss\n            lbda: Regularization parameter\n    '''\n   \n    # Instantiate the class\n    def __init__(self, A, y,lbda=0,loss='l2'):\n        self.A = A\n        self.y = y\n        self.n, self.d = A.shape\n        self.loss = loss\n        self.lbda = lbda\n        \n    \n    ###############################################################\n    # PARTS TO BE FILLED OUT BELOW\n    ###############################################################\n    \n    # Objective value\n    def fun(self, x):\n        loss = self.loss\n        n = self.n\n        A = self.A\n        y = self.y \n        lbda = self.lbda\n        \n        if loss == \"l2\":\n            residual = A @ x - y\n            obj = 1/(2*n) * np.sum(residual**2) + (lbda / 2) * np.sum(x**2)\n        elif loss == \"logit\":\n            logits = A @ x  # Compute A*x\n            log_loss = np.log(1 + np.exp(-y * logits))  # Compute log-loss for each sample\n            obj = (1/n) * np.sum(log_loss) + (lbda / (2*n)) * norm(x)**2  # Add regularization term\n        return obj\n\n\n    # Full gradient computation\n    def grad(self, x):\n        loss = self.loss\n        A = self.A\n        y = self.y\n        self.lbda = lbda\n        \n        if loss == \"l2\":\n            C = A.T.dot(A)\n            b = A.T.dot(y)\n            g = (1/n) * (C.dot(x)-b) + lbda*x\n        elif loss == \"logit\":\n            logits = A @ x\n            num = A * y[:, np.newaxis]\n            den = 1 + np.exp(y * logits)\n\n            g = (-1/n) * np.sum(num.T @ (1 / den)) + lbda * x\n\n        return g\n    \n    ###############################################################\n    # NOTHING MORE TO FILL OUT PAST THIS POINT\n    ###############################################################\n\n    # Lipschitz constant for the gradient\n    def lipgrad(self):\n        if self.loss=='l2':\n            L = norm(self.A, ord=2) ** 2 / self.n + self.lbda\n        elif self.loss=='logit':\n            L = norm(self.A, ord=2) ** 2 / (4. * self.n) + self.lbda\n        return L\n    \n    # ''Strong'' convexity constant (could be zero if self.lbda=0)\n    def cvxval(self):\n        if self.loss=='l2':\n            s = svdvals(self.A)\n            mu = min(s)**2 / self.n # More efficient than computing ||A^T A||\n            return mu + self.lbda\n        elif self.loss=='logit':\n            return self.lbda","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:00:24.791342Z","iopub.execute_input":"2024-09-26T15:00:24.791767Z","iopub.status.idle":"2024-09-26T15:00:24.813006Z","shell.execute_reply.started":"2024-09-26T15:00:24.791729Z","shell.execute_reply":"2024-09-26T15:00:24.810373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first script below generates two problem instances (one for each class), the second checks the implementation of the derivatives, and the third one computes an approximate solution for each problem.","metadata":{}},{"cell_type":"code","source":"# Generate the problem instances - we use moderate sizes but those will serve our purpose\n\nd = 50\nn = 1000\nidx = np.arange(d)\nlbda = 1. / n ** (0.5)\n\n# Fix random seed for reproducibility\nnp.random.seed(1)\n\n# Ground truth coefficients of the model\nx_model_truth = (-1)**idx * np.exp(-idx / 10.)\n\nAlin, ylin = simu_linmodel(x_model_truth, n, std=1., corr=0.1)\nAlog, ylog = simu_linmodel(x_model_truth, n, std=1., corr=0.7)\nylog = np.sign(ylog) # Taking the logarithm for binary classification\n\npblinreg = RegPb(Alin, ylin,lbda,loss='l2')\npblogreg = RegPb(Alog, ylog,lbda,loss='logit')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:00:25.785493Z","iopub.execute_input":"2024-09-26T15:00:25.78605Z","iopub.status.idle":"2024-09-26T15:00:25.806642Z","shell.execute_reply.started":"2024-09-26T15:00:25.785995Z","shell.execute_reply":"2024-09-26T15:00:25.80507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next code block aims at validating the implementation of the gradient.","metadata":{}},{"cell_type":"code","source":"# Check correctness of the gradient functions compared to a finite-difference estimate\n# Note: this is a check of the implementation of the gradient and the function value. If correct, the \n# output value, which represents the difference between a gradient and its estimation from function values, \n# should be of order 1e-06 at most.\n# (See the lecture by Gabriel Peyré on automatic differentiation)\n\n# Check for the linear regression problem\nprint(\"Linear regression gradient error:\",check_grad(pblinreg.fun, pblinreg.grad, np.random.randn(d)))\n\n# Check for the logistic regression problem\nprint(\"Logistic regression gradient error:\",check_grad(pblogreg.fun, pblogreg.grad, np.random.randn(d)))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:00:27.045218Z","iopub.execute_input":"2024-09-26T15:00:27.045763Z","iopub.status.idle":"2024-09-26T15:00:27.074901Z","shell.execute_reply.started":"2024-09-26T15:00:27.045726Z","shell.execute_reply":"2024-09-26T15:00:27.073538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\">2.5 Numerical estimates of $\\min$ and $\\mathrm{argmin}$</span>","metadata":{}},{"cell_type":"markdown","source":"In this lab, we work with relatively simple loss functions (and a moderate number of data points): we can thus efficiently compute a solution using a second-order method. This provides us with a target objective value as well as a target vector of weights.","metadata":{}},{"cell_type":"code","source":"# Use L-BFGS-B to determine a solution for both problems\n\nx_init = np.zeros(d)\n# Compute the optimal solution for linear regression\nx_min_lin, f_min_lin, _ = fmin_l_bfgs_b(pblinreg.fun, x_init, pblinreg.grad, args=(), pgtol=1e-30, factr =1e-30)\nprint(\"Linear regression:\")\nprint(\"\\t Numerical minimal value:\",f_min_lin)\nprint(\"\\t Numerical minimum gradient norm:\",norm(pblinreg.grad(x_min_lin)))\n\n# Compute the optimal solution for logistic regression\nx_min_log, f_min_log, _ = fmin_l_bfgs_b(pblogreg.fun, x_init, pblogreg.grad, args=(), pgtol=1e-30, factr =1e-30)\nprint(\"Logistic regression:\")\nprint(\"\\t Numerical minimal value:\",f_min_log)\nprint(\"\\t Numerical minimum gradient norm:\",norm(pblogreg.grad(x_min_log)))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:40.29678Z","iopub.status.idle":"2024-09-26T14:31:40.297291Z","shell.execute_reply.started":"2024-09-26T14:31:40.297036Z","shell.execute_reply":"2024-09-26T14:31:40.297059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These solutions will enable us to study the behavior of the distance to optimality in terms of function values \n$f(\\mathbf{x}_k)-f^*$ and iterates $\\|\\mathbf{x}_k -\\mathbf{x}^*\\|$. \n\n*Note: Recall that $\\|\\nabla f(\\mathbf{x}_k)\\| \\ge 2 \\mu (f(\\mathbf{x}_k)-f^*)$ for a $\\mu$-strongly convex, continuously differentiable function with optimal value $f^*$, thus the gradient could also be used as an upper estimate of the distance to optimality in terms of function values.*","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:rgb(199,21,11)\">2.6 Gradient descent implementation</span>","metadata":{}},{"cell_type":"markdown","source":"We will investigate several techniques for selecting the stepsize in a predetermined or adaptive fashion in gradient descent. Those are by no means exhaustive, but give a nice overview of existing options (see upcoming lectures for more details about the theory behind those methods). We will consider the three following options:\n\n- *Constant stepsize:* $\\alpha_k = \\frac{\\bar{\\alpha}}{L}$, where $L$ is the Lipschitz constant for $\\nabla f$ (assumed to be known in that case).\n\n- *Decreasing stepsize:* $\\alpha_k = \\frac{\\bar{\\alpha}}{(k+1)^a}$, where $a>0$ is given as input\n\n- *Armijo-type line search:* $\\alpha_k=\\frac{\\bar{\\alpha}}{2^{j_k}}$, where $j_k$ is the smallest nonnegative integer such that\n$$\nf\\left(\\mathbf{x}_k-\\frac{\\bar{\\alpha}}{2^{j_k}}\\nabla f(\\mathbf{x}_k)\\right) < f(\\mathbf{x}_k) - 0.0001 \\frac{\\alpha}{2^{j_k}}\\|\\nabla f(\\mathbf{x}_k)\\|^2.\n$$\nIn practice, the line search should stop if the decrease condition is satisfied or $\\frac{\\bar{\\alpha}}{2^{j_k}} < 10^{-10}$. Note that the parameters $2$,$0.0001$ and $10^{-10}$ are set for simplicity.\n","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Question 5</span>\n\n**Fill out the function template below with the gradient descent iteration and the three rules for the stepsize above (that are encoded using input parameters).**","metadata":{}},{"cell_type":"code","source":"# Gradient descent\ndef gd_reg(x0,problem,xopt,stepchoice=0,stepbar=1, n_iter=1000, verbose=False): \n    \"\"\"\n        An implementation of gradient descent with several stepsize rules.\n        \n        Inputs:\n            x0: Initial point\n            problem: Problem structure\n                problem.fun(x) Objective value\n                problem.grad(x) Gradient\n                problem.lipgrad() Lipschitz constant for the gradient\n            xopt: Target point for the optimization (approximate optimum computed beforehand)\n            stepchoice: Rule for choosing the stepsize (see above)\n                0: Constant equal to 1/L where L is a Lipschitz constant for the gradient\n                a>0: Decreasing, set to 1/((k+1)**a)\n                -1: Armijo line search\n            stepbar: Initial step size (used when stepchoice = 1)\n            n_iter: Maximum iteration number\n            verbose: Boolean value indicating whether iteration-level information should be displayed.\n      \n        Outputs:\n            w_output: Last iterate of the method\n            objvals: History of function values (Numpy array of length n_iter)\n            distits: History of distances to the target point (Numpy array of length n_iter)\n            ngvals: History of gradient norms (Numpy array of length n_iter)\n            \n    \"\"\"\n    \n    ############\n    # Initialization\n\n    # History of function values\n    objvals = []\n    \n    # History of gradient norms\n    ngvals = []\n    \n    # History of distances to the target\n    distits = []\n    \n    # Lipschitz constant\n    L = problem.lipgrad()\n    \n    # Initial value of the incumbent, a.k.a. current iterate\n    x = x0.copy()\n\n    # Initialize the iteration count\n    k=0    \n    \n    # Initial function value\n    obj = problem.fun(x) \n    objvals.append(obj);\n    \n    # Initial gradient\n    g = problem.grad(x)\n    ng = norm(g)\n    ngvals.append(ng)\n    \n    # Distance between the current point and the optimal point\n    dist = norm(x-xopt)\n    distits.append(dist)\n\n    # Plot initial values \n    if verbose:\n        print(\"Gradient descent:\")\n        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"dist\",\"stepsize\"]]))\n    \n    ####################\n    # Main loop\n    while (k < n_iter):\n        \n        #####################################################################################\n        ###### THIS PART SHOULD BE FILLED WITH THE ACTUAL STEPSIZE CALCULATION AND ITERATION\n        \n        # 1 - Define the stepsize s based on k (iteration index), L (Lipschitz constant), step0 (initial value)\n        # and g (the function)\n        \n        \n        # 2 - Perform the gradient descent iteration using the stepsize s and the gradient g\n        x[:] = \n      \n            \n        ###### NOTHING MORE TO FILL PAST THIS POINT. \n        ###### MAKE SURE THAT x NOW CONTAINS THE NEXT ITERATE AND s THE ASSOCIATED STEPSIZE\n        ####################################################################################\n        \n        \n        # Plot relevant information\n        if verbose:        \n            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % dist).rjust(8),(\"%.2e\" % s).rjust(8)]))\n        \n        # Compute values associated with the next iterate\n        obj = problem.fun(x)\n        objvals.append(obj)\n        g = problem.grad(x)\n        ng = norm(g)\n        ngvals.append(ng)\n        dist = norm(x-xopt)\n        distits.append(dist)\n        \n        # Increase iteration counter\n        k += 1\n    \n    # End main loop\n    ######################\n    \n    # Outputs\n    x_output = x.copy()\n    return x_output, np.array(objvals), np.array(distits), np.array(ngvals)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:40.299854Z","iopub.status.idle":"2024-09-26T14:31:40.300541Z","shell.execute_reply.started":"2024-09-26T14:31:40.300233Z","shell.execute_reply":"2024-09-26T14:31:40.300262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Question 6</span>\n\n**a) Run gradient descent when using the four following rules on linear regression:**\n\n- $\\alpha_k = \\frac{1}{L}$;\n\n- $\\alpha_k = \\frac{1}{k+1}$;\n\n- $\\alpha_k = \\frac{1}{\\sqrt{k+1}}$;\n\n- $\\alpha_k$ **chosen through Armijo line search with $\\bar{\\alpha}=1$.**\n\n**Compare the convergence curves in terms of (log of) function values $\\{f(x_k)\\}$ and $\\{f(x_k)-f^*_{lin}\\}$, where \n$f^*_{lin}$ is the value computed in Section 2.5.**\n\n**b) Plot the gradient norms and the distances to optimum as a function of the number of iterations. Are those plots consistent with the others? Do they bring additional insights about the methods?**\n\n**c) What other curves and/or quantities could you plot for comparing those four variants, so that the comparison is the fairest possible?** ","metadata":{}},{"cell_type":"code","source":"# Answer to Question 6a)b)c)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:40.30318Z","iopub.status.idle":"2024-09-26T14:31:40.304056Z","shell.execute_reply.started":"2024-09-26T14:31:40.303641Z","shell.execute_reply":"2024-09-26T14:31:40.303674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"color:rgb(199,21,11)\">Question 7</span>\n\n**Repeat the experiments of Question 6 using logistic regression. Try to find the best possible rule for the stepsize.**","metadata":{}},{"cell_type":"code","source":"# Answer to question 6)d)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:40.306537Z","iopub.status.idle":"2024-09-26T14:31:40.306986Z","shell.execute_reply.started":"2024-09-26T14:31:40.306777Z","shell.execute_reply":"2024-09-26T14:31:40.306798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Version 1.0 - C. W. Royer, September 2024.","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:31:40.308782Z","iopub.status.idle":"2024-09-26T14:31:40.309299Z","shell.execute_reply.started":"2024-09-26T14:31:40.309033Z","shell.execute_reply":"2024-09-26T14:31:40.309054Z"},"trusted":true},"execution_count":null,"outputs":[]}]}