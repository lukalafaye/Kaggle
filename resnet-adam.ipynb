{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6850838,"sourceType":"datasetVersion","datasetId":3937926}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport timm\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming 'tf_efficientnet_b0' is used\nmodel = timm.create_model('tf_efficientnet_b0', pretrained=False)\nNUM_FINETUNE_CLASSES = 100  # Adjust based on your specific number of classes\n\n# Modify the classifier for fine-tuning with your specific number of classes\nmodel.classifier = nn.Linear(model.num_features, NUM_FINETUNE_CLASSES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/datasets/ericrohmer/compet/\n\ntrain_features = pd.read_csv('/kaggle/input/compet/train_features.csv', header=None)\ntrain_labels = pd.read_csv('/kaggle/input/compet/train_labels.csv', header=None)\nencoder = OneHotEncoder()\ntrain_labels_encoded = encoder.fit_transform(train_labels.values.reshape(-1, 1)).toarray()\n\n# Standardize the features\ncolumn_names = train_features.columns.tolist()\nscaler = StandardScaler()\ntrain_features_scaled = scaler.fit_transform(train_features)\ntrain_features = pd.DataFrame(data=train_features_scaled, columns=column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming train_features and train_labels_encoded are your features and labels\ntrain_data, val_data, train_labels, val_labels = train_test_split(\n    train_features, \n    train_labels_encoded, \n    test_size=0.2, \n    random_state=42,\n    stratify=train_labels_encoded  # Ensure stratified splitting\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, features, labels, transform):\n        self.features = features\n        self.labels = labels\n        self.transform = transform\n        self.height = 224\n        self.width = 224\n        self.channels = 3\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        img_row = self.features.iloc[idx]\n        image_array = np.array(img_row).reshape((32, 32))\n        image = Image.fromarray(image_array.astype(np.uint8))\n        img = image.convert('RGB')\n        \n        img_np = np.array(img)\n        resized_img = cv2.resize(img_np, (self.width, self.height), interpolation=cv2.INTER_CUBIC)\n\n        # Convert NumPy array back to PIL image\n        resized_img_pil = Image.fromarray(resized_img)\n        \n        tensor = self.transform(resized_img_pil)\n        label = self.labels[idx]\n        return tensor, label\n\n# Add normalization to your transformation\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CustomDataset instances\ntrain_dataset = CustomDataset(features=train_data, labels=train_labels, transform=transform)\nval_dataset = CustomDataset(features=val_data, labels=val_labels, transform=transform)\n\n# Create DataLoader for training and validation sets\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.argmax(train_labels, axis=1)\n\nclass_counts = np.bincount(y)\nn_samples = len(y)\nn_classes = len(class_counts)\nclass_weights = n_samples / (n_classes * class_counts)\n\n# Convert class weights to a tensor\nweights_tensor = torch.tensor(class_weights, dtype=torch.float)\nweights_tensor = weights_tensor.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\ncriterion = nn.CrossEntropyLoss(weight=weights_tensor)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 100\npatience = 10  # Set the patience for early stopping\n\nbest_val_loss = float('inf')\ncurrent_patience = 0\n\n# Move the model to GPU if available\nmodel.to(device)\n\ntotal_train = 0\ncorrect_train = 0\ntotal_val = 0\ncorrect_val = 0\n\ntrain_losses = []\ntrain_accuracies = []\ntrain_f1_scores = []\nval_losses = []\nval_accuracies = []\nval_f1_scores = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    all_labels_train = []\n    all_predictions_train = []\n    \n\n    for inputs, one_hot_labels in train_loader:\n        inputs, one_hot_labels = inputs.to(device), one_hot_labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        # Convert one-hot encoded labels to class indices\n        labels = torch.argmax(one_hot_labels, dim=1).to(device)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n        # Calculate training accuracy\n        _, predicted_train = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted_train == labels).sum().item()\n\n        # Collect true labels and predicted labels for F1 score\n        all_labels_train.extend(labels.cpu().numpy())\n        all_predictions_train.extend(predicted_train.cpu().numpy())\n        \n        _, predicted_train = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted_train == labels).sum().item()\n\n    f1_train = f1_score(all_labels_train, all_predictions_train, average='weighted')\n    train_accuracy = correct_train / total_train\n        \n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    all_labels_val = []\n    all_predictions_val = []\n\n    with torch.no_grad():\n        for inputs, one_hot_labels in val_loader:\n            inputs, one_hot_labels = inputs.to(device), one_hot_labels.to(device)\n            \n            outputs = model(inputs)\n            \n            # Convert one-hot encoded labels to class indices\n            labels = torch.argmax(one_hot_labels, dim=1).to(device)\n            \n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            # Calculate validation accuracy\n            _, predicted_val = torch.max(outputs, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted_val == labels).sum().item()\n\n            # Collect true labels and predicted labels for F1 score\n            all_labels_val.extend(labels.cpu().numpy())\n            all_predictions_val.extend(predicted_val.cpu().numpy())\n\n            _, predicted_val = torch.max(outputs, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted_val == labels).sum().item()\n\n\n    # Calculate average losses\n    avg_train_loss = train_loss / len(train_loader.dataset)\n    avg_val_loss = val_loss / len(val_loader.dataset)\n    \n    val_accuracy = correct_val / total_val\n    f1_val = f1_score(all_labels_val, all_predictions_val, average='weighted')\n\n    # Access the learning rate from the optimizer\n    current_lr = optimizer.param_groups[0]['lr']\n\n    # Print epoch statistics\n    print(f'Epoch {epoch + 1}/{num_epochs}, '\n          f'Train Loss: {avg_train_loss:.4f}, '\n          f'Train Accuracy: {train_accuracy:.4f}, '\n          f'Train F1 Score: {f1_train:.4f}, '\n          f'Validation Loss: {avg_val_loss:.4f}, '\n          f'Validation Accuracy: {val_accuracy:.4f}, '\n          f'Validation F1 Score: {f1_val:.4f}, '\n          f'Learning Rate: {current_lr:.6f}')\n    \n    train_losses.append(avg_train_loss)\n    train_accuracies.append(train_accuracy)\n    train_f1_scores.append(f1_train)\n    val_losses.append(avg_val_loss)\n    val_accuracies.append(val_accuracy)\n    val_f1_scores.append(f1_val)\n    \n  # Early stopping check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        current_patience = 0\n    else:\n        current_patience += 1\n\n    if current_patience >= patience:\n        print(f'Early stopping after {epoch + 1} epochs without improvement.')\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting\nepochs_range = range(1, num_epochs + 1)\n\nplt.figure(figsize=(15, 5))\n\n# Plot Training and Validation Loss\nplt.subplot(1, 3, 1)\nplt.plot(epochs_range, train_losses, label='Train Loss')\nplt.plot(epochs_range, val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Plot Training and Validation Accuracy\nplt.subplot(1, 3, 2)\nplt.plot(epochs_range, train_accuracies, label='Train Accuracy')\nplt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\n# Plot Training and Validation F1 Score\nplt.subplot(1, 3, 3)\nplt.plot(epochs_range, train_f1_scores, label='Train F1 Score')\nplt.plot(epochs_range, val_f1_scores, label='Validation F1 Score')\nplt.xlabel('Epochs')\nplt.ylabel('F1 Score')\nplt.title('Training and Validation F1 Score')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features = pd.read_csv('/kaggle/input/compet/test_features.csv', header=None)\ntest_features = scaler.transform(test_features)\ntest_features = pd.DataFrame(test_features, columns=train_features.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, features, transform):\n        self.features = features\n        self.transform = transform\n        self.height = 224\n        self.width = 224\n        self.channels = 3\n        \n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        img_row = self.features.iloc[idx]\n        image_array = np.array(img_row).reshape((32, 32))\n        image = Image.fromarray(image_array.astype(np.uint8))\n        img = image.convert('RGB')\n        img_np = np.array(img)\n        resized_img = cv2.resize(img_np, (self.width, self.height), interpolation=cv2.INTER_CUBIC)\n\n        # Convert NumPy array back to PIL image\n        resized_img_pil = Image.fromarray(resized_img)\n        \n        tensor = self.transform(resized_img_pil)\n        return tensor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(features=test_features, transform=transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size_test = 1\ntest_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the appropriate device\nmodel.to(device)\n\n# Iterate through the test_loader and move input data to the device\nwith torch.no_grad():\n    test_predictions_list = []\n    for inputs in test_loader:\n        inputs = inputs.to(device)  # Move input data to the same device as the model\n        outputs = model(inputs)\n        \n        # Convert logits to predicted class indices\n        _, predicted_test = torch.max(outputs, 1)\n\n        # Move predicted_test to CPU before converting to NumPy\n        predicted_test_cpu = predicted_test.cpu().numpy()\n\n        # Append predictions to the list\n        test_predictions_list.append(predicted_test_cpu)\n\n# Convert the list of predictions to a numpy array\nargmax_predictions = np.concatenate(test_predictions_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(data={\n    'ID': range(0, len(argmax_predictions)),\n    'Prediction': argmax_predictions\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('onestdanslasauce.csv', index=False, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}