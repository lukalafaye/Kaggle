{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAIDIUM MEDICAL ORGANS CLUSTERING PROJECT","metadata":{"_uuid":"774fef7b-ed4e-4010-b86e-e9382a19cce8","_cell_guid":"cab21dea-6f00-4ef9-a73f-84645fd3569b","trusted":true,"collapsed":false,"id":"_OHVPnh9VVMw","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Imports","metadata":{"_uuid":"0c47f928-3438-4686-b586-15646c43a442","_cell_guid":"1c55bae5-02ca-4760-b58c-a642ba0b9841","trusted":true,"collapsed":false,"id":"5fp8WzwSVbhL","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!lscpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip3 install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n!pip -q install iterative-stratification","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf  \n# Check if GPU is available\nprint(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n\n# Ensure TensorFlow uses GPU\ntf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)","metadata":{"_uuid":"5446ef61-f119-497a-9577-594468356ea2","_cell_guid":"381e249f-c4dc-4479-a03c-ece2f17f407f","trusted":true,"collapsed":false,"id":"viH5dVM8d62k","outputId":"efcd13d7-34d7-459b-eb6b-0b1dda9adfbd","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fetching dataset","metadata":{"_uuid":"54ee9cef-662f-415a-be3b-223a3193cefe","_cell_guid":"7bfd5771-9e42-42c6-a5ab-e09f3ed18f66","trusted":true,"collapsed":false,"id":"R6WPNKUoVdGd","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!wget https://challengedata.ens.fr/media/public/label_Hnl61pT.csv -O y_train.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# downloading the images\n\n!wget https://challengedata.ens.fr/media/public/train-images.zip\n!wget  https://challengedata.ens.fr/media/public/test-images.zip\n!wget https://challengedata.ens.fr/media/public/annotated_labels.json","metadata":{"_uuid":"2016b895-7573-4bff-b139-e3a8eb416c88","_cell_guid":"cca0109b-d319-45ac-a1a6-704429fb5ca5","trusted":true,"collapsed":false,"id":"EZSMQF6uSJ4t","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unzip images\n\n!unzip -q -n train-images.zip\n!unzip -q -n test-images.zip","metadata":{"_uuid":"fb04c950-6636-42a1-9673-bdd789baa631","_cell_guid":"f6363096-6781-4186-8617-1d4eabb92f66","trusted":true,"collapsed":false,"id":"8azqvCN8G9HT","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd \n\n# Load the train labels\n# Note the transpose!\nlabels_train = pd.read_csv(\"y_train.csv\", index_col=0).T","metadata":{"_uuid":"9efcf075-f159-41aa-9ec3-2d6c40c4053d","_cell_guid":"fc49d914-33f1-44d7-a2a6-ee12c780524a","trusted":true,"collapsed":false,"id":"_iWbLcO5a_vb","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load annotated_labels.json\nimport json \n\nwith open(\"annotated_labels.json\", \"r\") as f:\n    annotated_labels = json.load(f)","metadata":{"_uuid":"af3dc295-b879-4ccf-97b5-9f2ffb618650","_cell_guid":"a9e5b5ab-4c4b-46c3-bfca-5d4b7d5122a1","trusted":true,"collapsed":false,"id":"T4T8Dn3XWtwo","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Here is a function to load the data\ndef load_dataset(dataset_dir):\n    dataset_list = []\n    # Note: It's very important to load the images in the correct numerical order!\n    for image_file in list(sorted(Path(dataset_dir).glob(\"*.png\"), key=lambda filename: int(filename.name.rstrip(\".png\")))):\n        dataset_list.append(cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE))\n    return np.stack(dataset_list, axis=0)","metadata":{"_uuid":"c579b3ca-164e-4546-bdf2-217f647a15e6","_cell_guid":"f58b5d97-6568-4180-959e-5a30eb276d37","trusted":true,"collapsed":false,"id":"TdFEfI0YZVGM","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install cv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport cv2\nimport numpy as np\n\n# Load the train and test sets\n# If you've put the shortcut directly in your drive, this should work out of the box\n# Else, edit the path\ndata_dir = Path(\"./\")\ndata_train = load_dataset(data_dir / \"train-images\")\ndata_test = load_dataset(data_dir / \"test-images\")","metadata":{"_uuid":"3474e074-f8f4-4bbb-a367-28640d2090a2","_cell_guid":"046750bf-ec0c-4951-9d4a-8497c9757c78","trusted":true,"collapsed":false,"id":"pUQsU4NhZVID","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize dataset","metadata":{"_uuid":"567d8efd-8019-467f-8e47-3e86624820eb","_cell_guid":"1044cc00-9fd6-4332-8c68-5d0af3e05eba","trusted":true,"collapsed":false,"id":"mmkIfgXmV4TH","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"labels_train","metadata":{"_uuid":"e3f77120-58a2-4e2d-afb0-f3d28d054a91","_cell_guid":"fc8c7589-7644-4c83-a86d-33b2057030fd","trusted":true,"collapsed":false,"id":"R6BfHhWJXa8I","outputId":"ded15f38-252f-4764-9caf-9206cb9bfdf4","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Each image in annotated_labels contains 27 clusters,\n# only some of these were actually annotated pixel by pixel in labels_train\n# hence most cluster ids are not represented on most images and replaced by 0s..\n\ndf_annotated_labels = pd.DataFrame(annotated_labels).fillna(-1)\ndf_annotated_labels.index = [f\"{i}.png\" for i in range(len(annotated_labels))]  # Add .png extension\n\ndf_annotated_labels\n\n# -1 means empty value... basically 1200 images do not have clusters?","metadata":{"_uuid":"2815bc46-40fc-4960-a4da-69e810e4958c","_cell_guid":"28297437-da9f-4611-800d-af74d10d8eb7","trusted":true,"collapsed":false,"id":"3h96bouCWxA8","outputId":"1a044edb-a330-417b-bc19-e140b5a291d0","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The train data is a numpy array of 1000 images of 512*512\nprint(f\"X_train shape: {data_train.shape}\")\n# The train label is a dataframe of 1000 rows with 262144 (=512x512) columns\nprint(f\"Y_train shape: {labels_train.shape}\")","metadata":{"_uuid":"18c9c61f-a9ee-4613-b798-aaf87d373df5","_cell_guid":"9d61b125-a138-4c8a-9042-1985816ce5bf","trusted":true,"collapsed":false,"id":"7M5KbeQlZeGA","outputId":"63e314eb-1399-49bc-bb8b-96026de40127","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's display some data\n# Here is a function to display side by side a slide and a segmented mask\ndef plot_slice_seg(slice_image, seg):\n    fig, axes = plt.subplots(1, 2)\n    axes[0].imshow(slice_image, cmap=\"gray\")\n    axes[1].imshow(slice_image, cmap=\"gray\")\n    seg_masked = np.ma.masked_where(seg.reshape((256,256)) == 0, (seg.reshape((256,256))))\n    axes[1].imshow(seg_masked, cmap=\"tab20\")\n    plt.axis(\"off\")","metadata":{"_uuid":"1249f434-f682-46c3-91bc-dc339c6d707e","_cell_guid":"2a9f0ca4-288e-4950-bd34-46fa5adab5fe","trusted":true,"collapsed":false,"id":"kavWfyrvZgUm","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's try it on some slides and their segmentation masks\n# Note: we have to reshape the rows of the dataframe into a 256*256 matrix\nimport matplotlib.pyplot as plt\n\nfor i in range(10, 15):\n  plot_slice_seg(data_train[i], labels_train.iloc[i].values.reshape((256,256)))","metadata":{"_uuid":"e811f264-37aa-44df-85f8-50c3bcc48e71","_cell_guid":"80225f1d-2449-46c2-851f-3c5de4c7a6ea","trusted":true,"collapsed":false,"id":"aPNpL3TbZspT","outputId":"119fa436-4126-4dd9-efc5-b21dc4f6f555","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_slice_class(slice_image, seg, class_i):\n    # Reshape the segmentation into a 256x256 matrix\n    seg_reshaped = seg.reshape((256, 256))\n    \n    # Check if the specified class is present in the segmentation\n    if class_i not in np.unique(seg_reshaped):\n        return  # Do nothing if the class is not found\n    \n    # Create a side-by-side plot\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Display the original slide image on the left\n    axes[0].imshow(slice_image, cmap=\"gray\")\n    axes[0].set_title(\"Slide Image\")\n    \n    # Display the slide image again on the right to overlay the mask for the specified class\n    axes[1].imshow(slice_image, cmap=\"gray\")\n    axes[1].set_title(f\"Segmentation for class {class_i}\")\n    \n    # Mask all pixels that are NOT equal to the specified class\n    seg_masked = np.ma.masked_where(seg_reshaped != class_i, seg_reshaped)\n    axes[1].imshow(seg_masked, cmap=\"tab20\")\n    \n    # Turn off axes for clarity\n    for ax in axes:\n        ax.axis(\"off\")\n    \n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(500):\n  plot_slice_class(data_train[i], labels_train.iloc[i].values.reshape((256,256)), 54)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset analysis","metadata":{"_uuid":"48c93c7e-dd2f-4fe4-829f-5fe5a6222a22","_cell_guid":"4aec5405-578c-4493-bd9d-62a314aeda42","trusted":true,"collapsed":false,"id":"NbxwLgPZWhJa","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Boolean mask for rows that contain only zeros\nzero_rows = labels_train.eq(0).all(axis=1)\n\n# Count rows that contain only zeros (unannotated images)\nunannotated_labels_train = zero_rows.sum()\n\nprint(f\"Number of unannotated images in labels_train: {unannotated_labels_train}\")","metadata":{"_uuid":"2696162f-218e-46a4-8a84-8f47cc51eb62","_cell_guid":"8542927a-100f-40dd-b101-6938f92c24be","trusted":true,"collapsed":false,"id":"CcB01JJb4OJ2","outputId":"61dce722-b644-4352-b395-03222f6879f2","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Boolean mask for rows that contain at least one nonzero pixel\nannotated_rows = labels_train.ne(0).any(axis=1)\n\n# Filter only the 800 images with labeled structures\nlabels_train_annotated = labels_train[annotated_rows]\n\n# Print the number of remaining images\nprint(f\"Number of annotated images in labels_train: {labels_train_annotated.shape[0]}\")","metadata":{"_uuid":"340326f1-ce56-41e4-8ebb-fe37b078c1ba","_cell_guid":"a3fcb09e-7f37-4784-8273-e11187673551","trusted":true,"collapsed":false,"id":"Vw2zwNnEWj6D","outputId":"bcf944c3-018e-46bc-a261-d4c6017f891d","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the number of images with empty lists (unannotated)\nunannotated_json = sum(len(labels) == 0 for labels in annotated_labels)\n\nprint(f\"Number of unannotated images in annotated_labels.json: {unannotated_json}\")","metadata":{"_uuid":"0f4ca32d-ae1f-46ac-8d2b-387c719c82b0","_cell_guid":"b5631700-3086-459e-ab76-9cf19c4c9a42","trusted":true,"collapsed":false,"id":"DILf_cbI536r","outputId":"cf02733e-0048-451c-a6ea-185a5b8aa29b","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the number of images with empty lists (unannotated)\nannotated_json = sum(len(labels) > 0 for labels in annotated_labels)\n\nprint(f\"Number of annotated images in annotated_labels.json: {annotated_json}\")","metadata":{"_uuid":"dbba120d-89c4-47e3-9dac-4f6e816c5cc2","_cell_guid":"0dc5296e-f015-4f63-abd7-2208ac114020","trusted":true,"collapsed":false,"id":"LezhlAuMYtbv","outputId":"12d2e884-d5cd-4eeb-9950-a5dc7ada84c1","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Labels are 0 for background and 1 to 54 for all organs\nprint(sorted(labels_train.stack().unique()))","metadata":{"_uuid":"1b44b861-4233-4bba-ad9a-4cb4d7d21e28","_cell_guid":"6c180daf-7a88-4372-9cfd-01b2c61ede32","trusted":true,"collapsed":false,"id":"w9BgrcPT2pUh","outputId":"f2cff32d-14d7-40d2-a58d-7c5eaf7a949b","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example\n\nimage_id = 39\nlabels_for_image = annotated_labels[image_id]\n\nprint(f\"{len(labels_for_image)} annotated labels for image {image_id}.png: {labels_for_image}\")","metadata":{"_uuid":"02309906-fbda-4620-a427-0bb4e3f11e6c","_cell_guid":"9523ac22-4d9c-4aa1-b34c-1471f9a4459b","trusted":true,"collapsed":false,"id":"Z5ff_w1jQiYZ","outputId":"edecaa34-cfbe-4e12-aa70-1e18c40167ed","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# less labels actually present in labels_train... others replaced by 0s...\nlabels_train.loc[\"39.png\"].value_counts()","metadata":{"_uuid":"ad99639d-1693-47b0-81e8-feccce57324e","_cell_guid":"720ad8be-4c77-4a91-9d95-4dad28002861","trusted":true,"collapsed":false,"id":"FwhU1iAKRcsN","outputId":"8fced0df-df2d-4c65-a5d2-8424b94da7e1","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of annotated images in labels_train: {labels_train_annotated.shape[0]}\")\n\n# Get the indices (or filenames) of annotated images.\n# (Assuming that the index of labels_df is the image filename, e.g. \"0.png\", \"1.png\", etc.)\nannotated_filenames = labels_train_annotated.index.tolist()\nprint(\"Annotated filenames (first 10):\", annotated_filenames[:10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming labels_train is already loaded, for example:\n# labels_train = pd.read_csv(\"y_train.csv\", index_col=0).T\n\n# Flatten all values and compute frequency counts\nall_values = labels_train.values.flatten()\nfrequency = pd.Series(all_values).value_counts().sort_index()\n\n# Exclude class 0\nif 0 in frequency.index:\n    frequency = frequency.drop(0)\n\nprint(frequency)\n\n# Create a bar chart\nplt.figure(figsize=(10, 6))\nfrequency.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\nplt.title(\"Frequency of Each Class in y_train.csv (excluding class 0)\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Read the entire CSV and select only 10 random rows\ntrain_df = pd.read_csv(\"y_train.csv\", index_col=0).sample(n=30, axis=1, random_state=42)\n\n# Save the selected subset to a new CSV file\ntrain_df.to_csv(\"y_train_sampled.csv\", index=True)\n\nprint(\"Saved y_train_sampled.csv with shape:\", train_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/segment-anything-2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd segment-anything-2 && ls && pip install -e .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/\")\n!wget -O sam2_hiera_base_plus.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir data && mv train-images test-images y_train.csv data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/\")\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# ----------------------------\n# Step 1. Configuration and Paths\n# ----------------------------\n# Define the base data folder (adjust as needed)\nDATA_FOLDER = \"data\"  # No trailing slash\n\n# Define paths for original train images and the CSV file containing masks\nTRAIN_IMAGES_DIR = os.path.join(DATA_FOLDER, \"train-images\")\nY_TRAIN_CSV = os.path.join(DATA_FOLDER, \"y_train.csv\")\n\n# Define the output folder for the masks and for the new train CSV\nMASKS_DIR = os.path.join(DATA_FOLDER, \"masks\")\nos.makedirs(MASKS_DIR, exist_ok=True)\n\n# ----------------------------\n# Step 2. Load and Filter y_train.csv\n# ----------------------------\n# Load the CSV; note that the CSV is transposed so that each row corresponds to an image\ndf = pd.read_csv(Y_TRAIN_CSV, index_col=0).T\nprint(\"Original y_train.csv shape:\", df.shape)\n\n# Filter out rows that contain only zeros (no annotated mask)\nmask = ~(df.values == 0).all(axis=-1)\ndf_filtered = df[mask]\nprint(\"Filtered train data shape:\", df_filtered.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_filtered.index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# Step 3. Reshape and Save Masks as Images\n# ----------------------------\n# For each row in the filtered DataFrame, reshape the flattened mask into 256x256 and save as an image\nfor file_name in df_filtered.index:\n    # Get the flattened mask as a numpy array (assumed type convertible to uint8)\n    mask_flat = df_filtered.loc[file_name].values.astype(np.uint8)\n    # Reshape into 256x256 (since 256*256 = 65536)\n    mask_img = mask_flat.reshape(256, 256)\n\n    # Replace all non-zero values with 1\n    mask_img[mask_img != 0] = 1\n    # Save the mask image into the MASKS_DIR with the same filename (e.g., \"23.png\")\n    out_path = os.path.join(MASKS_DIR, file_name)\n    cv2.imwrite(out_path, mask_img)\n\nprint(\"Saved masks to:\", MASKS_DIR)\n\n# Step 4. Create a New CSV File Mapping Filtered Images to Masks\n\n# Use the filtered train_df index (which contains only images for which a mask was saved)\nfiltered_filenames = list(df_filtered.index)  # e.g. ['23.png', '45.png', ..., 'xxx.png']\nprint(filtered_filenames[:10])\n\n# (Optionally, sort them numerically using a helper function)\nimport re\ndef numeric_key(filename):\n    number_str = re.sub(r'\\D', '', filename)\n    return int(number_str) if number_str else float('inf')\n\nfiltered_filenames = sorted(filtered_filenames, key=numeric_key)\n\n# Create a DataFrame with two columns: ImageId and MaskId.\n# We assume that the mask files in the masks folder have the same filenames as in the filtered list.\ntrain_csv_df = pd.DataFrame({\n    \"ImageId\": filtered_filenames,\n    \"MaskId\": filtered_filenames  # because masks are saved with the same names in data/masks\n})\n\n# Save this DataFrame as \"train.csv\" in your DATA_FOLDER.\nTRAIN_CSV_PATH = os.path.join(DATA_FOLDER, \"train.csv\")\ntrain_csv_df.to_csv(TRAIN_CSV_PATH, index=False)\nprint(\"Created new CSV mapping images to masks at:\", TRAIN_CSV_PATH)\n\n# ----------------------------\n# Step 5. (Optional) Split the Data into Train and Test Sets\n# ----------------------------\n# If you wish to split your dataset into train and test sets for fine-tuning SAM2,\n# you can use train_test_split. For example:\ntrain_split, test_split = train_test_split(train_csv_df, test_size=0.2, random_state=42)\ntrain_split.to_csv(os.path.join(DATA_FOLDER, \"train_split.csv\"), index=False)\ntest_split.to_csv(os.path.join(DATA_FOLDER, \"test_split.csv\"), index=False)\nprint(\"Saved train_split.csv and test_split.csv in\", DATA_FOLDER)\n\n# ----------------------------\n# Summary:\n# Your data is now restructured as follows:\n#   - Train images are in:    DATA_FOLDER/train-images/\n#   - Masks are now saved in:  DATA_FOLDER/masks/\n#   - A new CSV file mapping images to masks is saved as: DATA_FOLDER/train.csv\n#   - (Optionally) train/test split CSV files are also created.\n#\n# You can now use these files with the SAM2 fine-tuning tutorial.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.colors as mcolors\n\ndef read_batch(data, visualize_data=False):\n    # Select a random entry\n    ent = data[np.random.randint(len(data))]\n\n    image_path = ent[\"image\"]\n    annotation_path = ent[\"annotation\"]\n\n    Img = cv2.imread(image_path)\n\n    if Img is None:\n        print(f\"Error: Could not read image from {image_path}\")\n        return None, None, None, 0\n\n    # Convert BGR to RGB\n    Img = Img[..., ::-1]\n    \n    # Get full paths\n    ann_map = cv2.imread(annotation_path, cv2.IMREAD_GRAYSCALE)\n    \n    if Img is None or ann_map is None:\n       print(f\"Error: Could not read image or mask from path {ent['image']} or {ent['annotation']}\")\n       return None, None, None, 0\n    \n    # Resize image and mask\n    r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]])  # Scaling factor\n    Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n    ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n    \n    ### Continuation of read_batch() ###\n    \n    # Initialize a single binary mask\n    binary_mask = np.zeros_like(ann_map, dtype=np.uint8)\n    points = []\n    \n    # Get binary masks and combine them into a single mask\n    inds = np.unique(ann_map)[1:]  # Skip the background (index 0)\n    for ind in inds:\n       mask = (ann_map == ind).astype(np.uint8)  # Create binary mask for each unique index\n       binary_mask = np.maximum(binary_mask, mask)  # Combine with the existing binary mask\n    \n    # Erode the combined binary mask to avoid boundary points\n    eroded_mask = cv2.erode(binary_mask, np.ones((5, 5), np.uint8), iterations=1)\n\n    # Get all coordinates inside the eroded mask and choose a random point\n    coords = np.argwhere(eroded_mask > 0)\n    if len(coords) > 0:\n       for _ in inds:  # Select as many points as there are unique labels\n           yx = np.array(coords[np.random.randint(len(coords))])\n           points.append([yx[1], yx[0]])\n\n    points = np.array(points)\n    \n    ### Continuation of read_batch() ###\n    \n    if visualize_data:\n        # Plotting the images and points\n        plt.figure(figsize=(15, 5))\n    \n        # Original Image\n        plt.subplot(1, 3, 1)\n        plt.title('Original Image')\n        plt.imshow(Img)\n        plt.axis('off')\n\n        # Segmentation Mask (binary_mask)\n        plt.subplot(1, 3, 2)\n        plt.title('Binarized Mask')\n        plt.imshow(binary_mask, cmap='gray')\n        plt.axis('off')\n    \n        # Mask with Points in Different Colors\n        plt.subplot(1, 3, 3)\n        plt.title('Binarized Mask with Points')\n        plt.imshow(binary_mask, cmap='gray')\n    \n        # Plot points in different colors\n        colors = list(mcolors.TABLEAU_COLORS.values())\n        for i, point in enumerate(points):\n            plt.scatter(point[0], point[1], c=colors[i % len(colors)], s=100, label=f'Point {i+1}')  # Corrected to plot y, x order\n    \n        # plt.legend()\n        plt.axis('off')\n    \n        plt.tight_layout()\n        plt.show()\n\n    binary_mask = np.expand_dims(binary_mask, axis=-1)  # Now shape is (1024, 1024, 1)\n    binary_mask = binary_mask.transpose((2, 0, 1))\n    points = np.expand_dims(points, axis=1)\n    \n    # Return the image, binarized mask, points, and number of masks\n    return Img, binary_mask, points, len(inds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_dir = \"data/train-images\"\nmasks_dir = \"data/masks\"\n\n# Prepare the training data list\ntrain_data = []\nfor index, row in train_split.iterrows():\n   image_name = row['ImageId']\n   mask_name = row['MaskId']\n\n   # Append image and corresponding mask paths\n   train_data.append({\n       \"image\": os.path.join(images_dir, image_name),\n       \"annotation\": os.path.join(masks_dir, mask_name)\n   })\n\n# Prepare the testing data list (if needed for inference or evaluation later)\ntest_data = []\nfor index, row in test_split.iterrows():\n   image_name = row['ImageId']\n   mask_name = row['MaskId']\n\n   # Append image and corresponding mask paths\n   test_data.append({\n       \"image\": os.path.join(images_dir, image_name),\n       \"annotation\": os.path.join(masks_dir, mask_name)\n   })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the data\nos.chdir(\"/kaggle/working\")\nImg1, masks1, points1, num_masks = read_batch(train_data, visualize_data=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/segment-anything-2\")\nfrom sklearn.model_selection import train_test_split\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\nsam2_checkpoint = \"/kaggle/working/sam2_hiera_base_plus.pt\"  # @param [\"sam2_hiera_tiny.pt\", \"sam2_hiera_small.pt\", \"sam2_hiera_base_plus.pt\", \"sam2_hiera_large.pt\"]\nmodel_cfg = \"sam2_hiera_b+.yaml\" # @param [\"sam2_hiera_t.yaml\", \"sam2_hiera_s.yaml\", \"sam2_hiera_b+.yaml\", \"sam2_hiera_l.yaml\"]\n\nsam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\npredictor = SAM2ImagePredictor(sam2_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \n\n# Train mask decoder.\npredictor.model.sam_mask_decoder.train(True)\n\n# Train prompt encoder.\npredictor.model.sam_prompt_encoder.train(True)\n\n# Configure optimizer.\noptimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=0.0001,weight_decay=1e-4) #1e-5, weight_decay = 4e-5\n\n# Mix precision.\nscaler = torch.cuda.amp.GradScaler()\n\n# No. of steps to train the model.\nNO_OF_STEPS = 3000 # @param \n\n# Fine-tuned model name.\nFINE_TUNED_MODEL_NAME = \"fine_tuned_sam2\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")\n\nprint(\"Preparing training\")\n\n# Initialize scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.2) # 500 , 250, gamma = 0.1\naccumulation_steps = 2  # Number of steps to accumulate gradients before updating\n\nfor step in range(1, NO_OF_STEPS + 1):\n    with torch.cuda.amp.autocast():\n        image, mask, input_point, num_masks = read_batch(train_data, visualize_data=False)\n\n    if image is None or mask is None or num_masks == 0:\n       continue\n    \n    input_label = np.ones((num_masks, 1))\n    if not isinstance(input_point, np.ndarray) or not isinstance(input_label, np.ndarray):\n       continue\n    \n    if input_point.size == 0 or input_label.size == 0:\n       continue\n    \n    predictor.set_image(image)\n    mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n    if unnorm_coords is None or labels is None or unnorm_coords.shape[0] == 0 or labels.shape[0] == 0:\n       continue\n    \n    sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n       points=(unnorm_coords, labels), boxes=None, masks=None,\n    )\n    \n    batched_mode = unnorm_coords.shape[0] > 1\n    high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n    low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n       image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n       image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n       sparse_prompt_embeddings=sparse_embeddings,\n       dense_prompt_embeddings=dense_embeddings,\n       multimask_output=True,\n       repeat_image=batched_mode,\n       high_res_features=high_res_features,\n    )\n    prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n\n    \n    gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n    prd_mask = torch.sigmoid(prd_masks[:, 0])\n    \n    # Set a confidence threshold and a lower weight for false positives above that threshold.\n    confidence_threshold = 0.9\n    # Create a weight tensor: same shape as prd_mask, default weight 1.\n    fp_weight = torch.ones_like(prd_mask)\n    # Where the predicted probability is greater than the threshold, set weight to 0.5 (or another value < 1)\n    fp_weight = torch.where(prd_mask > confidence_threshold, torch.tensor(0.5, device=prd_mask.device), fp_weight)\n    \n    # Compute segmentation loss with the modified weighting\n    seg_loss = (\n        -gt_mask * torch.log(prd_mask + 1e-6) -\n        fp_weight * (1 - gt_mask) * torch.log((1 - prd_mask) + 1e-5)\n    ).mean()\n    \n    # Continue with the rest of your loss calculation\n    inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n    iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n    score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n    loss = seg_loss + score_loss * 0.05\n    \"\"\"\n    \n    \n    gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n    prd_mask = torch.sigmoid(prd_masks[:, 0])\n    seg_loss = (-gt_mask * torch.log(prd_mask + 0.000001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n    \n    inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n    iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n    score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n    loss = seg_loss + score_loss * 0.05\n    \"\"\"\n    \n    # Apply gradient accumulation\n    loss = loss / accumulation_steps\n    scaler.scale(loss).backward()\n    \n    # Clip gradients\n    torch.nn.utils.clip_grad_norm_(predictor.model.parameters(), max_norm=1.0)\n    \n    if step % accumulation_steps == 0:\n       scaler.step(optimizer)\n       scaler.update()\n       predictor.model.zero_grad()\n    \n    # Update scheduler\n    scheduler.step()\n    \n    if step % 500 == 0:\n       FINE_TUNED_MODEL = FINE_TUNED_MODEL_NAME + \"_\" + str(step) + \".torch\"\n       torch.save(predictor.model.state_dict(), FINE_TUNED_MODEL)\n    \n    if step == 1:\n       mean_iou = 0\n\n    if step % 50 == 0:\n        print(step)\n    \n    mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n    \n    if step % 100 == 0:\n       print(\"Step \" + str(step) + \":\\t\", \"Accuracy (IoU) = \", mean_iou)","metadata":{"execution":{"iopub.status.busy":"2025-03-07T19:28:24.304155Z","iopub.execute_input":"2025-03-07T19:28:24.304506Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Preparing training\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-153-7ad5af4044d0>:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def read_image(image_path, mask_path):  # read and resize image and mask\n   img = cv2.imread(image_path)[..., ::-1]  # Convert BGR to RGB\n   mask = cv2.imread(mask_path, 0)\n   r = np.min([1024 / img.shape[1], 1024 / img.shape[0]])\n   img = cv2.resize(img, (int(img.shape[1] * r), int(img.shape[0] * r)))\n   mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n   return img, mask\n\ndef get_points(mask, num_points):  # Sample points inside the input mask\n   points = []\n   coords = np.argwhere(mask > 0)\n   for i in range(num_points):\n       yx = np.array(coords[np.random.randint(len(coords))])\n       points.append([[yx[1], yx[0]]])\n   return np.array(points)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport random\ntorch.cuda.empty_cache()\n\n# Randomly select a test image from the test_data\nselected_entry = random.choice(test_data)\nimage_path = selected_entry['image']\nmask_path = selected_entry['annotation']\n\n# Load the selected image and mask\nimage, mask = read_image(image_path, mask_path)\n\n# Generate random points for the input\nnum_samples = 20  # Number of points per segment to sample\ninput_points = get_points(mask, num_samples)\n\n# Load the fine-tuned model\nFINE_TUNED_MODEL_WEIGHTS = \"fine_tuned_sam2_2000.torch\"\nsam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n\n# Build net and load weights\npredictor = SAM2ImagePredictor(sam2_model)\npredictor.model.load_state_dict(torch.load(FINE_TUNED_MODEL_WEIGHTS))\n\n# Perform inference and predict masks\nwith torch.no_grad():\n   predictor.set_image(image)\n   masks, scores, logits = predictor.predict(\n       point_coords=input_points,\n       point_labels=np.ones([input_points.shape[0], 1])\n   )\n\n# Process the predicted masks and sort by scores\nnp_masks = np.array(masks[:, 0])\n\nif scores.ndim == 1:\n    np_scores = scores\nelse:\n    np_scores = scores[:, 0]\n\nsorted_masks = np_masks[np.argsort(np_scores)][::-1]\n\nprint(sorted_masks.shape)\n\n# Initialize segmentation map and occupancy mask\nseg_map = np.zeros_like(sorted_masks[0], dtype=np.uint8)\noccupancy_mask = np.zeros_like(sorted_masks[0], dtype=bool)\n\n# Combine masks to create the final segmentation map\nfor i in range(sorted_masks.shape[0]):\n   mask = sorted_masks[i]\n   if (mask * occupancy_mask).sum() / mask.sum() > 0.15:\n       continue\n\n   mask_bool = mask.astype(bool)\n   mask_bool[occupancy_mask] = False  # Set overlapping areas to False in the mask\n   seg_map[mask_bool] = i + 1  # Use boolean mask to index seg_map\n   occupancy_mask[mask_bool] = True  # Update occupancy_mask\n\n# Visualization: Show the original image, mask, and final segmentation side by side\nplt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nplt.title('Test Image')\nplt.imshow(image)\nplt.axis('off')\n\nif mask.ndim == 1:\n    mask = mask.reshape(32, 32)\n\nplt.subplot(1, 3, 2)\nplt.title('Original Mask')\nplt.imshow(mask, cmap='gray')\nplt.axis('off')\n\nif seg_map.ndim == 1:\n    seg_map =seg_map.reshape(32, 32)\n\nplt.subplot(1, 3, 3)\nplt.title('Final Segmentation')\nplt.imshow(seg_map, cmap='jet')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(seg_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}